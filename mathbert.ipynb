{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11666150,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom collections import Counter\nimport os\n\n# Step 1: Load the arXiv dataset\ninput_path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\ndata = []\nprint(\"Loading arXiv JSON lines dataset...\")\nwith open(input_path, 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nprint(f\"Total papers loaded: {len(data)}\")\n\n# Step 2: Convert to DataFrame\ndf = pd.DataFrame(data)\nprint(\"Available columns:\")\nprint(df.columns.tolist())\n\n# Step 3: Count papers per category\ncategory_counts = Counter()\nfor cats in df['categories']:\n    for cat in cats.split():\n        category_counts[cat] += 1\n\ncategory_df = pd.DataFrame(category_counts.items(), columns=['Category', 'Count'])\ncategory_df = category_df.sort_values('Count', ascending=False)\n\n# Step 4: Filter categories with >5,000 papers\npopular_categories = category_df[category_df['Count'] > 5000]['Category'].tolist()\nprint(f\"\\nCategories with > 5,000 papers: {len(popular_categories)}\")\nprint(category_df[category_df['Category'].isin(popular_categories)].head(20))\n\n# Step 5: Filter papers that have at least one popular category\ndef has_popular_category(category_str):\n    return any(cat in popular_categories for cat in category_str.split())\n\nfiltered_df = df[df['categories'].apply(has_popular_category)]\nprint(f\"\\nFiltered papers: {len(filtered_df)}\")\n\n# Step 6: Keep only relevant columns and enrich text\nfiltered_df = filtered_df[['title', 'abstract', 'categories']]\nfiltered_df = filtered_df.dropna()\nfiltered_df['title_abstract'] = filtered_df['title'] + \" \" + filtered_df['abstract']\n\n# Step 7: Save outputs\noutput_dir = '/kaggle/working'\nos.makedirs(output_dir, exist_ok=True)\n\nfiltered_csv_path = os.path.join(output_dir, 'filtered_arxiv_papers.csv')\nstats_csv_path = os.path.join(output_dir, 'category_statistics.csv')\n\nfiltered_df.to_csv(filtered_csv_path, index=False)\ncategory_df.to_csv(stats_csv_path, index=False)\n\n# Step 8: Print summary\nprint(\"\\n Processing Complete\")\nprint(f\"Original total papers: {len(df)}\")\nprint(f\"Filtered papers: {len(filtered_df)}\")\nprint(f\"Filtered dataset saved to: {filtered_csv_path}\")\nprint(f\"Category statistics saved to: {stats_csv_path}\")\nprint(\"\\nSample rows:\")\nprint(filtered_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:38:33.124119Z","iopub.execute_input":"2025-05-10T09:38:33.124622Z","iopub.status.idle":"2025-05-10T09:43:18.251570Z","shell.execute_reply.started":"2025-05-10T09:38:33.124599Z","shell.execute_reply":"2025-05-10T09:43:18.250865Z"}},"outputs":[{"name":"stdout","text":"Loading arXiv JSON lines dataset...\nTotal papers loaded: 2725401\nAvailable columns:\n['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n\nCategories with > 5,000 papers: 128\n               Category   Count\n96                cs.LG  215117\n0                hep-ph  187176\n13               hep-th  173367\n27             quant-ph  161245\n114               cs.CV  153832\n42                cs.AI  124171\n7                 gr-qc  113340\n9              astro-ph  105380\n8     cond-mat.mtrl-sci   99338\n6     cond-mat.mes-hall   95166\n34              math.MP   83839\n33              math-ph   83839\n126               cs.CL   82766\n20      cond-mat.str-el   77579\n21   cond-mat.stat-mech   76742\n136         astro-ph.CO   71425\n1               math.CO   71057\n110             stat.ML   70559\n144         astro-ph.GA   69725\n66              math.AP   67040\n\nFiltered papers: 2697904\n\n Processing Complete\nOriginal total papers: 2725401\nFiltered papers: 2697904\nFiltered dataset saved to: /kaggle/working/filtered_arxiv_papers.csv\nCategory statistics saved to: /kaggle/working/category_statistics.csv\n\nSample rows:\n                                               title  \\\n0  Calculation of prompt diphoton production cros...   \n1           Sparsity-certifying Graph Decompositions   \n2  The evolution of the Earth-Moon system based o...   \n3  A determinant of Stirling cycle numbers counts...   \n4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n\n                                            abstract       categories  \\\n0    A fully differential calculation in perturba...           hep-ph   \n1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG   \n2    The evolution of Earth-Moon system is descri...   physics.gen-ph   \n3    We show that a determinant of Stirling cycle...          math.CO   \n4    In this paper we show how to compute the $\\L...  math.CA math.FA   \n\n                                      title_abstract  \n0  Calculation of prompt diphoton production cros...  \n1  Sparsity-certifying Graph Decompositions   We ...  \n2  The evolution of the Earth-Moon system based o...  \n3  A determinant of Stirling cycle numbers counts...  \n4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\n\n# File paths\nfile_path = '/kaggle/working/filtered_arxiv_papers.csv'\nstats_path = '/kaggle/working/category_statistics.csv'\n\n# Check file\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\n# Load dataset\nprint(\"Loading filtered_arxiv_papers.csv...\")\ndf = pd.read_csv(file_path)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Enrich text\nif 'title_abstract' not in df.columns:\n    print(\"Enriching text column with title + abstract...\")\n    df['title_abstract'] = df['title'].astype(str) + \" \" + df['abstract'].astype(str)\n\n# Load and filter category stats for math.* only\nprint(\"Loading category statistics...\")\nstats_df = pd.read_csv(stats_path)\nstats_df = stats_df[stats_df['Category'].str.startswith('math.')]\nstats_df = stats_df.sort_values('Count', ascending=False)\n\ntop_categories = stats_df['Category'].tolist()\nprint(f\"\\nUsing ALL {len(top_categories)} math categories:\")\nprint(top_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:43:26.433646Z","iopub.execute_input":"2025-05-10T09:43:26.434184Z","iopub.status.idle":"2025-05-10T09:44:36.774752Z","shell.execute_reply.started":"2025-05-10T09:43:26.434158Z","shell.execute_reply":"2025-05-10T09:44:36.774128Z"}},"outputs":[{"name":"stdout","text":"Checking for /kaggle/working/filtered_arxiv_papers.csv...\nLoading filtered_arxiv_papers.csv...\nLoaded 2697904 rows\nColumns: ['title', 'abstract', 'categories', 'title_abstract']\nLoading category statistics...\n\nUsing ALL 32 math categories:\n['math.MP', 'math.CO', 'math.AP', 'math.PR', 'math.AG', 'math.OC', 'math.IT', 'math.NT', 'math.DG', 'math.NA', 'math.DS', 'math.FA', 'math.RT', 'math.ST', 'math.GT', 'math.GR', 'math.CA', 'math.QA', 'math.RA', 'math.CV', 'math.AT', 'math.LO', 'math.AC', 'math.OA', 'math.MG', 'math.SP', 'math.SG', 'math.CT', 'math.KT', 'math.GN', 'math.GM', 'math.HO']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Map labels\nlabel_map = {cat: idx for idx, cat in enumerate(top_categories)}\n\n# Extract primary math category\ndef extract_primary_category(cat_str):\n    if pd.isna(cat_str):\n        return None\n    for cat in cat_str.split():\n        if cat.startswith('math.') and cat in label_map:\n            return cat\n    return None\n\n# Apply extraction\nprint(\"Mapping categories to labels...\")\ndf['category'] = df['categories'].apply(extract_primary_category)\ndf['label'] = df['category'].map(label_map)\n\n# Filter valid rows\ndf = df[df['label'].notnull()].copy()\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:48:20.415262Z","iopub.execute_input":"2025-05-10T09:48:20.416020Z","iopub.status.idle":"2025-05-10T09:48:23.812240Z","shell.execute_reply.started":"2025-05-10T09:48:20.415984Z","shell.execute_reply":"2025-05-10T09:48:23.811499Z"}},"outputs":[{"name":"stdout","text":"Mapping categories to labels...\n                                                     title  \\\n1                 Sparsity-certifying Graph Decompositions   \n3        A determinant of Stirling cycle numbers counts...   \n4        From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n9        Partial cubes: structures, characterizations, ...   \n10       Computing genus 2 Hilbert-Siegel modular forms...   \n...                                                    ...   \n2697800  Yang-Baxter Algebra for the n-Harmonic Oscilla...   \n2697813  Integrable deformations of oscillator chains f...   \n2697814  A note on real forms of the complex N=4 supers...   \n2697828  Real forms of the complex twisted N=2 supersym...   \n2697833  Vector NLS hierarchy solitons revisited: dress...   \n\n                                                  abstract  \\\n1          We describe a new algorithm, the $(k,\\ell)$-...   \n3          We show that a determinant of Stirling cycle...   \n4          In this paper we show how to compute the $\\L...   \n9          Partial cubes are isometric subgraphs of hyp...   \n10         In this paper we present an algorithm for co...   \n...                                                    ...   \n2697800    Using a rational R-matrix associated with th...   \n2697813    A family of completely integrable nonlinear ...   \n2697814    Three inequivalent real forms of the complex...   \n2697828    Three nonequivalent real forms of the comple...   \n2697833    We discuss some algebraic aspects of the int...   \n\n                                      categories  \\\n1                                  math.CO cs.CG   \n3                                        math.CO   \n4                                math.CA math.FA   \n9                                        math.CO   \n10                               math.NT math.AG   \n...                                          ...   \n2697800         solv-int math-ph math.MP nlin.SI   \n2697813                 solv-int math.QA nlin.SI   \n2697814  solv-int hep-th math-ph math.MP nlin.SI   \n2697828  solv-int hep-th math-ph math.MP nlin.SI   \n2697833  solv-int hep-th math-ph math.MP nlin.SI   \n\n                                            title_abstract category  label  \n1        Sparsity-certifying Graph Decompositions   We ...  math.CO    1.0  \n3        A determinant of Stirling cycle numbers counts...  math.CO    1.0  \n4        From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  math.CA   16.0  \n9        Partial cubes: structures, characterizations, ...  math.CO    1.0  \n10       Computing genus 2 Hilbert-Siegel modular forms...  math.NT    7.0  \n...                                                    ...      ...    ...  \n2697800  Yang-Baxter Algebra for the n-Harmonic Oscilla...  math.MP    0.0  \n2697813  Integrable deformations of oscillator chains f...  math.QA   17.0  \n2697814  A note on real forms of the complex N=4 supers...  math.MP    0.0  \n2697828  Real forms of the complex twisted N=2 supersym...  math.MP    0.0  \n2697833  Vector NLS hierarchy solitons revisited: dress...  math.MP    0.0  \n\n[686777 rows x 6 columns]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\n\n# File paths\nfile_path = '/kaggle/working/filtered_arxiv_papers.csv'\nstats_path = '/kaggle/working/category_statistics.csv'\noutput_path = '/kaggle/working/math_dataset_labeled.csv'\nlabel_map_path = '/kaggle/working/label_map.csv'\n\n# Check file existence\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\n# Load the dataset\nprint(\"Loading filtered_arxiv_papers.csv...\")\ndf = pd.read_csv(file_path)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Enrich title_abstract if not present\nif 'title_abstract' not in df.columns:\n    print(\"Creating 'title_abstract' from title + abstract...\")\n    df['title'] = df['title'].fillna('')\n    df['abstract'] = df['abstract'].fillna('')\n    df['title_abstract'] = df['title'] + \" \" + df['abstract']\n\n# Load category stats and filter math.* categories\nprint(\"Loading and filtering category statistics...\")\nstats_df = pd.read_csv(stats_path)\nstats_df = stats_df[stats_df['Category'].str.startswith('math.')]\nstats_df = stats_df.sort_values('Count', ascending=False)\n\ntop_categories = stats_df['Category'].tolist()\nprint(f\"\\nUsing ALL {len(top_categories)} math categories:\")\nprint(top_categories)\n\n# Create label mapping\nlabel_map = {cat: idx for idx, cat in enumerate(top_categories)}\n\n# Function to extract primary math category\ndef extract_primary_category(cat_str):\n    if pd.isna(cat_str):\n        return None\n    for cat in cat_str.split():\n        if cat.startswith('math.') and cat in label_map:\n            return cat\n    return None\n\n# Apply category extraction and label mapping\nprint(\"Mapping categories to labels...\")\ndf['category'] = df['categories'].apply(extract_primary_category)\ndf['label'] = df['category'].map(label_map)\n\n# Filter out rows without math category\ndf = df[df['label'].notnull()].copy()\n\n# Save label map to CSV\npd.Series(label_map).to_csv(label_map_path, header=['Label'], index_label='Category')\nprint(f\"Saved label mapping to {label_map_path}\")\n\n# Drop unneeded columns\ndf = df.drop(columns=['title','abstract'])\n\n# Save final labeled dataset\ndf.to_csv(output_path, index=False)\nprint(f\"Saved cleaned dataset to {output_path}\")\n\n# Reload and print category distribution\nprint(\"\\n--- Category Distribution ---\")\ndf_loaded = pd.read_csv(output_path)\ncategory_counts = df_loaded['category'].value_counts()\n\nfor category, count in category_counts.items():\n    label = df_loaded[df_loaded['category'] == category]['label'].iloc[0]\n    print(f\"Category: {category} | Label: {label} | Papers: {count}\")\n\nprint(\"\\nTotal unique categories:\", df_loaded['category'].nunique())\nprint(\"Total number of papers:\", len(df_loaded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:48:31.227021Z","iopub.execute_input":"2025-05-10T09:48:31.227321Z","iopub.status.idle":"2025-05-10T09:50:07.943791Z","shell.execute_reply.started":"2025-05-10T09:48:31.227299Z","shell.execute_reply":"2025-05-10T09:50:07.943026Z"}},"outputs":[{"name":"stdout","text":"Checking for /kaggle/working/filtered_arxiv_papers.csv...\nLoading filtered_arxiv_papers.csv...\nLoaded 2697904 rows\nColumns: ['title', 'abstract', 'categories', 'title_abstract']\nLoading and filtering category statistics...\n\nUsing ALL 32 math categories:\n['math.MP', 'math.CO', 'math.AP', 'math.PR', 'math.AG', 'math.OC', 'math.IT', 'math.NT', 'math.DG', 'math.NA', 'math.DS', 'math.FA', 'math.RT', 'math.ST', 'math.GT', 'math.GR', 'math.CA', 'math.QA', 'math.RA', 'math.CV', 'math.AT', 'math.LO', 'math.AC', 'math.OA', 'math.MG', 'math.SP', 'math.SG', 'math.CT', 'math.KT', 'math.GN', 'math.GM', 'math.HO']\nMapping categories to labels...\nSaved label mapping to /kaggle/working/label_map.csv\nSaved cleaned dataset to /kaggle/working/math_dataset_labeled.csv\n\n--- Category Distribution ---\nCategory: math.AP | Label: 2.0 | Papers: 56030\nCategory: math.CO | Label: 1.0 | Papers: 55123\nCategory: math.MP | Label: 0.0 | Papers: 51286\nCategory: math.OC | Label: 5.0 | Papers: 46708\nCategory: math.IT | Label: 6.0 | Papers: 45739\nCategory: math.PR | Label: 3.0 | Papers: 45152\nCategory: math.AG | Label: 4.0 | Papers: 41107\nCategory: math.NA | Label: 9.0 | Papers: 36481\nCategory: math.NT | Label: 7.0 | Papers: 34589\nCategory: math.DG | Label: 8.0 | Papers: 33941\nCategory: math.DS | Label: 10.0 | Papers: 29266\nCategory: math.FA | Label: 11.0 | Papers: 22326\nCategory: math.ST | Label: 13.0 | Papers: 20607\nCategory: math.CA | Label: 16.0 | Papers: 16835\nCategory: math.GT | Label: 14.0 | Papers: 16643\nCategory: math.RT | Label: 12.0 | Papers: 16061\nCategory: math.GR | Label: 15.0 | Papers: 15345\nCategory: math.QA | Label: 17.0 | Papers: 11648\nCategory: math.RA | Label: 18.0 | Papers: 11481\nCategory: math.LO | Label: 21.0 | Papers: 11384\nCategory: math.CV | Label: 19.0 | Papers: 11194\nCategory: math.AT | Label: 20.0 | Papers: 10258\nCategory: math.AC | Label: 22.0 | Papers: 9138\nCategory: math.OA | Label: 23.0 | Papers: 8679\nCategory: math.MG | Label: 24.0 | Papers: 6669\nCategory: math.CT | Label: 27.0 | Papers: 5177\nCategory: math.SG | Label: 26.0 | Papers: 5068\nCategory: math.SP | Label: 25.0 | Papers: 4992\nCategory: math.GN | Label: 29.0 | Papers: 3509\nCategory: math.KT | Label: 28.0 | Papers: 2624\nCategory: math.HO | Label: 31.0 | Papers: 1430\nCategory: math.GM | Label: 30.0 | Papers: 287\n\nTotal unique categories: 32\nTotal number of papers: 686777\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/working/math_dataset_labeled.csv')\n\n# Check the columns to ensure proper structure\nprint(df.columns)\n\n# Balance dataset\nrows_per_category = 3000\nprint(f\"\\nBalancing: {rows_per_category} rows per category...\")\n\n# Placeholder for filtered data and skipped categories\n\nfiltered_list = []\nskipped = []\n\n# Get the top categories (those that have the most papers)\ntop_categories = df['category'].value_counts().index.tolist()\n\n# Iterate over the top categories to balance\nfor cat in top_categories:\n    cat_df = df[df['category'] == cat]\n    available = len(cat_df)\n    if available >= rows_per_category:\n        filtered_list.append(cat_df.sample(n=rows_per_category, random_state=42))\n    else:\n        skipped.append((cat, available))\n        print(f\"⚠ Skipping {cat}: only {available} papers (needs ≥ {rows_per_category})\")\n\n# Combine all balanced samples into a new DataFrame\nif not filtered_list:\n    raise ValueError(\"No categories had enough papers to sample from!\")\n\nbalanced_df = pd.concat(filtered_list).reset_index(drop=True)\n\n# Print summary of balancing\nprint(f\"\\nFinal balanced dataset size: {len(balanced_df)}\")\nprint(f\"Included categories: {balanced_df['category'].nunique()}\")\nprint(f\"Skipped categories: {len(skipped)}\")\n\n# Update label map for actually used categories\nused_categories = sorted(balanced_df['category'].unique())\nlabel_map = {cat: i for i, cat in enumerate(used_categories)}\nbalanced_df['label'] = balanced_df['category'].map(label_map)\n\n# Report label distribution\nprint(\"\\nLabel mapping:\")\nfor cat, idx in label_map.items():\n    count = (balanced_df['category'] == cat).sum()\n    print(f\" Category: {cat:<10} | Label: {idx:<2} | Papers: {count}\")\n\nprint(f\"\\nTotal categories used: {len(label_map)}\")\nprint(f\"Total papers: {len(balanced_df)}\")\n\n# Split into train/val/test (80/10/10)\nprint(\"\\nSplitting dataset...\")\ntrain_df, temp_df = train_test_split(\n    balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42\n)\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n)\n\n# Save the splits\noutput_dir = '/kaggle/working/'\nos.makedirs(output_dir, exist_ok=True)\n\ntrain_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\nval_df.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\ntest_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n\n# Final summary\nprint(f\"\\nSaved train/val/test splits:\")\nprint(f\" Train size: {len(train_df)}\")\nprint(f\" Val size:   {len(val_df)}\")\nprint(f\" Test size:  {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:50:17.789312Z","iopub.execute_input":"2025-05-10T09:50:17.789562Z","iopub.status.idle":"2025-05-10T09:50:29.556214Z","shell.execute_reply.started":"2025-05-10T09:50:17.789546Z","shell.execute_reply":"2025-05-10T09:50:29.555436Z"}},"outputs":[{"name":"stdout","text":"Index(['categories', 'title_abstract', 'category', 'label'], dtype='object')\n\nBalancing: 3000 rows per category...\n⚠ Skipping math.KT: only 2624 papers (needs ≥ 3000)\n⚠ Skipping math.HO: only 1430 papers (needs ≥ 3000)\n⚠ Skipping math.GM: only 287 papers (needs ≥ 3000)\n\nFinal balanced dataset size: 87000\nIncluded categories: 29\nSkipped categories: 3\n\nLabel mapping:\n Category: math.AC    | Label: 0  | Papers: 3000\n Category: math.AG    | Label: 1  | Papers: 3000\n Category: math.AP    | Label: 2  | Papers: 3000\n Category: math.AT    | Label: 3  | Papers: 3000\n Category: math.CA    | Label: 4  | Papers: 3000\n Category: math.CO    | Label: 5  | Papers: 3000\n Category: math.CT    | Label: 6  | Papers: 3000\n Category: math.CV    | Label: 7  | Papers: 3000\n Category: math.DG    | Label: 8  | Papers: 3000\n Category: math.DS    | Label: 9  | Papers: 3000\n Category: math.FA    | Label: 10 | Papers: 3000\n Category: math.GN    | Label: 11 | Papers: 3000\n Category: math.GR    | Label: 12 | Papers: 3000\n Category: math.GT    | Label: 13 | Papers: 3000\n Category: math.IT    | Label: 14 | Papers: 3000\n Category: math.LO    | Label: 15 | Papers: 3000\n Category: math.MG    | Label: 16 | Papers: 3000\n Category: math.MP    | Label: 17 | Papers: 3000\n Category: math.NA    | Label: 18 | Papers: 3000\n Category: math.NT    | Label: 19 | Papers: 3000\n Category: math.OA    | Label: 20 | Papers: 3000\n Category: math.OC    | Label: 21 | Papers: 3000\n Category: math.PR    | Label: 22 | Papers: 3000\n Category: math.QA    | Label: 23 | Papers: 3000\n Category: math.RA    | Label: 24 | Papers: 3000\n Category: math.RT    | Label: 25 | Papers: 3000\n Category: math.SG    | Label: 26 | Papers: 3000\n Category: math.SP    | Label: 27 | Papers: 3000\n Category: math.ST    | Label: 28 | Papers: 3000\n\nTotal categories used: 29\nTotal papers: 87000\n\nSplitting dataset...\n\nSaved train/val/test splits:\n Train size: 69600\n Val size:   8700\n Test size:  8700\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Step 2: Tokenize dataset for Kaggle\nimport pandas as pd\nfrom transformers import LongformerTokenizer\nimport torch\nimport pickle\nimport logging\nimport os\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/step2_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Define input/output directories\ninput_dir = '/kaggle/working/'  # Kaggle uploaded files are here\noutput_dir = '/kaggle/working/'  # Save outputs here\n\n# Find train.csv and val.csv automatically\nprint(\"Searching for train.csv and val.csv in /kaggle/input/...\")\nlogger.info(\"Searching for train.csv and val.csv in /kaggle/input/...\")\ntrain_path = None\nval_path = None\n\nfor root, dirs, files in os.walk(input_dir):\n    for file in files:\n        if file == 'train.csv':\n            train_path = os.path.join(root, file)\n        if file == 'val.csv':\n            val_path = os.path.join(root, file)\n    if train_path and val_path:\n        break\n\nif not train_path or not val_path:\n    logger.error(\"train.csv or val.csv not found in /kaggle/input/.\")\n    raise FileNotFoundError(\"train.csv or val.csv not found. Please upload them.\")\n\nprint(f\"Found train.csv at {train_path}\")\nprint(f\"Found val.csv at {val_path}\")\nlogger.info(f\"Found train.csv at {train_path}\")\nlogger.info(f\"Found val.csv at {val_path}\")\n\n# Load tokenizer\nprint(\"Loading Longformer tokenizer...\")\nlogger.info(\"Loading Longformer tokenizer...\")\ntry:\n    tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nexcept Exception as e:\n    logger.error(f\"Error loading tokenizer: {e}\")\n    raise\n\n# Load datasets\nprint(\"Loading datasets...\")\nlogger.info(\"Loading datasets...\")\ntrain_df = pd.read_csv(train_path)\nval_df = pd.read_csv(val_path)\n\n# Tokenization function\ndef tokenize_data(df, max_length=4096):\n    texts = df['title_abstract'].tolist()  # Assuming 'text' column\n    labels = df['label'].tolist()\n    encodings = tokenizer(\n        texts, \n        truncation=True, \n        padding=True, \n        max_length=max_length, \n        return_tensors='pt'\n    )\n    return {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': labels\n    }\n\n# Tokenize in batches\nbatch_size = 100\ntrain_tokenized = []\nval_tokenized = []\n\nprint(\"Tokenizing training data...\")\nlogger.info(\"Tokenizing training data...\")\nfor i in range(0, len(train_df), batch_size):\n    batch_df = train_df[i:i+batch_size]\n    tokenized_batch = tokenize_data(batch_df)\n    train_tokenized.append(tokenized_batch)\n    print(f\"Tokenized train batch {i//batch_size + 1}/{len(train_df)//batch_size + 1}\")\n    logger.info(f\"Tokenized train batch {i//batch_size + 1}/{len(train_df)//batch_size + 1}\")\n\nprint(\"Tokenizing validation data...\")\nlogger.info(\"Tokenizing validation data...\")\nfor i in range(0, len(val_df), batch_size):\n    batch_df = val_df[i:i+batch_size]\n    tokenized_batch = tokenize_data(batch_df)\n    val_tokenized.append(tokenized_batch)\n    print(f\"Tokenized val batch {i//batch_size + 1}/{len(val_df)//batch_size + 1}\")\n    logger.info(f\"Tokenized val batch {i//batch_size + 1}/{len(val_df)//batch_size + 1}\")\n\n# Save tokenized datasets\nprint(f\"Saving tokenized datasets to {output_dir}...\")\nlogger.info(f\"Saving tokenized datasets to {output_dir}...\")\nwith open(os.path.join(output_dir, 'train_tokenized.pkl'), 'wb') as f:\n    pickle.dump(train_tokenized, f)\nwith open(os.path.join(output_dir, 'val_tokenized.pkl'), 'wb') as f:\n    pickle.dump(val_tokenized, f)\n\nprint(f\"Train tokenized: {len(train_tokenized)} batches, Val tokenized: {len(val_tokenized)} batches\")\nlogger.info(f\"Train tokenized: {len(train_tokenized)} batches, Val tokenized: {len(val_tokenized)} batches\")\nprint(\"Step 2 complete.\")\nlogger.info(\"Step 2 complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:50:57.730194Z","iopub.execute_input":"2025-05-10T09:50:57.730482Z","iopub.status.idle":"2025-05-10T09:52:54.912960Z","shell.execute_reply.started":"2025-05-10T09:50:57.730460Z","shell.execute_reply":"2025-05-10T09:52:54.912073Z"}},"outputs":[{"name":"stdout","text":"Searching for train.csv and val.csv in /kaggle/input/...\nFound train.csv at /kaggle/working/train.csv\nFound val.csv at /kaggle/working/val.csv\nLoading Longformer tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b0e1b0433f748eaac6c1f23b7e637f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"654576f0bbee4504881c2dbda293f04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c69f4764b44882a7c5da8fb368a033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a081f12b091400ba119c3d6220566f0"}},"metadata":{}},{"name":"stdout","text":"Loading datasets...\nTokenizing training data...\nTokenized train batch 1/697\nTokenized train batch 2/697\nTokenized train batch 3/697\nTokenized train batch 4/697\nTokenized train batch 5/697\nTokenized train batch 6/697\nTokenized train batch 7/697\nTokenized train batch 8/697\nTokenized train batch 9/697\nTokenized train batch 10/697\nTokenized train batch 11/697\nTokenized train batch 12/697\nTokenized train batch 13/697\nTokenized train batch 14/697\nTokenized train batch 15/697\nTokenized train batch 16/697\nTokenized train batch 17/697\nTokenized train batch 18/697\nTokenized train batch 19/697\nTokenized train batch 20/697\nTokenized train batch 21/697\nTokenized train batch 22/697\nTokenized train batch 23/697\nTokenized train batch 24/697\nTokenized train batch 25/697\nTokenized train batch 26/697\nTokenized train batch 27/697\nTokenized train batch 28/697\nTokenized train batch 29/697\nTokenized train batch 30/697\nTokenized train batch 31/697\nTokenized train batch 32/697\nTokenized train batch 33/697\nTokenized train batch 34/697\nTokenized train batch 35/697\nTokenized train batch 36/697\nTokenized train batch 37/697\nTokenized train batch 38/697\nTokenized train batch 39/697\nTokenized train batch 40/697\nTokenized train batch 41/697\nTokenized train batch 42/697\nTokenized train batch 43/697\nTokenized train batch 44/697\nTokenized train batch 45/697\nTokenized train batch 46/697\nTokenized train batch 47/697\nTokenized train batch 48/697\nTokenized train batch 49/697\nTokenized train batch 50/697\nTokenized train batch 51/697\nTokenized train batch 52/697\nTokenized train batch 53/697\nTokenized train batch 54/697\nTokenized train batch 55/697\nTokenized train batch 56/697\nTokenized train batch 57/697\nTokenized train batch 58/697\nTokenized train batch 59/697\nTokenized train batch 60/697\nTokenized train batch 61/697\nTokenized train batch 62/697\nTokenized train batch 63/697\nTokenized train batch 64/697\nTokenized train batch 65/697\nTokenized train batch 66/697\nTokenized train batch 67/697\nTokenized train batch 68/697\nTokenized train batch 69/697\nTokenized train batch 70/697\nTokenized train batch 71/697\nTokenized train batch 72/697\nTokenized train batch 73/697\nTokenized train batch 74/697\nTokenized train batch 75/697\nTokenized train batch 76/697\nTokenized train batch 77/697\nTokenized train batch 78/697\nTokenized train batch 79/697\nTokenized train batch 80/697\nTokenized train batch 81/697\nTokenized train batch 82/697\nTokenized train batch 83/697\nTokenized train batch 84/697\nTokenized train batch 85/697\nTokenized train batch 86/697\nTokenized train batch 87/697\nTokenized train batch 88/697\nTokenized train batch 89/697\nTokenized train batch 90/697\nTokenized train batch 91/697\nTokenized train batch 92/697\nTokenized train batch 93/697\nTokenized train batch 94/697\nTokenized train batch 95/697\nTokenized train batch 96/697\nTokenized train batch 97/697\nTokenized train batch 98/697\nTokenized train batch 99/697\nTokenized train batch 100/697\nTokenized train batch 101/697\nTokenized train batch 102/697\nTokenized train batch 103/697\nTokenized train batch 104/697\nTokenized train batch 105/697\nTokenized train batch 106/697\nTokenized train batch 107/697\nTokenized train batch 108/697\nTokenized train batch 109/697\nTokenized train batch 110/697\nTokenized train batch 111/697\nTokenized train batch 112/697\nTokenized train batch 113/697\nTokenized train batch 114/697\nTokenized train batch 115/697\nTokenized train batch 116/697\nTokenized train batch 117/697\nTokenized train batch 118/697\nTokenized train batch 119/697\nTokenized train batch 120/697\nTokenized train batch 121/697\nTokenized train batch 122/697\nTokenized train batch 123/697\nTokenized train batch 124/697\nTokenized train batch 125/697\nTokenized train batch 126/697\nTokenized train batch 127/697\nTokenized train batch 128/697\nTokenized train batch 129/697\nTokenized train batch 130/697\nTokenized train batch 131/697\nTokenized train batch 132/697\nTokenized train batch 133/697\nTokenized train batch 134/697\nTokenized train batch 135/697\nTokenized train batch 136/697\nTokenized train batch 137/697\nTokenized train batch 138/697\nTokenized train batch 139/697\nTokenized train batch 140/697\nTokenized train batch 141/697\nTokenized train batch 142/697\nTokenized train batch 143/697\nTokenized train batch 144/697\nTokenized train batch 145/697\nTokenized train batch 146/697\nTokenized train batch 147/697\nTokenized train batch 148/697\nTokenized train batch 149/697\nTokenized train batch 150/697\nTokenized train batch 151/697\nTokenized train batch 152/697\nTokenized train batch 153/697\nTokenized train batch 154/697\nTokenized train batch 155/697\nTokenized train batch 156/697\nTokenized train batch 157/697\nTokenized train batch 158/697\nTokenized train batch 159/697\nTokenized train batch 160/697\nTokenized train batch 161/697\nTokenized train batch 162/697\nTokenized train batch 163/697\nTokenized train batch 164/697\nTokenized train batch 165/697\nTokenized train batch 166/697\nTokenized train batch 167/697\nTokenized train batch 168/697\nTokenized train batch 169/697\nTokenized train batch 170/697\nTokenized train batch 171/697\nTokenized train batch 172/697\nTokenized train batch 173/697\nTokenized train batch 174/697\nTokenized train batch 175/697\nTokenized train batch 176/697\nTokenized train batch 177/697\nTokenized train batch 178/697\nTokenized train batch 179/697\nTokenized train batch 180/697\nTokenized train batch 181/697\nTokenized train batch 182/697\nTokenized train batch 183/697\nTokenized train batch 184/697\nTokenized train batch 185/697\nTokenized train batch 186/697\nTokenized train batch 187/697\nTokenized train batch 188/697\nTokenized train batch 189/697\nTokenized train batch 190/697\nTokenized train batch 191/697\nTokenized train batch 192/697\nTokenized train batch 193/697\nTokenized train batch 194/697\nTokenized train batch 195/697\nTokenized train batch 196/697\nTokenized train batch 197/697\nTokenized train batch 198/697\nTokenized train batch 199/697\nTokenized train batch 200/697\nTokenized train batch 201/697\nTokenized train batch 202/697\nTokenized train batch 203/697\nTokenized train batch 204/697\nTokenized train batch 205/697\nTokenized train batch 206/697\nTokenized train batch 207/697\nTokenized train batch 208/697\nTokenized train batch 209/697\nTokenized train batch 210/697\nTokenized train batch 211/697\nTokenized train batch 212/697\nTokenized train batch 213/697\nTokenized train batch 214/697\nTokenized train batch 215/697\nTokenized train batch 216/697\nTokenized train batch 217/697\nTokenized train batch 218/697\nTokenized train batch 219/697\nTokenized train batch 220/697\nTokenized train batch 221/697\nTokenized train batch 222/697\nTokenized train batch 223/697\nTokenized train batch 224/697\nTokenized train batch 225/697\nTokenized train batch 226/697\nTokenized train batch 227/697\nTokenized train batch 228/697\nTokenized train batch 229/697\nTokenized train batch 230/697\nTokenized train batch 231/697\nTokenized train batch 232/697\nTokenized train batch 233/697\nTokenized train batch 234/697\nTokenized train batch 235/697\nTokenized train batch 236/697\nTokenized train batch 237/697\nTokenized train batch 238/697\nTokenized train batch 239/697\nTokenized train batch 240/697\nTokenized train batch 241/697\nTokenized train batch 242/697\nTokenized train batch 243/697\nTokenized train batch 244/697\nTokenized train batch 245/697\nTokenized train batch 246/697\nTokenized train batch 247/697\nTokenized train batch 248/697\nTokenized train batch 249/697\nTokenized train batch 250/697\nTokenized train batch 251/697\nTokenized train batch 252/697\nTokenized train batch 253/697\nTokenized train batch 254/697\nTokenized train batch 255/697\nTokenized train batch 256/697\nTokenized train batch 257/697\nTokenized train batch 258/697\nTokenized train batch 259/697\nTokenized train batch 260/697\nTokenized train batch 261/697\nTokenized train batch 262/697\nTokenized train batch 263/697\nTokenized train batch 264/697\nTokenized train batch 265/697\nTokenized train batch 266/697\nTokenized train batch 267/697\nTokenized train batch 268/697\nTokenized train batch 269/697\nTokenized train batch 270/697\nTokenized train batch 271/697\nTokenized train batch 272/697\nTokenized train batch 273/697\nTokenized train batch 274/697\nTokenized train batch 275/697\nTokenized train batch 276/697\nTokenized train batch 277/697\nTokenized train batch 278/697\nTokenized train batch 279/697\nTokenized train batch 280/697\nTokenized train batch 281/697\nTokenized train batch 282/697\nTokenized train batch 283/697\nTokenized train batch 284/697\nTokenized train batch 285/697\nTokenized train batch 286/697\nTokenized train batch 287/697\nTokenized train batch 288/697\nTokenized train batch 289/697\nTokenized train batch 290/697\nTokenized train batch 291/697\nTokenized train batch 292/697\nTokenized train batch 293/697\nTokenized train batch 294/697\nTokenized train batch 295/697\nTokenized train batch 296/697\nTokenized train batch 297/697\nTokenized train batch 298/697\nTokenized train batch 299/697\nTokenized train batch 300/697\nTokenized train batch 301/697\nTokenized train batch 302/697\nTokenized train batch 303/697\nTokenized train batch 304/697\nTokenized train batch 305/697\nTokenized train batch 306/697\nTokenized train batch 307/697\nTokenized train batch 308/697\nTokenized train batch 309/697\nTokenized train batch 310/697\nTokenized train batch 311/697\nTokenized train batch 312/697\nTokenized train batch 313/697\nTokenized train batch 314/697\nTokenized train batch 315/697\nTokenized train batch 316/697\nTokenized train batch 317/697\nTokenized train batch 318/697\nTokenized train batch 319/697\nTokenized train batch 320/697\nTokenized train batch 321/697\nTokenized train batch 322/697\nTokenized train batch 323/697\nTokenized train batch 324/697\nTokenized train batch 325/697\nTokenized train batch 326/697\nTokenized train batch 327/697\nTokenized train batch 328/697\nTokenized train batch 329/697\nTokenized train batch 330/697\nTokenized train batch 331/697\nTokenized train batch 332/697\nTokenized train batch 333/697\nTokenized train batch 334/697\nTokenized train batch 335/697\nTokenized train batch 336/697\nTokenized train batch 337/697\nTokenized train batch 338/697\nTokenized train batch 339/697\nTokenized train batch 340/697\nTokenized train batch 341/697\nTokenized train batch 342/697\nTokenized train batch 343/697\nTokenized train batch 344/697\nTokenized train batch 345/697\nTokenized train batch 346/697\nTokenized train batch 347/697\nTokenized train batch 348/697\nTokenized train batch 349/697\nTokenized train batch 350/697\nTokenized train batch 351/697\nTokenized train batch 352/697\nTokenized train batch 353/697\nTokenized train batch 354/697\nTokenized train batch 355/697\nTokenized train batch 356/697\nTokenized train batch 357/697\nTokenized train batch 358/697\nTokenized train batch 359/697\nTokenized train batch 360/697\nTokenized train batch 361/697\nTokenized train batch 362/697\nTokenized train batch 363/697\nTokenized train batch 364/697\nTokenized train batch 365/697\nTokenized train batch 366/697\nTokenized train batch 367/697\nTokenized train batch 368/697\nTokenized train batch 369/697\nTokenized train batch 370/697\nTokenized train batch 371/697\nTokenized train batch 372/697\nTokenized train batch 373/697\nTokenized train batch 374/697\nTokenized train batch 375/697\nTokenized train batch 376/697\nTokenized train batch 377/697\nTokenized train batch 378/697\nTokenized train batch 379/697\nTokenized train batch 380/697\nTokenized train batch 381/697\nTokenized train batch 382/697\nTokenized train batch 383/697\nTokenized train batch 384/697\nTokenized train batch 385/697\nTokenized train batch 386/697\nTokenized train batch 387/697\nTokenized train batch 388/697\nTokenized train batch 389/697\nTokenized train batch 390/697\nTokenized train batch 391/697\nTokenized train batch 392/697\nTokenized train batch 393/697\nTokenized train batch 394/697\nTokenized train batch 395/697\nTokenized train batch 396/697\nTokenized train batch 397/697\nTokenized train batch 398/697\nTokenized train batch 399/697\nTokenized train batch 400/697\nTokenized train batch 401/697\nTokenized train batch 402/697\nTokenized train batch 403/697\nTokenized train batch 404/697\nTokenized train batch 405/697\nTokenized train batch 406/697\nTokenized train batch 407/697\nTokenized train batch 408/697\nTokenized train batch 409/697\nTokenized train batch 410/697\nTokenized train batch 411/697\nTokenized train batch 412/697\nTokenized train batch 413/697\nTokenized train batch 414/697\nTokenized train batch 415/697\nTokenized train batch 416/697\nTokenized train batch 417/697\nTokenized train batch 418/697\nTokenized train batch 419/697\nTokenized train batch 420/697\nTokenized train batch 421/697\nTokenized train batch 422/697\nTokenized train batch 423/697\nTokenized train batch 424/697\nTokenized train batch 425/697\nTokenized train batch 426/697\nTokenized train batch 427/697\nTokenized train batch 428/697\nTokenized train batch 429/697\nTokenized train batch 430/697\nTokenized train batch 431/697\nTokenized train batch 432/697\nTokenized train batch 433/697\nTokenized train batch 434/697\nTokenized train batch 435/697\nTokenized train batch 436/697\nTokenized train batch 437/697\nTokenized train batch 438/697\nTokenized train batch 439/697\nTokenized train batch 440/697\nTokenized train batch 441/697\nTokenized train batch 442/697\nTokenized train batch 443/697\nTokenized train batch 444/697\nTokenized train batch 445/697\nTokenized train batch 446/697\nTokenized train batch 447/697\nTokenized train batch 448/697\nTokenized train batch 449/697\nTokenized train batch 450/697\nTokenized train batch 451/697\nTokenized train batch 452/697\nTokenized train batch 453/697\nTokenized train batch 454/697\nTokenized train batch 455/697\nTokenized train batch 456/697\nTokenized train batch 457/697\nTokenized train batch 458/697\nTokenized train batch 459/697\nTokenized train batch 460/697\nTokenized train batch 461/697\nTokenized train batch 462/697\nTokenized train batch 463/697\nTokenized train batch 464/697\nTokenized train batch 465/697\nTokenized train batch 466/697\nTokenized train batch 467/697\nTokenized train batch 468/697\nTokenized train batch 469/697\nTokenized train batch 470/697\nTokenized train batch 471/697\nTokenized train batch 472/697\nTokenized train batch 473/697\nTokenized train batch 474/697\nTokenized train batch 475/697\nTokenized train batch 476/697\nTokenized train batch 477/697\nTokenized train batch 478/697\nTokenized train batch 479/697\nTokenized train batch 480/697\nTokenized train batch 481/697\nTokenized train batch 482/697\nTokenized train batch 483/697\nTokenized train batch 484/697\nTokenized train batch 485/697\nTokenized train batch 486/697\nTokenized train batch 487/697\nTokenized train batch 488/697\nTokenized train batch 489/697\nTokenized train batch 490/697\nTokenized train batch 491/697\nTokenized train batch 492/697\nTokenized train batch 493/697\nTokenized train batch 494/697\nTokenized train batch 495/697\nTokenized train batch 496/697\nTokenized train batch 497/697\nTokenized train batch 498/697\nTokenized train batch 499/697\nTokenized train batch 500/697\nTokenized train batch 501/697\nTokenized train batch 502/697\nTokenized train batch 503/697\nTokenized train batch 504/697\nTokenized train batch 505/697\nTokenized train batch 506/697\nTokenized train batch 507/697\nTokenized train batch 508/697\nTokenized train batch 509/697\nTokenized train batch 510/697\nTokenized train batch 511/697\nTokenized train batch 512/697\nTokenized train batch 513/697\nTokenized train batch 514/697\nTokenized train batch 515/697\nTokenized train batch 516/697\nTokenized train batch 517/697\nTokenized train batch 518/697\nTokenized train batch 519/697\nTokenized train batch 520/697\nTokenized train batch 521/697\nTokenized train batch 522/697\nTokenized train batch 523/697\nTokenized train batch 524/697\nTokenized train batch 525/697\nTokenized train batch 526/697\nTokenized train batch 527/697\nTokenized train batch 528/697\nTokenized train batch 529/697\nTokenized train batch 530/697\nTokenized train batch 531/697\nTokenized train batch 532/697\nTokenized train batch 533/697\nTokenized train batch 534/697\nTokenized train batch 535/697\nTokenized train batch 536/697\nTokenized train batch 537/697\nTokenized train batch 538/697\nTokenized train batch 539/697\nTokenized train batch 540/697\nTokenized train batch 541/697\nTokenized train batch 542/697\nTokenized train batch 543/697\nTokenized train batch 544/697\nTokenized train batch 545/697\nTokenized train batch 546/697\nTokenized train batch 547/697\nTokenized train batch 548/697\nTokenized train batch 549/697\nTokenized train batch 550/697\nTokenized train batch 551/697\nTokenized train batch 552/697\nTokenized train batch 553/697\nTokenized train batch 554/697\nTokenized train batch 555/697\nTokenized train batch 556/697\nTokenized train batch 557/697\nTokenized train batch 558/697\nTokenized train batch 559/697\nTokenized train batch 560/697\nTokenized train batch 561/697\nTokenized train batch 562/697\nTokenized train batch 563/697\nTokenized train batch 564/697\nTokenized train batch 565/697\nTokenized train batch 566/697\nTokenized train batch 567/697\nTokenized train batch 568/697\nTokenized train batch 569/697\nTokenized train batch 570/697\nTokenized train batch 571/697\nTokenized train batch 572/697\nTokenized train batch 573/697\nTokenized train batch 574/697\nTokenized train batch 575/697\nTokenized train batch 576/697\nTokenized train batch 577/697\nTokenized train batch 578/697\nTokenized train batch 579/697\nTokenized train batch 580/697\nTokenized train batch 581/697\nTokenized train batch 582/697\nTokenized train batch 583/697\nTokenized train batch 584/697\nTokenized train batch 585/697\nTokenized train batch 586/697\nTokenized train batch 587/697\nTokenized train batch 588/697\nTokenized train batch 589/697\nTokenized train batch 590/697\nTokenized train batch 591/697\nTokenized train batch 592/697\nTokenized train batch 593/697\nTokenized train batch 594/697\nTokenized train batch 595/697\nTokenized train batch 596/697\nTokenized train batch 597/697\nTokenized train batch 598/697\nTokenized train batch 599/697\nTokenized train batch 600/697\nTokenized train batch 601/697\nTokenized train batch 602/697\nTokenized train batch 603/697\nTokenized train batch 604/697\nTokenized train batch 605/697\nTokenized train batch 606/697\nTokenized train batch 607/697\nTokenized train batch 608/697\nTokenized train batch 609/697\nTokenized train batch 610/697\nTokenized train batch 611/697\nTokenized train batch 612/697\nTokenized train batch 613/697\nTokenized train batch 614/697\nTokenized train batch 615/697\nTokenized train batch 616/697\nTokenized train batch 617/697\nTokenized train batch 618/697\nTokenized train batch 619/697\nTokenized train batch 620/697\nTokenized train batch 621/697\nTokenized train batch 622/697\nTokenized train batch 623/697\nTokenized train batch 624/697\nTokenized train batch 625/697\nTokenized train batch 626/697\nTokenized train batch 627/697\nTokenized train batch 628/697\nTokenized train batch 629/697\nTokenized train batch 630/697\nTokenized train batch 631/697\nTokenized train batch 632/697\nTokenized train batch 633/697\nTokenized train batch 634/697\nTokenized train batch 635/697\nTokenized train batch 636/697\nTokenized train batch 637/697\nTokenized train batch 638/697\nTokenized train batch 639/697\nTokenized train batch 640/697\nTokenized train batch 641/697\nTokenized train batch 642/697\nTokenized train batch 643/697\nTokenized train batch 644/697\nTokenized train batch 645/697\nTokenized train batch 646/697\nTokenized train batch 647/697\nTokenized train batch 648/697\nTokenized train batch 649/697\nTokenized train batch 650/697\nTokenized train batch 651/697\nTokenized train batch 652/697\nTokenized train batch 653/697\nTokenized train batch 654/697\nTokenized train batch 655/697\nTokenized train batch 656/697\nTokenized train batch 657/697\nTokenized train batch 658/697\nTokenized train batch 659/697\nTokenized train batch 660/697\nTokenized train batch 661/697\nTokenized train batch 662/697\nTokenized train batch 663/697\nTokenized train batch 664/697\nTokenized train batch 665/697\nTokenized train batch 666/697\nTokenized train batch 667/697\nTokenized train batch 668/697\nTokenized train batch 669/697\nTokenized train batch 670/697\nTokenized train batch 671/697\nTokenized train batch 672/697\nTokenized train batch 673/697\nTokenized train batch 674/697\nTokenized train batch 675/697\nTokenized train batch 676/697\nTokenized train batch 677/697\nTokenized train batch 678/697\nTokenized train batch 679/697\nTokenized train batch 680/697\nTokenized train batch 681/697\nTokenized train batch 682/697\nTokenized train batch 683/697\nTokenized train batch 684/697\nTokenized train batch 685/697\nTokenized train batch 686/697\nTokenized train batch 687/697\nTokenized train batch 688/697\nTokenized train batch 689/697\nTokenized train batch 690/697\nTokenized train batch 691/697\nTokenized train batch 692/697\nTokenized train batch 693/697\nTokenized train batch 694/697\nTokenized train batch 695/697\nTokenized train batch 696/697\nTokenizing validation data...\nTokenized val batch 1/88\nTokenized val batch 2/88\nTokenized val batch 3/88\nTokenized val batch 4/88\nTokenized val batch 5/88\nTokenized val batch 6/88\nTokenized val batch 7/88\nTokenized val batch 8/88\nTokenized val batch 9/88\nTokenized val batch 10/88\nTokenized val batch 11/88\nTokenized val batch 12/88\nTokenized val batch 13/88\nTokenized val batch 14/88\nTokenized val batch 15/88\nTokenized val batch 16/88\nTokenized val batch 17/88\nTokenized val batch 18/88\nTokenized val batch 19/88\nTokenized val batch 20/88\nTokenized val batch 21/88\nTokenized val batch 22/88\nTokenized val batch 23/88\nTokenized val batch 24/88\nTokenized val batch 25/88\nTokenized val batch 26/88\nTokenized val batch 27/88\nTokenized val batch 28/88\nTokenized val batch 29/88\nTokenized val batch 30/88\nTokenized val batch 31/88\nTokenized val batch 32/88\nTokenized val batch 33/88\nTokenized val batch 34/88\nTokenized val batch 35/88\nTokenized val batch 36/88\nTokenized val batch 37/88\nTokenized val batch 38/88\nTokenized val batch 39/88\nTokenized val batch 40/88\nTokenized val batch 41/88\nTokenized val batch 42/88\nTokenized val batch 43/88\nTokenized val batch 44/88\nTokenized val batch 45/88\nTokenized val batch 46/88\nTokenized val batch 47/88\nTokenized val batch 48/88\nTokenized val batch 49/88\nTokenized val batch 50/88\nTokenized val batch 51/88\nTokenized val batch 52/88\nTokenized val batch 53/88\nTokenized val batch 54/88\nTokenized val batch 55/88\nTokenized val batch 56/88\nTokenized val batch 57/88\nTokenized val batch 58/88\nTokenized val batch 59/88\nTokenized val batch 60/88\nTokenized val batch 61/88\nTokenized val batch 62/88\nTokenized val batch 63/88\nTokenized val batch 64/88\nTokenized val batch 65/88\nTokenized val batch 66/88\nTokenized val batch 67/88\nTokenized val batch 68/88\nTokenized val batch 69/88\nTokenized val batch 70/88\nTokenized val batch 71/88\nTokenized val batch 72/88\nTokenized val batch 73/88\nTokenized val batch 74/88\nTokenized val batch 75/88\nTokenized val batch 76/88\nTokenized val batch 77/88\nTokenized val batch 78/88\nTokenized val batch 79/88\nTokenized val batch 80/88\nTokenized val batch 81/88\nTokenized val batch 82/88\nTokenized val batch 83/88\nTokenized val batch 84/88\nTokenized val batch 85/88\nTokenized val batch 86/88\nTokenized val batch 87/88\nSaving tokenized datasets to /kaggle/working/...\nTrain tokenized: 696 batches, Val tokenized: 87 batches\nStep 2 complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport torch\nimport gc\nimport glob\nimport re\nimport shutil\nimport logging\nfrom datasets import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    BertTokenizerFast,\n    BertForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# Setup\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/training_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# File paths\ninput_dir = '/kaggle/working/'\ntrain_tokenized_path = os.path.join(input_dir, 'train_tokenized.pkl')\nval_tokenized_path = os.path.join(input_dir, 'val_tokenized.pkl')\nresults_dir = os.path.join(input_dir, 'results')\nos.makedirs(results_dir, exist_ok=True)\n\n# Tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Load tokenized data\nwith open(train_tokenized_path, 'rb') as f:\n    train_tokenized = pickle.load(f)\nwith open(val_tokenized_path, 'rb') as f:\n    val_tokenized = pickle.load(f)\n\n# Convert tokenized data to Huggingface Dataset\ndef convert_to_dataset(tokenized_data):\n    input_ids, attention_mask, labels = [], [], []\n    for batch in tokenized_data:\n        input_ids.extend(batch['input_ids'])\n        attention_mask.extend(batch['attention_mask'])\n        batch_labels = batch['labels'].tolist() if isinstance(batch['labels'], torch.Tensor) else batch['labels']\n        labels.extend(batch_labels)\n    dataset = Dataset.from_dict({\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'labels': labels\n    })\n    return dataset.map(lambda x: {'labels': int(x['labels'])})\n\n# Prepare datasets\ntrain_dataset = convert_to_dataset(train_tokenized)\nval_dataset = convert_to_dataset(val_tokenized)\ndel train_tokenized, val_tokenized\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Class weights\ntrain_labels = np.array(train_dataset['labels'])\nnum_labels = len(np.unique(train_labels))\nclass_weights_tensor = torch.tensor(\n    compute_class_weight(class_weight='balanced', classes=np.arange(num_labels), y=train_labels),\n    dtype=torch.float\n)\n\n# Load model\ncheckpoint_dirs = glob.glob(os.path.join(results_dir, 'checkpoint-*'))\ncheckpoint_path = max(checkpoint_dirs, key=os.path.getmtime) if checkpoint_dirs else None\nmodel = BertForSequenceClassification.from_pretrained(\n    checkpoint_path if checkpoint_path else 'bert-base-uncased',\n    num_labels=num_labels\n)\nmodel.to(torch.device('cuda'))\n\n# Custom trainer with class weights\nclass WeightedTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=results_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir=os.path.join(input_dir, 'logs'),\n    logging_steps=10,\n    save_steps=15000,\n    eval_strategy='epoch',\n    eval_steps='epoch',\n    save_total_limit=2,\n    fp16=True,\n    report_to='none'\n)\n\n# Train\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=lambda pred: {'accuracy': (np.argmax(pred.predictions, axis=1) == pred.label_ids).mean()},\n    data_collator=DataCollatorWithPadding(tokenizer),\n    class_weights=class_weights_tensor\n)\n\nlogger.info(\"Starting training...\")\ntrainer.train(resume_from_checkpoint=checkpoint_path if checkpoint_path else None)\n\n# Save final model\nfinal_model_path = os.path.join(input_dir, 'final_model')\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\n# Evaluate\nmetrics = trainer.evaluate()\npd.DataFrame([metrics]).to_csv(os.path.join(input_dir, \"final_eval_metrics.csv\"), index=False)\nlogger.info(f\"Training completed. Model saved to {final_model_path}. Metrics saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T10:04:10.409719Z","iopub.execute_input":"2025-05-10T10:04:10.410011Z","iopub.status.idle":"2025-05-10T10:04:32.268886Z","shell.execute_reply.started":"2025-05-10T10:04:10.409988Z","shell.execute_reply":"2025-05-10T10:04:32.267943Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/69600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac05c6670104402ca4fde70de19519e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7103424f8ce04eaa9cde751bab241530"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\",\n    \"10\": \"LABEL_10\",\n    \"11\": \"LABEL_11\",\n    \"12\": \"LABEL_12\",\n    \"13\": \"LABEL_13\",\n    \"14\": \"LABEL_14\",\n    \"15\": \"LABEL_15\",\n    \"16\": \"LABEL_16\",\n    \"17\": \"LABEL_17\",\n    \"18\": \"LABEL_18\",\n    \"19\": \"LABEL_19\",\n    \"20\": \"LABEL_20\",\n    \"21\": \"LABEL_21\",\n    \"22\": \"LABEL_22\",\n    \"23\": \"LABEL_23\",\n    \"24\": \"LABEL_24\",\n    \"25\": \"LABEL_25\",\n    \"26\": \"LABEL_26\",\n    \"27\": \"LABEL_27\",\n    \"28\": \"LABEL_28\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_10\": 10,\n    \"LABEL_11\": 11,\n    \"LABEL_12\": 12,\n    \"LABEL_13\": 13,\n    \"LABEL_14\": 14,\n    \"LABEL_15\": 15,\n    \"LABEL_16\": 16,\n    \"LABEL_17\": 17,\n    \"LABEL_18\": 18,\n    \"LABEL_19\": 19,\n    \"LABEL_2\": 2,\n    \"LABEL_20\": 20,\n    \"LABEL_21\": 21,\n    \"LABEL_22\": 22,\n    \"LABEL_23\": 23,\n    \"LABEL_24\": 24,\n    \"LABEL_25\": 25,\n    \"LABEL_26\": 26,\n    \"LABEL_27\": 27,\n    \"LABEL_28\": 28,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_30/4223538214.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m )\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Custom trainer with class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3697\u001b[0m                 )\n\u001b[0;32m-> 3698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}