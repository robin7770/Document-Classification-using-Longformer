{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11768414,"sourceType":"datasetVersion","datasetId":7388168}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom collections import defaultdict\nimport logging\nimport gc\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/math_papers_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Define all math subcategories (32 subcategories)\nSELECTED_SUBCATEGORIES = [\n    'math.AC', 'math.AG', 'math.AP', 'math.AT', 'math.CA',\n    'math.CO', 'math.CT', 'math.CV', 'math.DG', 'math.DS',\n    'math.FA', 'math.GM', 'math.GN', 'math.GR', 'math.GT',\n    'math.HO', 'math.IT', 'math.KT', 'math.LO', 'math.MG',\n    'math.MP', 'math.NA', 'math.NT', 'math.OA', 'math.OC',\n    'math.PR', 'math.QA', 'math.RA', 'math.RT', 'math.SG',\n    'math.SP', 'math.ST'\n]\n\n# Map deprecated subcategories\nDEPRECATED_MAP = {\n    'q-alg': 'math.QA',\n    'alg-geom': 'math.AG',\n    'dg-ga': 'math.DG',\n    'funct-an': 'math.FA'\n}\n\n# Define main category\nMAIN_CATEGORY = 'math'\n\n# Output paths\nOUTPUT_DIR = '/kaggle/working/'\nCOUNTS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, 'math_papers_per_category.csv')\n\n# Verify output directory\nlogger.info(f\"Checking output directory: {OUTPUT_DIR}\")\nif not os.path.exists(OUTPUT_DIR):\n    try:\n        os.makedirs(OUTPUT_DIR)\n        logger.info(f\"Created output directory: {OUTPUT_DIR}\")\n    except Exception as e:\n        logger.error(f\"Failed to create output directory {OUTPUT_DIR}: {e}\")\n        raise\nif not os.access(OUTPUT_DIR, os.W_OK):\n    logger.error(f\"Output directory {OUTPUT_DIR} is not writable\")\n    raise PermissionError(f\"Output directory {OUTPUT_DIR} is not writable\")\n\n# Path to input CSV\ncsv_path = '/kaggle/input/mathematics/math_papers_3000_per_category.csv'\n\n# Verify file exists\nlogger.info(f\"Checking for CSV file at {csv_path}...\")\nif not os.path.exists(csv_path):\n    logger.error(f\"CSV file not found: {csv_path}\")\n    raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n\n# Initialize storage\npaper_counts = defaultdict(int)  # Track paper counts per subcategory\n\n# Process CSV in chunks\nlogger.info(\"Processing CSV file for math subcategories...\")\ntry:\n    chunk_size = 10000  # Balanced for ~100–500 MB CSV\n    total_papers = 0\n    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n        for _, row in chunk.iterrows():\n            categories = row['categories']\n            subcategory = row['subcategory']\n            if pd.isna(categories) or pd.isna(subcategory):\n                continue\n            # Split and map categories\n            subcategories = [DEPRECATED_MAP.get(subcat, subcat) for subcat in categories.split()]\n            # Ensure all subcategories are math-related\n            is_math_only = all(\n                subcat.startswith('math.') or subcat in DEPRECATED_MAP.values()\n                for subcat in subcategories\n            )\n            if is_math_only and subcategory in SELECTED_SUBCATEGORIES:\n                paper_counts[subcategory] += 1\n        total_papers += len(chunk)\n        logger.info(f\"Processed {total_papers} papers...\")\n        # Clear memory\n        del chunk\n        gc.collect()\nexcept Exception as e:\n    logger.error(f\"Error processing CSV: {e}\")\n    raise\n\n# Prepare counts for CSV\ncounts_data = [\n    {\n        'Main_Category': MAIN_CATEGORY,\n        'Subcategory': subcat,\n        'Paper_Count': paper_counts[subcat]\n    }\n    for subcat in SELECTED_SUBCATEGORIES\n]\ncounts_df = pd.DataFrame(counts_data)\n\n# Sort by Subcategory\ncounts_df = counts_df.sort_values('Subcategory')\n\n# Save counts to CSV\nlogger.info(f\"Saving math papers per category to {COUNTS_OUTPUT_PATH}...\")\ntry:\n    counts_df.to_csv(COUNTS_OUTPUT_PATH, index=False)\n    # Verify file was saved\n    if os.path.exists(COUNTS_OUTPUT_PATH):\n        logger.info(f\"Math papers per category successfully saved to {COUNTS_OUTPUT_PATH}\")\n        logger.info(f\"File size: {os.path.getsize(COUNTS_OUTPUT_PATH)} bytes\")\n    else:\n        logger.error(f\"Failed to verify saved file: {COUNTS_OUTPUT_PATH}\")\n        raise FileNotFoundError(f\"Failed to verify saved file: {COUNTS_OUTPUT_PATH}\")\nexcept Exception as e:\n    logger.error(f\"Error saving counts CSV to {COUNTS_OUTPUT_PATH}: {e}\")\n    raise\n\n# Print counts\nprint(\"\\nMath Papers Per Category:\")\nprint(counts_df)\nlogger.info(\"\\nMath Papers Per Category:\\n\" + counts_df.to_string())\n\n# Print summary\nprint(f\"\\nTotal Subcategories: {len(SELECTED_SUBCATEGORIES)}\")\nprint(f\"Total Papers Counted: {counts_df['Paper_Count'].sum()}\")\nlogger.info(f\"\\nTotal Subcategories: {len(SELECTED_SUBCATEGORIES)}\")\nlogger.info(f\"Total Papers Counted: {counts_df['Paper_Count'].sum()}\")\n\nlogger.info(\"Processing complete.\")\nprint(\"Processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:08:11.255362Z","iopub.execute_input":"2025-05-11T12:08:11.255550Z","iopub.status.idle":"2025-05-11T12:08:17.086044Z","shell.execute_reply.started":"2025-05-11T12:08:11.255531Z","shell.execute_reply":"2025-05-11T12:08:17.085270Z"}},"outputs":[{"name":"stdout","text":"\nMath Papers Per Category:\n   Main_Category Subcategory  Paper_Count\n0           math     math.AC         3000\n1           math     math.AG         3000\n2           math     math.AP         3000\n3           math     math.AT         3000\n4           math     math.CA         3000\n5           math     math.CO         3000\n6           math     math.CT         3000\n7           math     math.CV         3000\n8           math     math.DG         3000\n9           math     math.DS         3000\n10          math     math.FA         3000\n11          math     math.GM         3000\n12          math     math.GN         3000\n13          math     math.GR         3000\n14          math     math.GT         3000\n15          math     math.HO         3000\n16          math     math.IT            0\n17          math     math.KT         3000\n18          math     math.LO         3000\n19          math     math.MG         3000\n20          math     math.MP            0\n21          math     math.NA         3000\n22          math     math.NT         3000\n23          math     math.OA         3000\n24          math     math.OC         3000\n25          math     math.PR         3000\n26          math     math.QA         3000\n27          math     math.RA         3000\n28          math     math.RT         3000\n29          math     math.SG         3000\n30          math     math.SP         3000\n31          math     math.ST            0\n\nTotal Subcategories: 32\nTotal Papers Counted: 87000\nProcessing complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom collections import defaultdict\nimport logging\nimport gc\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/math_papers_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Define math subcategories (29 subcategories, excluding math.ST, math.MP, math.IT)\nSELECTED_SUBCATEGORIES = [\n    'math.AC', 'math.AG', 'math.AP', 'math.AT', 'math.CA',\n    'math.CO', 'math.CT', 'math.CV', 'math.DG', 'math.DS',\n    'math.FA', 'math.GM', 'math.GN', 'math.GR', 'math.GT',\n    'math.HO', 'math.KT', 'math.LO', 'math.MG', 'math.NA',\n    'math.NT', 'math.OA', 'math.OC', 'math.PR', 'math.QA',\n    'math.RA', 'math.RT', 'math.SG', 'math.SP'\n]\n\n# Map deprecated subcategories\nDEPRECATED_MAP = {\n    'q-alg': 'math.QA',\n    'alg-geom': 'math.AG',\n    'dg-ga': 'math.DG',\n    'funct-an': 'math.FA'\n}\n\n# Define main category\nMAIN_CATEGORY = 'math'\n\n# Output paths\nOUTPUT_DIR = '/kaggle/working/'\nCOUNTS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, 'math_papers_per_category.csv')\n\n# Verify output directory\nlogger.info(f\"Checking output directory: {OUTPUT_DIR}\")\nif not os.path.exists(OUTPUT_DIR):\n    try:\n        os.makedirs(OUTPUT_DIR)\n        logger.info(f\"Created output directory: {OUTPUT_DIR}\")\n    except Exception as e:\n        logger.error(f\"Failed to create output directory {OUTPUT_DIR}: {e}\")\n        raise\nif not os.access(OUTPUT_DIR, os.W_OK):\n    logger.error(f\"Output directory {OUTPUT_DIR} is not writable\")\n    raise PermissionError(f\"Output directory {OUTPUT_DIR} is not writable\")\n\n# Path to input CSV\ncsv_path = '/kaggle/input/mathematics/math_papers_3000_per_category.csv'\n\n# Verify file exists\nlogger.info(f\"Checking for CSV file at {csv_path}...\")\nif not os.path.exists(csv_path):\n    logger.error(f\"CSV file not found: {csv_path}\")\n    raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n\n# Initialize storage\npaper_counts = defaultdict(int)  # Track paper counts per subcategory\n\n# Process CSV in chunks\nlogger.info(\"Processing CSV file for math subcategories...\")\ntry:\n    chunk_size = 10000  # Balanced for ~100–500 MB CSV\n    total_papers = 0\n    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n        for _, row in chunk.iterrows():\n            categories = row['categories']\n            subcategory = row['subcategory']\n            if pd.isna(categories) or pd.isna(subcategory):\n                continue\n            # Split and map categories\n            subcategories = [DEPRECATED_MAP.get(subcat, subcat) for subcat in categories.split()]\n            # Ensure all subcategories are math-related\n            is_math_only = all(\n                subcat.startswith('math.') or subcat in DEPRECATED_MAP.values()\n                for subcat in subcategories\n            )\n            if is_math_only and subcategory in SELECTED_SUBCATEGORIES:\n                paper_counts[subcategory] += 1\n        total_papers += len(chunk)\n        logger.info(f\"Processed {total_papers} papers...\")\n        # Clear memory\n        del chunk\n        gc.collect()\nexcept Exception as e:\n    logger.error(f\"Error processing CSV: {e}\")\n    raise\n\n# Prepare counts for CSV\ncounts_data = [\n    {\n        'Main_Category': MAIN_CATEGORY,\n        'Subcategory': subcat,\n        'Paper_Count': paper_counts[subcat]\n    }\n    for subcat in SELECTED_SUBCATEGORIES\n]\ncounts_df = pd.DataFrame(counts_data)\n\n# Sort by Subcategory\ncounts_df = counts_df.sort_values('Subcategory')\n\n# Save counts to CSV\nlogger.info(f\"Saving math papers per category to {COUNTS_OUTPUT_PATH}...\")\ntry:\n    counts_df.to_csv(COUNTS_OUTPUT_PATH, index=False)\n    # Verify file was saved\n    if os.path.exists(COUNTS_OUTPUT_PATH):\n        logger.info(f\"Math papers per category successfully saved to {COUNTS_OUTPUT_PATH}\")\n        logger.info(f\"File size: {os.path.getsize(COUNTS_OUTPUT_PATH)} bytes\")\n    else:\n        logger.error(f\"Failed to verify saved file: {COUNTS_OUTPUT_PATH}\")\n        raise FileNotFoundError(f\"Failed to verify saved file: {COUNTS_OUTPUT_PATH}\")\nexcept Exception as e:\n    logger.error(f\"Error saving counts CSV to {COUNTS_OUTPUT_PATH}: {e}\")\n    raise\n\n# Print counts\nprint(\"\\nMath Papers Per Category:\")\nprint(counts_df)\nlogger.info(\"\\nMath Papers Per Category:\\n\" + counts_df.to_string())\n\n# Print summary\nprint(f\"\\nTotal Subcategories: {len(SELECTED_SUBCATEGORIES)}\")\nprint(f\"Total Papers Counted: {counts_df['Paper_Count'].sum()}\")\nlogger.info(f\"\\nTotal Subcategories: {len(SELECTED_SUBCATEGORIES)}\")\nlogger.info(f\"Total Papers Counted: {counts_df['Paper_Count'].sum()}\")\n\nlogger.info(\"Processing complete.\")\nprint(\"Processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:08:17.087732Z","iopub.execute_input":"2025-05-11T12:08:17.087920Z","iopub.status.idle":"2025-05-11T12:08:21.768731Z","shell.execute_reply.started":"2025-05-11T12:08:17.087906Z","shell.execute_reply":"2025-05-11T12:08:21.768015Z"}},"outputs":[{"name":"stdout","text":"\nMath Papers Per Category:\n   Main_Category Subcategory  Paper_Count\n0           math     math.AC         3000\n1           math     math.AG         3000\n2           math     math.AP         3000\n3           math     math.AT         3000\n4           math     math.CA         3000\n5           math     math.CO         3000\n6           math     math.CT         3000\n7           math     math.CV         3000\n8           math     math.DG         3000\n9           math     math.DS         3000\n10          math     math.FA         3000\n11          math     math.GM         3000\n12          math     math.GN         3000\n13          math     math.GR         3000\n14          math     math.GT         3000\n15          math     math.HO         3000\n16          math     math.KT         3000\n17          math     math.LO         3000\n18          math     math.MG         3000\n19          math     math.NA         3000\n20          math     math.NT         3000\n21          math     math.OA         3000\n22          math     math.OC         3000\n23          math     math.PR         3000\n24          math     math.QA         3000\n25          math     math.RA         3000\n26          math     math.RT         3000\n27          math     math.SG         3000\n28          math     math.SP         3000\n\nTotal Subcategories: 29\nTotal Papers Counted: 87000\nProcessing complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport logging\nimport gc\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/math_papers_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Define math subcategories (29 subcategories, excluding math.ST, math.MP, math.IT)\nSELECTED_SUBCATEGORIES = [\n    'math.AC', 'math.AG', 'math.AP', 'math.AT', 'math.CA',\n    'math.CO', 'math.CT', 'math.CV', 'math.DG', 'math.DS',\n    'math.FA', 'math.GM', 'math.GN', 'math.GR', 'math.GT',\n    'math.HO', 'math.KT', 'math.LO', 'math.MG', 'math.NA',\n    'math.NT', 'math.OA', 'math.OC', 'math.PR', 'math.QA',\n    'math.RA', 'math.RT', 'math.SG', 'math.SP'\n]\n\n# Map deprecated subcategories\nDEPRECATED_MAP = {\n    'q-alg': 'math.QA',\n    'alg-geom': 'math.AG',\n    'dg-ga': 'math.DG',\n    'funct-an': 'math.FA'\n}\n\n# Output paths\nOUTPUT_DIR = '/kaggle/working/'\nOUTPUT_PATH = os.path.join(OUTPUT_DIR, 'math_papers_title_abstract.csv')\n\n# Verify output directory\nlogger.info(f\"Checking output directory: {OUTPUT_DIR}\")\nif not os.path.exists(OUTPUT_DIR):\n    try:\n        os.makedirs(OUTPUT_DIR)\n        logger.info(f\"Created output directory: {OUTPUT_DIR}\")\n    except Exception as e:\n        logger.error(f\"Failed to create output directory {OUTPUT_DIR}: {e}\")\n        raise\nif not os.access(OUTPUT_DIR, os.W_OK):\n    logger.error(f\"Output directory {OUTPUT_DIR} is not writable\")\n    raise PermissionError(f\"Output directory {OUTPUT_DIR} is not writable\")\n\n# Path to input CSV\ncsv_path = '/kaggle/input/mathematics/math_papers_3000_per_category.csv'\n\n# Verify file exists\nlogger.info(f\"Checking for CSV file at {csv_path}...\")\nif not os.path.exists(csv_path):\n    logger.error(f\"CSV file not found: {csv_path}\")\n    raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n\n# Initialize storage for processed data\nprocessed_papers = []\n\n# Process CSV in chunks\nlogger.info(\"Processing CSV file for math papers...\")\ntry:\n    chunk_size = 10000  # Balanced for ~100–500 MB CSV\n    total_papers = 0\n    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n        # Filter for selected subcategories\n        chunk = chunk[chunk['subcategory'].isin(SELECTED_SUBCATEGORIES)]\n        if chunk.empty:\n            continue\n        # Verify math-only categories\n        chunk['is_math_only'] = chunk['categories'].apply(\n            lambda x: all(\n                DEPRECATED_MAP.get(subcat, subcat).startswith('math.') or\n                DEPRECATED_MAP.get(subcat, subcat) in DEPRECATED_MAP.values()\n                for subcat in str(x).split()\n            ) if pd.notna(x) else False\n        )\n        chunk = chunk[chunk['is_math_only']]\n        if chunk.empty:\n            continue\n        # Combine title and abstract\n        chunk['title_abstract'] = chunk.apply(\n            lambda x: (str(x['title']) + ' ' + str(x['abstract'])).strip()\n            if pd.notna(x['title']) and pd.notna(x['abstract'])\n            else str(x['title']) if pd.notna(x['title'])\n            else str(x['abstract']) if pd.notna(x['abstract'])\n            else '',\n            axis=1\n        )\n        # Drop unnecessary columns\n        chunk = chunk[['subcategory', 'title_abstract']]\n        # Append to processed papers\n        processed_papers.append(chunk)\n        total_papers += len(chunk)\n        logger.info(f\"Processed {total_papers} papers...\")\n        # Clear memory\n        del chunk\n        gc.collect()\nexcept Exception as e:\n    logger.error(f\"Error processing CSV: {e}\")\n    raise\n\n# Concatenate all chunks\nlogger.info(\"Concatenating processed papers...\")\ntry:\n    if processed_papers:\n        final_df = pd.concat(processed_papers, ignore_index=True)\n    else:\n        final_df = pd.DataFrame(columns=['subcategory', 'title_abstract'])\n    logger.info(f\"Total papers after processing: {len(final_df)}\")\nexcept Exception as e:\n    logger.error(f\"Error concatenating data: {e}\")\n    raise\n\n# Save to CSV\nlogger.info(f\"Saving processed papers to {OUTPUT_PATH}...\")\ntry:\n    final_df.to_csv(OUTPUT_PATH, index=False)\n    # Verify file was saved\n    if os.path.exists(OUTPUT_PATH):\n        logger.info(f\"Processed papers successfully saved to {OUTPUT_PATH}\")\n        logger.info(f\"File size: {os.path.getsize(OUTPUT_PATH)} bytes\")\n    else:\n        logger.error(f\"Failed to verify saved file: {OUTPUT_PATH}\")\n        raise FileNotFoundError(f\"Failed to verify saved file: {OUTPUT_PATH}\")\nexcept Exception as e:\n    logger.error(f\"Error saving CSV to {OUTPUT_PATH}: {e}\")\n    raise\n\n# Print summary\nprint(\"\\nProcessed Papers Summary:\")\nprint(f\"Total Papers: {len(final_df)}\")\nprint(f\"Subcategories: {final_df['subcategory'].nunique()}\")\nprint(\"\\nPapers per Subcategory:\")\nprint(final_df['subcategory'].value_counts().sort_index())\nlogger.info(f\"\\nProcessed Papers Summary:\\nTotal Papers: {len(final_df)}\")\nlogger.info(f\"Subcategories: {final_df['subcategory'].nunique()}\")\nlogger.info(\"\\nPapers per Subcategory:\\n\" + final_df['subcategory'].value_counts().sort_index().to_string())\n\nlogger.info(\"Processing complete.\")\nprint(\"Processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:08:21.769557Z","iopub.execute_input":"2025-05-11T12:08:21.769846Z","iopub.status.idle":"2025-05-11T12:08:26.061232Z","shell.execute_reply.started":"2025-05-11T12:08:21.769823Z","shell.execute_reply":"2025-05-11T12:08:26.060245Z"}},"outputs":[{"name":"stdout","text":"\nProcessed Papers Summary:\nTotal Papers: 87000\nSubcategories: 29\n\nPapers per Subcategory:\nsubcategory\nmath.AC    3000\nmath.AG    3000\nmath.AP    3000\nmath.AT    3000\nmath.CA    3000\nmath.CO    3000\nmath.CT    3000\nmath.CV    3000\nmath.DG    3000\nmath.DS    3000\nmath.FA    3000\nmath.GM    3000\nmath.GN    3000\nmath.GR    3000\nmath.GT    3000\nmath.HO    3000\nmath.KT    3000\nmath.LO    3000\nmath.MG    3000\nmath.NA    3000\nmath.NT    3000\nmath.OA    3000\nmath.OC    3000\nmath.PR    3000\nmath.QA    3000\nmath.RA    3000\nmath.RT    3000\nmath.SG    3000\nmath.SP    3000\nName: count, dtype: int64\nProcessing complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport logging\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/math_papers_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Path to the CSV file\ncsv_path = '/kaggle/working/math_papers_title_abstract.csv'\n\n# Verify file exists\nlogger.info(f\"Checking for CSV file at {csv_path}...\")\nif not os.path.exists(csv_path):\n    logger.error(f\"CSV file not found: {csv_path}\")\n    raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n\n# Load the CSV\nlogger.info(f\"Loading CSV file from {csv_path}...\")\ntry:\n    df = pd.read_csv(csv_path)\n    logger.info(f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns\")\nexcept Exception as e:\n    logger.error(f\"Error loading CSV: {e}\")\n    raise\n\n# Display the first few rows\nlogger.info(\"Displaying the first 5 rows of the dataset...\")\nprint(\"\\nFirst 5 rows of math_papers_title_abstract.csv:\")\nprint(df.head())\nlogger.info(\"\\nFirst 5 rows of math_papers_title_abstract.csv:\\n\" + df.head().to_string())\n\n# Print basic info\nprint(f\"\\nTotal Papers: {len(df)}\")\nprint(f\"Subcategories: {df['subcategory'].nunique()}\")\nprint(\"\\nColumns:\", list(df.columns))\nlogger.info(f\"\\nTotal Papers: {len(df)}\")\nlogger.info(f\"Subcategories: {df['subcategory'].nunique()}\")\nlogger.info(f\"\\nColumns: {list(df.columns)}\")\n\nlogger.info(\"Display complete.\")\nprint(\"Display complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:08:26.062153Z","iopub.execute_input":"2025-05-11T12:08:26.062420Z","iopub.status.idle":"2025-05-11T12:08:26.667442Z","shell.execute_reply.started":"2025-05-11T12:08:26.062399Z","shell.execute_reply":"2025-05-11T12:08:26.666598Z"}},"outputs":[{"name":"stdout","text":"\nFirst 5 rows of math_papers_title_abstract.csv:\n  subcategory                                     title_abstract\n0     math.AC  Duality and normalization, variations on a the...\n1     math.AC  Matrix invertible extensions over commutative ...\n2     math.AC  Lcm-lattice, Taylor Bases and Minimal Free Res...\n3     math.AC  On the $(S_2)$-condition of edge rings for cac...\n4     math.AC  Adjacency Spectrum and Wiener Index of the Ess...\n\nTotal Papers: 87000\nSubcategories: 29\n\nColumns: ['subcategory', 'title_abstract']\nDisplay complete.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\nfrom sklearn.model_selection import train_test_split\n\n# File path\nfile_path = '/kaggle/working/math_papers_title_abstract.csv'\n\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}. Please ensure math_papers_title_abstract.csv is generated.\")\n\n# Load dataset\nprint(\"Loading math_papers_title_abstract.csv...\")\ntry:\n    df = pd.read_csv(\n        file_path,\n        quoting=csv.QUOTE_ALL,\n        on_bad_lines='warn',\n        engine='python'\n    )\n    print(f\"Loaded {len(df)} rows\")\n    print(f\"Columns: {df.columns.tolist()}\")\nexcept Exception as e:\n    print(f\"Error loading math_papers_title_abstract.csv: {e}\")\n    raise\n\n# Define selected 29 math categories\ncategories = [\n    'math.AC', 'math.AG', 'math.AP', 'math.AT', 'math.CA',\n    'math.CO', 'math.CT', 'math.CV', 'math.DG', 'math.DS',\n    'math.FA', 'math.GM', 'math.GN', 'math.GR', 'math.GT',\n    'math.HO', 'math.KT', 'math.LO', 'math.MG', 'math.NA',\n    'math.NT', 'math.OA', 'math.OC', 'math.PR', 'math.QA',\n    'math.RA', 'math.RT', 'math.SG', 'math.SP'\n]\nlabel_map = {cat: i for i, cat in enumerate(categories)}\nmath_prefix = 'math.'\n\n# Use subcategory as category\nprint(\"Counting math subcategories...\")\ndf['category'] = df['subcategory']\n\n# Count and display available papers per category\ncategory_counts = df['category'].value_counts().to_dict()\nprint(\"\\nFound math subcategories:\\n\")\nfor cat in categories:\n    print(f\"{cat}: {category_counts.get(cat, 0)} papers\")\n\n# Map labels\ndf['label'] = df['category'].map(label_map)\n\n# Filter to N rows per category\nprint(\"\\nFiltering to 3,500 rows per category...\")\nfiltered_df = pd.DataFrame()\nrows_per_category = 3000\nfor cat in categories:\n    cat_df = df[df['category'] == cat].head(rows_per_category)\n    if len(cat_df) < rows_per_category:\n        print(f\"⚠ Only {len(cat_df)} papers found for {cat}\")\n    filtered_df = pd.concat([filtered_df, cat_df], ignore_index=True)\n\n# Final check\nactual_counts = filtered_df['category'].value_counts().to_dict()\nexpected_total = sum(min(category_counts.get(cat, 0), rows_per_category) for cat in categories)\nif len(filtered_df) != expected_total:\n    raise ValueError(f\"Mismatch in expected filtered count. Got {len(filtered_df)} instead of {expected_total}.\")\n\nprint(f\"\\nFiltered to {len(filtered_df)} rows\")\nprint(f\"Label distribution:\\n{filtered_df['label'].value_counts().sort_index()}\")\n\n# Split dataset\nprint(\"Splitting dataset...\")\ntrain_df, temp_df = train_test_split(filtered_df, test_size=0.2, stratify=filtered_df['label'], random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n\n# Save splits\noutput_dir = '/kaggle/working/'\nprint(f\"Saving train.csv, val.csv, test.csv to {output_dir}...\")\ntrain_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\nval_df.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\ntest_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n\nprint(f\"\\nTrain size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\nprint(\"Step1 complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:08:26.668235Z","iopub.execute_input":"2025-05-11T12:08:26.668480Z","iopub.status.idle":"2025-05-11T12:08:30.306831Z","shell.execute_reply.started":"2025-05-11T12:08:26.668458Z","shell.execute_reply":"2025-05-11T12:08:30.306019Z"}},"outputs":[{"name":"stdout","text":"Checking for /kaggle/working/math_papers_title_abstract.csv...\nLoading math_papers_title_abstract.csv...\nLoaded 87000 rows\nColumns: ['subcategory', 'title_abstract']\nCounting math subcategories...\n\nFound math subcategories:\n\nmath.AC: 3000 papers\nmath.AG: 3000 papers\nmath.AP: 3000 papers\nmath.AT: 3000 papers\nmath.CA: 3000 papers\nmath.CO: 3000 papers\nmath.CT: 3000 papers\nmath.CV: 3000 papers\nmath.DG: 3000 papers\nmath.DS: 3000 papers\nmath.FA: 3000 papers\nmath.GM: 3000 papers\nmath.GN: 3000 papers\nmath.GR: 3000 papers\nmath.GT: 3000 papers\nmath.HO: 3000 papers\nmath.KT: 3000 papers\nmath.LO: 3000 papers\nmath.MG: 3000 papers\nmath.NA: 3000 papers\nmath.NT: 3000 papers\nmath.OA: 3000 papers\nmath.OC: 3000 papers\nmath.PR: 3000 papers\nmath.QA: 3000 papers\nmath.RA: 3000 papers\nmath.RT: 3000 papers\nmath.SG: 3000 papers\nmath.SP: 3000 papers\n\nFiltering to 3,500 rows per category...\n\nFiltered to 87000 rows\nLabel distribution:\nlabel\n0     3000\n1     3000\n2     3000\n3     3000\n4     3000\n5     3000\n6     3000\n7     3000\n8     3000\n9     3000\n10    3000\n11    3000\n12    3000\n13    3000\n14    3000\n15    3000\n16    3000\n17    3000\n18    3000\n19    3000\n20    3000\n21    3000\n22    3000\n23    3000\n24    3000\n25    3000\n26    3000\n27    3000\n28    3000\nName: count, dtype: int64\nSplitting dataset...\nSaving train.csv, val.csv, test.csv to /kaggle/working/...\n\nTrain size: 69600, Val size: 8700, Test size: 8700\nStep1 complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import LongformerTokenizer\nimport torch\nimport pickle\nimport logging\nimport os\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/step2_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Define input/output directories\ninput_dir = '/kaggle/working/'  # Files are in working directory\noutput_dir = '/kaggle/working/'  # Save outputs here\n\n# Find train.csv and val.csv\nprint(\"Searching for train.csv and val.csv in /kaggle/working/...\")\nlogger.info(\"Searching for train.csv and val.csv in /kaggle/working/...\")\ntrain_path = None\nval_path = None\n\nfor root, dirs, files in os.walk(input_dir):\n    for file in files:\n        if file == 'train.csv':\n            train_path = os.path.join(root, file)\n        if file == 'val.csv':\n            val_path = os.path.join(root, file)\n    if train_path and val_path:\n        break\n\nif not train_path or not val_path:\n    logger.error(\"train.csv or val.csv not found in /kaggle/working/.\")\n    raise FileNotFoundError(\"train.csv or val.csv not found. Please ensure they are generated.\")\n\nprint(f\"Found train.csv at {train_path}\")\nprint(f\"Found val.csv at {val_path}\")\nlogger.info(f\"Found train.csv at {train_path}\")\nlogger.info(f\"Found val.csv at {val_path}\")\n\n# Load tokenizer\nprint(\"Loading Longformer tokenizer...\")\nlogger.info(\"Loading Longformer tokenizer...\")\ntry:\n    tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nexcept Exception as e:\n    logger.error(f\"Error loading tokenizer: {e}\")\n    raise\n\n# Load datasets\nprint(\"Loading datasets...\")\nlogger.info(\"Loading datasets...\")\ntrain_df = pd.read_csv(train_path)\nval_df = pd.read_csv(val_path)\n\n# Tokenization function\ndef tokenize_data(df, max_length=4096):\n    texts = df['title_abstract'].tolist()  # Use title_abstract column\n    labels = df['label'].tolist()\n    encodings = tokenizer(\n        texts, \n        truncation=True, \n        padding=True, \n        max_length=max_length, \n        return_tensors='pt'\n    )\n    return {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': labels\n    }\n\n# Tokenize in batches\nbatch_size = 100\ntrain_tokenized = []\nval_tokenized = []\n\nprint(\"Tokenizing training data...\")\nlogger.info(\"Tokenizing training data...\")\nfor i in range(0, len(train_df), batch_size):\n    batch_df = train_df[i:i+batch_size]\n    tokenized_batch = tokenize_data(batch_df)\n    train_tokenized.append(tokenized_batch)\n    print(f\"Tokenized train batch {i//batch_size + 1}/{len(train_df)//batch_size + 1}\")\n    logger.info(f\"Tokenized train batch {i//batch_size + 1}/{len(train_df)//batch_size + 1}\")\n\nprint(\"Tokenizing validation data...\")\nlogger.info(\"Tokenizing validation data...\")\nfor i in range(0, len(val_df), batch_size):\n    batch_df = val_df[i:i+batch_size]\n    tokenized_batch = tokenize_data(batch_df)\n    val_tokenized.append(tokenized_batch)\n    print(f\"Tokenized val batch {i//batch_size + 1}/{len(val_df)//batch_size + 1}\")\n    logger.info(f\"Tokenized val batch {i//batch_size + 1}/{len(val_df)//batch_size + 1}\")\n\n# Save tokenized datasets\nprint(f\"Saving tokenized datasets to {output_dir}...\")\nlogger.info(f\"Saving tokenized datasets to {output_dir}...\")\nwith open(os.path.join(output_dir, 'train_tokenized.pkl'), 'wb') as f:\n    pickle.dump(train_tokenized, f)\nwith open(os.path.join(output_dir, 'val_tokenized.pkl'), 'wb') as f:\n    pickle.dump(val_tokenized, f)\n\nprint(f\"Train tokenized: {len(train_tokenized)} batches, Val tokenized: {len(val_tokenized)} batches\")\nlogger.info(f\"Train tokenized: {len(train_tokenized)} batches, Val tokenized: {len(val_tokenized)} batches\")\nprint(\"Step 2 complete.\")\nlogger.info(\"Step 2 complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:08:30.308704Z","iopub.execute_input":"2025-05-11T12:08:30.308925Z","iopub.status.idle":"2025-05-11T12:10:20.063786Z","shell.execute_reply.started":"2025-05-11T12:08:30.308908Z","shell.execute_reply":"2025-05-11T12:10:20.063096Z"}},"outputs":[{"name":"stdout","text":"Searching for train.csv and val.csv in /kaggle/working/...\nFound train.csv at /kaggle/working/train.csv\nFound val.csv at /kaggle/working/val.csv\nLoading Longformer tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b509c8acd1db4007bafdd29ff5df1a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f85ce43ecb7411298b7219bdee388e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7197ba81dc46c7ae6cde67778b2539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7f41cf1e3a46648cb225d98af0c11e"}},"metadata":{}},{"name":"stdout","text":"Loading datasets...\nTokenizing training data...\nTokenized train batch 1/697\nTokenized train batch 2/697\nTokenized train batch 3/697\nTokenized train batch 4/697\nTokenized train batch 5/697\nTokenized train batch 6/697\nTokenized train batch 7/697\nTokenized train batch 8/697\nTokenized train batch 9/697\nTokenized train batch 10/697\nTokenized train batch 11/697\nTokenized train batch 12/697\nTokenized train batch 13/697\nTokenized train batch 14/697\nTokenized train batch 15/697\nTokenized train batch 16/697\nTokenized train batch 17/697\nTokenized train batch 18/697\nTokenized train batch 19/697\nTokenized train batch 20/697\nTokenized train batch 21/697\nTokenized train batch 22/697\nTokenized train batch 23/697\nTokenized train batch 24/697\nTokenized train batch 25/697\nTokenized train batch 26/697\nTokenized train batch 27/697\nTokenized train batch 28/697\nTokenized train batch 29/697\nTokenized train batch 30/697\nTokenized train batch 31/697\nTokenized train batch 32/697\nTokenized train batch 33/697\nTokenized train batch 34/697\nTokenized train batch 35/697\nTokenized train batch 36/697\nTokenized train batch 37/697\nTokenized train batch 38/697\nTokenized train batch 39/697\nTokenized train batch 40/697\nTokenized train batch 41/697\nTokenized train batch 42/697\nTokenized train batch 43/697\nTokenized train batch 44/697\nTokenized train batch 45/697\nTokenized train batch 46/697\nTokenized train batch 47/697\nTokenized train batch 48/697\nTokenized train batch 49/697\nTokenized train batch 50/697\nTokenized train batch 51/697\nTokenized train batch 52/697\nTokenized train batch 53/697\nTokenized train batch 54/697\nTokenized train batch 55/697\nTokenized train batch 56/697\nTokenized train batch 57/697\nTokenized train batch 58/697\nTokenized train batch 59/697\nTokenized train batch 60/697\nTokenized train batch 61/697\nTokenized train batch 62/697\nTokenized train batch 63/697\nTokenized train batch 64/697\nTokenized train batch 65/697\nTokenized train batch 66/697\nTokenized train batch 67/697\nTokenized train batch 68/697\nTokenized train batch 69/697\nTokenized train batch 70/697\nTokenized train batch 71/697\nTokenized train batch 72/697\nTokenized train batch 73/697\nTokenized train batch 74/697\nTokenized train batch 75/697\nTokenized train batch 76/697\nTokenized train batch 77/697\nTokenized train batch 78/697\nTokenized train batch 79/697\nTokenized train batch 80/697\nTokenized train batch 81/697\nTokenized train batch 82/697\nTokenized train batch 83/697\nTokenized train batch 84/697\nTokenized train batch 85/697\nTokenized train batch 86/697\nTokenized train batch 87/697\nTokenized train batch 88/697\nTokenized train batch 89/697\nTokenized train batch 90/697\nTokenized train batch 91/697\nTokenized train batch 92/697\nTokenized train batch 93/697\nTokenized train batch 94/697\nTokenized train batch 95/697\nTokenized train batch 96/697\nTokenized train batch 97/697\nTokenized train batch 98/697\nTokenized train batch 99/697\nTokenized train batch 100/697\nTokenized train batch 101/697\nTokenized train batch 102/697\nTokenized train batch 103/697\nTokenized train batch 104/697\nTokenized train batch 105/697\nTokenized train batch 106/697\nTokenized train batch 107/697\nTokenized train batch 108/697\nTokenized train batch 109/697\nTokenized train batch 110/697\nTokenized train batch 111/697\nTokenized train batch 112/697\nTokenized train batch 113/697\nTokenized train batch 114/697\nTokenized train batch 115/697\nTokenized train batch 116/697\nTokenized train batch 117/697\nTokenized train batch 118/697\nTokenized train batch 119/697\nTokenized train batch 120/697\nTokenized train batch 121/697\nTokenized train batch 122/697\nTokenized train batch 123/697\nTokenized train batch 124/697\nTokenized train batch 125/697\nTokenized train batch 126/697\nTokenized train batch 127/697\nTokenized train batch 128/697\nTokenized train batch 129/697\nTokenized train batch 130/697\nTokenized train batch 131/697\nTokenized train batch 132/697\nTokenized train batch 133/697\nTokenized train batch 134/697\nTokenized train batch 135/697\nTokenized train batch 136/697\nTokenized train batch 137/697\nTokenized train batch 138/697\nTokenized train batch 139/697\nTokenized train batch 140/697\nTokenized train batch 141/697\nTokenized train batch 142/697\nTokenized train batch 143/697\nTokenized train batch 144/697\nTokenized train batch 145/697\nTokenized train batch 146/697\nTokenized train batch 147/697\nTokenized train batch 148/697\nTokenized train batch 149/697\nTokenized train batch 150/697\nTokenized train batch 151/697\nTokenized train batch 152/697\nTokenized train batch 153/697\nTokenized train batch 154/697\nTokenized train batch 155/697\nTokenized train batch 156/697\nTokenized train batch 157/697\nTokenized train batch 158/697\nTokenized train batch 159/697\nTokenized train batch 160/697\nTokenized train batch 161/697\nTokenized train batch 162/697\nTokenized train batch 163/697\nTokenized train batch 164/697\nTokenized train batch 165/697\nTokenized train batch 166/697\nTokenized train batch 167/697\nTokenized train batch 168/697\nTokenized train batch 169/697\nTokenized train batch 170/697\nTokenized train batch 171/697\nTokenized train batch 172/697\nTokenized train batch 173/697\nTokenized train batch 174/697\nTokenized train batch 175/697\nTokenized train batch 176/697\nTokenized train batch 177/697\nTokenized train batch 178/697\nTokenized train batch 179/697\nTokenized train batch 180/697\nTokenized train batch 181/697\nTokenized train batch 182/697\nTokenized train batch 183/697\nTokenized train batch 184/697\nTokenized train batch 185/697\nTokenized train batch 186/697\nTokenized train batch 187/697\nTokenized train batch 188/697\nTokenized train batch 189/697\nTokenized train batch 190/697\nTokenized train batch 191/697\nTokenized train batch 192/697\nTokenized train batch 193/697\nTokenized train batch 194/697\nTokenized train batch 195/697\nTokenized train batch 196/697\nTokenized train batch 197/697\nTokenized train batch 198/697\nTokenized train batch 199/697\nTokenized train batch 200/697\nTokenized train batch 201/697\nTokenized train batch 202/697\nTokenized train batch 203/697\nTokenized train batch 204/697\nTokenized train batch 205/697\nTokenized train batch 206/697\nTokenized train batch 207/697\nTokenized train batch 208/697\nTokenized train batch 209/697\nTokenized train batch 210/697\nTokenized train batch 211/697\nTokenized train batch 212/697\nTokenized train batch 213/697\nTokenized train batch 214/697\nTokenized train batch 215/697\nTokenized train batch 216/697\nTokenized train batch 217/697\nTokenized train batch 218/697\nTokenized train batch 219/697\nTokenized train batch 220/697\nTokenized train batch 221/697\nTokenized train batch 222/697\nTokenized train batch 223/697\nTokenized train batch 224/697\nTokenized train batch 225/697\nTokenized train batch 226/697\nTokenized train batch 227/697\nTokenized train batch 228/697\nTokenized train batch 229/697\nTokenized train batch 230/697\nTokenized train batch 231/697\nTokenized train batch 232/697\nTokenized train batch 233/697\nTokenized train batch 234/697\nTokenized train batch 235/697\nTokenized train batch 236/697\nTokenized train batch 237/697\nTokenized train batch 238/697\nTokenized train batch 239/697\nTokenized train batch 240/697\nTokenized train batch 241/697\nTokenized train batch 242/697\nTokenized train batch 243/697\nTokenized train batch 244/697\nTokenized train batch 245/697\nTokenized train batch 246/697\nTokenized train batch 247/697\nTokenized train batch 248/697\nTokenized train batch 249/697\nTokenized train batch 250/697\nTokenized train batch 251/697\nTokenized train batch 252/697\nTokenized train batch 253/697\nTokenized train batch 254/697\nTokenized train batch 255/697\nTokenized train batch 256/697\nTokenized train batch 257/697\nTokenized train batch 258/697\nTokenized train batch 259/697\nTokenized train batch 260/697\nTokenized train batch 261/697\nTokenized train batch 262/697\nTokenized train batch 263/697\nTokenized train batch 264/697\nTokenized train batch 265/697\nTokenized train batch 266/697\nTokenized train batch 267/697\nTokenized train batch 268/697\nTokenized train batch 269/697\nTokenized train batch 270/697\nTokenized train batch 271/697\nTokenized train batch 272/697\nTokenized train batch 273/697\nTokenized train batch 274/697\nTokenized train batch 275/697\nTokenized train batch 276/697\nTokenized train batch 277/697\nTokenized train batch 278/697\nTokenized train batch 279/697\nTokenized train batch 280/697\nTokenized train batch 281/697\nTokenized train batch 282/697\nTokenized train batch 283/697\nTokenized train batch 284/697\nTokenized train batch 285/697\nTokenized train batch 286/697\nTokenized train batch 287/697\nTokenized train batch 288/697\nTokenized train batch 289/697\nTokenized train batch 290/697\nTokenized train batch 291/697\nTokenized train batch 292/697\nTokenized train batch 293/697\nTokenized train batch 294/697\nTokenized train batch 295/697\nTokenized train batch 296/697\nTokenized train batch 297/697\nTokenized train batch 298/697\nTokenized train batch 299/697\nTokenized train batch 300/697\nTokenized train batch 301/697\nTokenized train batch 302/697\nTokenized train batch 303/697\nTokenized train batch 304/697\nTokenized train batch 305/697\nTokenized train batch 306/697\nTokenized train batch 307/697\nTokenized train batch 308/697\nTokenized train batch 309/697\nTokenized train batch 310/697\nTokenized train batch 311/697\nTokenized train batch 312/697\nTokenized train batch 313/697\nTokenized train batch 314/697\nTokenized train batch 315/697\nTokenized train batch 316/697\nTokenized train batch 317/697\nTokenized train batch 318/697\nTokenized train batch 319/697\nTokenized train batch 320/697\nTokenized train batch 321/697\nTokenized train batch 322/697\nTokenized train batch 323/697\nTokenized train batch 324/697\nTokenized train batch 325/697\nTokenized train batch 326/697\nTokenized train batch 327/697\nTokenized train batch 328/697\nTokenized train batch 329/697\nTokenized train batch 330/697\nTokenized train batch 331/697\nTokenized train batch 332/697\nTokenized train batch 333/697\nTokenized train batch 334/697\nTokenized train batch 335/697\nTokenized train batch 336/697\nTokenized train batch 337/697\nTokenized train batch 338/697\nTokenized train batch 339/697\nTokenized train batch 340/697\nTokenized train batch 341/697\nTokenized train batch 342/697\nTokenized train batch 343/697\nTokenized train batch 344/697\nTokenized train batch 345/697\nTokenized train batch 346/697\nTokenized train batch 347/697\nTokenized train batch 348/697\nTokenized train batch 349/697\nTokenized train batch 350/697\nTokenized train batch 351/697\nTokenized train batch 352/697\nTokenized train batch 353/697\nTokenized train batch 354/697\nTokenized train batch 355/697\nTokenized train batch 356/697\nTokenized train batch 357/697\nTokenized train batch 358/697\nTokenized train batch 359/697\nTokenized train batch 360/697\nTokenized train batch 361/697\nTokenized train batch 362/697\nTokenized train batch 363/697\nTokenized train batch 364/697\nTokenized train batch 365/697\nTokenized train batch 366/697\nTokenized train batch 367/697\nTokenized train batch 368/697\nTokenized train batch 369/697\nTokenized train batch 370/697\nTokenized train batch 371/697\nTokenized train batch 372/697\nTokenized train batch 373/697\nTokenized train batch 374/697\nTokenized train batch 375/697\nTokenized train batch 376/697\nTokenized train batch 377/697\nTokenized train batch 378/697\nTokenized train batch 379/697\nTokenized train batch 380/697\nTokenized train batch 381/697\nTokenized train batch 382/697\nTokenized train batch 383/697\nTokenized train batch 384/697\nTokenized train batch 385/697\nTokenized train batch 386/697\nTokenized train batch 387/697\nTokenized train batch 388/697\nTokenized train batch 389/697\nTokenized train batch 390/697\nTokenized train batch 391/697\nTokenized train batch 392/697\nTokenized train batch 393/697\nTokenized train batch 394/697\nTokenized train batch 395/697\nTokenized train batch 396/697\nTokenized train batch 397/697\nTokenized train batch 398/697\nTokenized train batch 399/697\nTokenized train batch 400/697\nTokenized train batch 401/697\nTokenized train batch 402/697\nTokenized train batch 403/697\nTokenized train batch 404/697\nTokenized train batch 405/697\nTokenized train batch 406/697\nTokenized train batch 407/697\nTokenized train batch 408/697\nTokenized train batch 409/697\nTokenized train batch 410/697\nTokenized train batch 411/697\nTokenized train batch 412/697\nTokenized train batch 413/697\nTokenized train batch 414/697\nTokenized train batch 415/697\nTokenized train batch 416/697\nTokenized train batch 417/697\nTokenized train batch 418/697\nTokenized train batch 419/697\nTokenized train batch 420/697\nTokenized train batch 421/697\nTokenized train batch 422/697\nTokenized train batch 423/697\nTokenized train batch 424/697\nTokenized train batch 425/697\nTokenized train batch 426/697\nTokenized train batch 427/697\nTokenized train batch 428/697\nTokenized train batch 429/697\nTokenized train batch 430/697\nTokenized train batch 431/697\nTokenized train batch 432/697\nTokenized train batch 433/697\nTokenized train batch 434/697\nTokenized train batch 435/697\nTokenized train batch 436/697\nTokenized train batch 437/697\nTokenized train batch 438/697\nTokenized train batch 439/697\nTokenized train batch 440/697\nTokenized train batch 441/697\nTokenized train batch 442/697\nTokenized train batch 443/697\nTokenized train batch 444/697\nTokenized train batch 445/697\nTokenized train batch 446/697\nTokenized train batch 447/697\nTokenized train batch 448/697\nTokenized train batch 449/697\nTokenized train batch 450/697\nTokenized train batch 451/697\nTokenized train batch 452/697\nTokenized train batch 453/697\nTokenized train batch 454/697\nTokenized train batch 455/697\nTokenized train batch 456/697\nTokenized train batch 457/697\nTokenized train batch 458/697\nTokenized train batch 459/697\nTokenized train batch 460/697\nTokenized train batch 461/697\nTokenized train batch 462/697\nTokenized train batch 463/697\nTokenized train batch 464/697\nTokenized train batch 465/697\nTokenized train batch 466/697\nTokenized train batch 467/697\nTokenized train batch 468/697\nTokenized train batch 469/697\nTokenized train batch 470/697\nTokenized train batch 471/697\nTokenized train batch 472/697\nTokenized train batch 473/697\nTokenized train batch 474/697\nTokenized train batch 475/697\nTokenized train batch 476/697\nTokenized train batch 477/697\nTokenized train batch 478/697\nTokenized train batch 479/697\nTokenized train batch 480/697\nTokenized train batch 481/697\nTokenized train batch 482/697\nTokenized train batch 483/697\nTokenized train batch 484/697\nTokenized train batch 485/697\nTokenized train batch 486/697\nTokenized train batch 487/697\nTokenized train batch 488/697\nTokenized train batch 489/697\nTokenized train batch 490/697\nTokenized train batch 491/697\nTokenized train batch 492/697\nTokenized train batch 493/697\nTokenized train batch 494/697\nTokenized train batch 495/697\nTokenized train batch 496/697\nTokenized train batch 497/697\nTokenized train batch 498/697\nTokenized train batch 499/697\nTokenized train batch 500/697\nTokenized train batch 501/697\nTokenized train batch 502/697\nTokenized train batch 503/697\nTokenized train batch 504/697\nTokenized train batch 505/697\nTokenized train batch 506/697\nTokenized train batch 507/697\nTokenized train batch 508/697\nTokenized train batch 509/697\nTokenized train batch 510/697\nTokenized train batch 511/697\nTokenized train batch 512/697\nTokenized train batch 513/697\nTokenized train batch 514/697\nTokenized train batch 515/697\nTokenized train batch 516/697\nTokenized train batch 517/697\nTokenized train batch 518/697\nTokenized train batch 519/697\nTokenized train batch 520/697\nTokenized train batch 521/697\nTokenized train batch 522/697\nTokenized train batch 523/697\nTokenized train batch 524/697\nTokenized train batch 525/697\nTokenized train batch 526/697\nTokenized train batch 527/697\nTokenized train batch 528/697\nTokenized train batch 529/697\nTokenized train batch 530/697\nTokenized train batch 531/697\nTokenized train batch 532/697\nTokenized train batch 533/697\nTokenized train batch 534/697\nTokenized train batch 535/697\nTokenized train batch 536/697\nTokenized train batch 537/697\nTokenized train batch 538/697\nTokenized train batch 539/697\nTokenized train batch 540/697\nTokenized train batch 541/697\nTokenized train batch 542/697\nTokenized train batch 543/697\nTokenized train batch 544/697\nTokenized train batch 545/697\nTokenized train batch 546/697\nTokenized train batch 547/697\nTokenized train batch 548/697\nTokenized train batch 549/697\nTokenized train batch 550/697\nTokenized train batch 551/697\nTokenized train batch 552/697\nTokenized train batch 553/697\nTokenized train batch 554/697\nTokenized train batch 555/697\nTokenized train batch 556/697\nTokenized train batch 557/697\nTokenized train batch 558/697\nTokenized train batch 559/697\nTokenized train batch 560/697\nTokenized train batch 561/697\nTokenized train batch 562/697\nTokenized train batch 563/697\nTokenized train batch 564/697\nTokenized train batch 565/697\nTokenized train batch 566/697\nTokenized train batch 567/697\nTokenized train batch 568/697\nTokenized train batch 569/697\nTokenized train batch 570/697\nTokenized train batch 571/697\nTokenized train batch 572/697\nTokenized train batch 573/697\nTokenized train batch 574/697\nTokenized train batch 575/697\nTokenized train batch 576/697\nTokenized train batch 577/697\nTokenized train batch 578/697\nTokenized train batch 579/697\nTokenized train batch 580/697\nTokenized train batch 581/697\nTokenized train batch 582/697\nTokenized train batch 583/697\nTokenized train batch 584/697\nTokenized train batch 585/697\nTokenized train batch 586/697\nTokenized train batch 587/697\nTokenized train batch 588/697\nTokenized train batch 589/697\nTokenized train batch 590/697\nTokenized train batch 591/697\nTokenized train batch 592/697\nTokenized train batch 593/697\nTokenized train batch 594/697\nTokenized train batch 595/697\nTokenized train batch 596/697\nTokenized train batch 597/697\nTokenized train batch 598/697\nTokenized train batch 599/697\nTokenized train batch 600/697\nTokenized train batch 601/697\nTokenized train batch 602/697\nTokenized train batch 603/697\nTokenized train batch 604/697\nTokenized train batch 605/697\nTokenized train batch 606/697\nTokenized train batch 607/697\nTokenized train batch 608/697\nTokenized train batch 609/697\nTokenized train batch 610/697\nTokenized train batch 611/697\nTokenized train batch 612/697\nTokenized train batch 613/697\nTokenized train batch 614/697\nTokenized train batch 615/697\nTokenized train batch 616/697\nTokenized train batch 617/697\nTokenized train batch 618/697\nTokenized train batch 619/697\nTokenized train batch 620/697\nTokenized train batch 621/697\nTokenized train batch 622/697\nTokenized train batch 623/697\nTokenized train batch 624/697\nTokenized train batch 625/697\nTokenized train batch 626/697\nTokenized train batch 627/697\nTokenized train batch 628/697\nTokenized train batch 629/697\nTokenized train batch 630/697\nTokenized train batch 631/697\nTokenized train batch 632/697\nTokenized train batch 633/697\nTokenized train batch 634/697\nTokenized train batch 635/697\nTokenized train batch 636/697\nTokenized train batch 637/697\nTokenized train batch 638/697\nTokenized train batch 639/697\nTokenized train batch 640/697\nTokenized train batch 641/697\nTokenized train batch 642/697\nTokenized train batch 643/697\nTokenized train batch 644/697\nTokenized train batch 645/697\nTokenized train batch 646/697\nTokenized train batch 647/697\nTokenized train batch 648/697\nTokenized train batch 649/697\nTokenized train batch 650/697\nTokenized train batch 651/697\nTokenized train batch 652/697\nTokenized train batch 653/697\nTokenized train batch 654/697\nTokenized train batch 655/697\nTokenized train batch 656/697\nTokenized train batch 657/697\nTokenized train batch 658/697\nTokenized train batch 659/697\nTokenized train batch 660/697\nTokenized train batch 661/697\nTokenized train batch 662/697\nTokenized train batch 663/697\nTokenized train batch 664/697\nTokenized train batch 665/697\nTokenized train batch 666/697\nTokenized train batch 667/697\nTokenized train batch 668/697\nTokenized train batch 669/697\nTokenized train batch 670/697\nTokenized train batch 671/697\nTokenized train batch 672/697\nTokenized train batch 673/697\nTokenized train batch 674/697\nTokenized train batch 675/697\nTokenized train batch 676/697\nTokenized train batch 677/697\nTokenized train batch 678/697\nTokenized train batch 679/697\nTokenized train batch 680/697\nTokenized train batch 681/697\nTokenized train batch 682/697\nTokenized train batch 683/697\nTokenized train batch 684/697\nTokenized train batch 685/697\nTokenized train batch 686/697\nTokenized train batch 687/697\nTokenized train batch 688/697\nTokenized train batch 689/697\nTokenized train batch 690/697\nTokenized train batch 691/697\nTokenized train batch 692/697\nTokenized train batch 693/697\nTokenized train batch 694/697\nTokenized train batch 695/697\nTokenized train batch 696/697\nTokenizing validation data...\nTokenized val batch 1/88\nTokenized val batch 2/88\nTokenized val batch 3/88\nTokenized val batch 4/88\nTokenized val batch 5/88\nTokenized val batch 6/88\nTokenized val batch 7/88\nTokenized val batch 8/88\nTokenized val batch 9/88\nTokenized val batch 10/88\nTokenized val batch 11/88\nTokenized val batch 12/88\nTokenized val batch 13/88\nTokenized val batch 14/88\nTokenized val batch 15/88\nTokenized val batch 16/88\nTokenized val batch 17/88\nTokenized val batch 18/88\nTokenized val batch 19/88\nTokenized val batch 20/88\nTokenized val batch 21/88\nTokenized val batch 22/88\nTokenized val batch 23/88\nTokenized val batch 24/88\nTokenized val batch 25/88\nTokenized val batch 26/88\nTokenized val batch 27/88\nTokenized val batch 28/88\nTokenized val batch 29/88\nTokenized val batch 30/88\nTokenized val batch 31/88\nTokenized val batch 32/88\nTokenized val batch 33/88\nTokenized val batch 34/88\nTokenized val batch 35/88\nTokenized val batch 36/88\nTokenized val batch 37/88\nTokenized val batch 38/88\nTokenized val batch 39/88\nTokenized val batch 40/88\nTokenized val batch 41/88\nTokenized val batch 42/88\nTokenized val batch 43/88\nTokenized val batch 44/88\nTokenized val batch 45/88\nTokenized val batch 46/88\nTokenized val batch 47/88\nTokenized val batch 48/88\nTokenized val batch 49/88\nTokenized val batch 50/88\nTokenized val batch 51/88\nTokenized val batch 52/88\nTokenized val batch 53/88\nTokenized val batch 54/88\nTokenized val batch 55/88\nTokenized val batch 56/88\nTokenized val batch 57/88\nTokenized val batch 58/88\nTokenized val batch 59/88\nTokenized val batch 60/88\nTokenized val batch 61/88\nTokenized val batch 62/88\nTokenized val batch 63/88\nTokenized val batch 64/88\nTokenized val batch 65/88\nTokenized val batch 66/88\nTokenized val batch 67/88\nTokenized val batch 68/88\nTokenized val batch 69/88\nTokenized val batch 70/88\nTokenized val batch 71/88\nTokenized val batch 72/88\nTokenized val batch 73/88\nTokenized val batch 74/88\nTokenized val batch 75/88\nTokenized val batch 76/88\nTokenized val batch 77/88\nTokenized val batch 78/88\nTokenized val batch 79/88\nTokenized val batch 80/88\nTokenized val batch 81/88\nTokenized val batch 82/88\nTokenized val batch 83/88\nTokenized val batch 84/88\nTokenized val batch 85/88\nTokenized val batch 86/88\nTokenized val batch 87/88\nSaving tokenized datasets to /kaggle/working/...\nTrain tokenized: 696 batches, Val tokenized: 87 batches\nStep 2 complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:10:20.064532Z","iopub.execute_input":"2025-05-11T12:10:20.064788Z","iopub.status.idle":"2025-05-11T12:10:24.914931Z","shell.execute_reply.started":"2025-05-11T12:10:20.064762Z","shell.execute_reply":"2025-05-11T12:10:24.914045Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport glob\nimport re\nimport torch\nimport numpy as np\nimport pickle\nimport gc\nimport logging\nimport time\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom transformers import (\n    LongformerForSequenceClassification,\n    LongformerTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# ----------------------------\n# Setup\n# ----------------------------\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ntorch.backends.cuda.matmul.allow_tf32 = True\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/training_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ninput_dir = '/kaggle/working/'\ntrain_tokenized_path = os.path.join(input_dir, 'train_tokenized.pkl')\nval_tokenized_path = os.path.join(input_dir, 'val_tokenized.pkl')\nresults_dir = os.path.join(input_dir, 'results')\nos.makedirs(results_dir, exist_ok=True)\n\n# ----------------------------\n# Checkpoint\n# ----------------------------\ndef get_latest_checkpoint(results_dir):\n    checkpoint_dirs = glob.glob(os.path.join(results_dir, 'checkpoint-*'))\n    if not checkpoint_dirs:\n        return None\n    checkpoint_nums = [int(re.search(r'checkpoint-(\\d+)', d).group(1)) for d in checkpoint_dirs]\n    return os.path.join(results_dir, f'checkpoint-{max(checkpoint_nums)}')\n\ncheckpoint_path = get_latest_checkpoint(results_dir)\nprint(f\"Checkpoint: {checkpoint_path}\" if checkpoint_path else \"No checkpoints found.\")\n\n# ----------------------------\n# Load Data\n# ----------------------------\nwith open(train_tokenized_path, 'rb') as f:\n    train_tokenized = pickle.load(f)\nwith open(val_tokenized_path, 'rb') as f:\n    val_tokenized = pickle.load(f)\n\ndef flatten_batches(batched_data):\n    \"\"\"Flatten list of batches into a flat list of dicts with same length input tensors.\"\"\"\n    flat_data = []\n    for batch in batched_data:\n        for i in range(len(batch['input_ids'])):\n            item = {\n                'input_ids': batch['input_ids'][i][:1024],\n                'attention_mask': batch['attention_mask'][i][:1024],\n                'labels': int(batch['labels'][i])\n            }\n            flat_data.append(item)\n    return flat_data\n\ntrain_dataset = Dataset.from_list(flatten_batches(train_tokenized))\nval_dataset = Dataset.from_list(flatten_batches(val_tokenized))\n\ndel train_tokenized, val_tokenized\ngc.collect()\ntorch.cuda.empty_cache()\n\n# ----------------------------\n# Class Weights\n# ----------------------------\nlabels = np.array(train_dataset['labels'])\nnum_labels = 29\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\nif len(class_weights_tensor) < num_labels:\n    pad = torch.zeros(num_labels - len(class_weights_tensor))\n    class_weights_tensor = torch.cat((class_weights_tensor, pad))\n\n# ----------------------------\n# Tokenizer & Model\n# ----------------------------\ntokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\ntokenizer.model_max_length = 1024\n\nif checkpoint_path:\n    model = LongformerForSequenceClassification.from_pretrained(\n        checkpoint_path, num_labels=num_labels, ignore_mismatched_sizes=True\n    )\nelse:\n    model = LongformerForSequenceClassification.from_pretrained(\n        'allenai/longformer-base-4096', num_labels=num_labels\n    )\n\nmodel.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n\n# ----------------------------\n# Custom Trainer\n# ----------------------------\nclass WeightedTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# ----------------------------\n# Metrics\n# ----------------------------\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    return {\n        'accuracy': (preds == labels).mean(),\n        'precision_weighted': precision,\n        'recall_weighted': recall,\n        'f1_weighted': f1\n    }\n\n# ----------------------------\n# Training Arguments\n# ----------------------------\ntraining_args = TrainingArguments(\n    output_dir=results_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir=os.path.join(input_dir, 'logs'),\n    logging_steps=10,\n    logging_first_step=True,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=False,\n    fp16=True,\n    report_to='none',\n    log_level=\"info\",\n    disable_tqdm=False\n)\n\n# ----------------------------\n# Trainer and Training\n# ----------------------------\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer, padding=True),\n    class_weights=class_weights_tensor,\n)\n\nprint(\"Starting training...\")\nstart_time = time.time()\ntrainer.train(resume_from_checkpoint=checkpoint_path)\nend_time = time.time()\n\n# ----------------------------\n# Save Final Model\n# ----------------------------\nfinal_model_path = os.path.join(input_dir, 'final_model')\ntrainer.save_model(final_model_path)\nprint(f\"Model saved to {final_model_path}\")\nprint(f\"Training completed in {(end_time - start_time)/60:.2f} minutes.\")\n\n# ----------------------------\n# Save Final Evaluation Metrics\n# ----------------------------\nmetrics = trainer.evaluate()\npd.DataFrame([metrics]).to_csv(os.path.join(input_dir, \"final_eval_metrics.csv\"), index=False)\nprint(\"Metrics saved to final_eval_metrics.csv\")\n\n# Cleanup\ndel model, trainer\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:13:41.880208Z","iopub.execute_input":"2025-05-11T12:13:41.880946Z"}},"outputs":[{"name":"stdout","text":"No checkpoints found.\n","output_type":"stream"},{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/config.json\nModel config LongformerConfig {\n  \"attention_mode\": \"longformer\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"attention_window\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"ignore_attention_mask\": false,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 4098,\n  \"model_type\": \"longformer\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"onnx_export\": false,\n  \"pad_token_id\": 1,\n  \"sep_token_id\": 2,\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/config.json\nModel config LongformerConfig {\n  \"attention_mode\": \"longformer\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"attention_window\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\",\n    \"10\": \"LABEL_10\",\n    \"11\": \"LABEL_11\",\n    \"12\": \"LABEL_12\",\n    \"13\": \"LABEL_13\",\n    \"14\": \"LABEL_14\",\n    \"15\": \"LABEL_15\",\n    \"16\": \"LABEL_16\",\n    \"17\": \"LABEL_17\",\n    \"18\": \"LABEL_18\",\n    \"19\": \"LABEL_19\",\n    \"20\": \"LABEL_20\",\n    \"21\": \"LABEL_21\",\n    \"22\": \"LABEL_22\",\n    \"23\": \"LABEL_23\",\n    \"24\": \"LABEL_24\",\n    \"25\": \"LABEL_25\",\n    \"26\": \"LABEL_26\",\n    \"27\": \"LABEL_27\",\n    \"28\": \"LABEL_28\"\n  },\n  \"ignore_attention_mask\": false,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_10\": 10,\n    \"LABEL_11\": 11,\n    \"LABEL_12\": 12,\n    \"LABEL_13\": 13,\n    \"LABEL_14\": 14,\n    \"LABEL_15\": 15,\n    \"LABEL_16\": 16,\n    \"LABEL_17\": 17,\n    \"LABEL_18\": 18,\n    \"LABEL_19\": 19,\n    \"LABEL_2\": 2,\n    \"LABEL_20\": 20,\n    \"LABEL_21\": 21,\n    \"LABEL_22\": 22,\n    \"LABEL_23\": 23,\n    \"LABEL_24\": 24,\n    \"LABEL_25\": 25,\n    \"LABEL_26\": 26,\n    \"LABEL_27\": 27,\n    \"LABEL_28\": 28,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 4098,\n  \"model_type\": \"longformer\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"onnx_export\": false,\n  \"pad_token_id\": 1,\n  \"sep_token_id\": 2,\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/pytorch_model.bin\nAttempting to create safetensors variant\nSome weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSafetensors PR exists\nPyTorch: setting up devices\nUsing auto half precision backend\n***** Running training *****\n  Num examples = 69,600\n  Num Epochs = 3\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 2\n  Total optimization steps = 26,100\n  Number of trainable parameters = 148,681,757\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2415' max='26100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2415/26100 1:10:09 < 11:28:43, 0.57 it/s, Epoch 0.28/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}