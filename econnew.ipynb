{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11666150,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\n\nprint(\"[Step 1] Reading arXiv JSON in chunks...\")\n\n# Path to dataset (correct if added via Kaggle “Add Data” panel)\nfile_path = \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\"\n\necon_data = pd.DataFrame()\nchunk_num = 0\n\n# Memory-efficient loading\nfor chunk in pd.read_json(file_path, lines=True, chunksize=100_000):\n    chunk_num += 1\n    print(f\"Processing chunk {chunk_num}...\")\n    \n    # Extract primary category (first tag)\n    chunk['primary_category'] = chunk['categories'].str.split().str[0]\n    \n    # Filter for only econ.* categories\n    econ_chunk = chunk[chunk['primary_category'].str.startswith('econ.')][['id', 'title', 'abstract', 'categories', 'primary_category']]\n    \n    # Append to main econ_data\n    econ_data = pd.concat([econ_data, econ_chunk])\n    \n    # Free memory\n    del chunk, econ_chunk\n    gc.collect()\n\nprint(f\" Found {len(econ_data)} economics papers.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:39:53.610302Z","iopub.execute_input":"2025-05-10T19:39:53.610572Z","iopub.status.idle":"2025-05-10T19:42:29.976255Z","shell.execute_reply.started":"2025-05-10T19:39:53.610550Z","shell.execute_reply":"2025-05-10T19:42:29.975466Z"}},"outputs":[{"name":"stdout","text":"[Step 1] Reading arXiv JSON in chunks...\nProcessing chunk 1...\nProcessing chunk 2...\nProcessing chunk 3...\nProcessing chunk 4...\nProcessing chunk 5...\nProcessing chunk 6...\nProcessing chunk 7...\nProcessing chunk 8...\nProcessing chunk 9...\nProcessing chunk 10...\nProcessing chunk 11...\nProcessing chunk 12...\nProcessing chunk 13...\nProcessing chunk 14...\nProcessing chunk 15...\nProcessing chunk 16...\nProcessing chunk 17...\nProcessing chunk 18...\nProcessing chunk 19...\nProcessing chunk 20...\nProcessing chunk 21...\nProcessing chunk 22...\nProcessing chunk 23...\nProcessing chunk 24...\nProcessing chunk 25...\nProcessing chunk 26...\nProcessing chunk 27...\nProcessing chunk 28...\n Found 8833 economics papers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Save for reuse\necon_data.to_csv('/kaggle/working/econ_filtered.csv', index=False)\nprint(\" Saved to /kaggle/working/econ_filtered.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:42:44.597621Z","iopub.execute_input":"2025-05-10T19:42:44.597944Z","iopub.status.idle":"2025-05-10T19:42:44.905635Z","shell.execute_reply.started":"2025-05-10T19:42:44.597916Z","shell.execute_reply":"2025-05-10T19:42:44.905054Z"}},"outputs":[{"name":"stdout","text":" Saved to /kaggle/working/econ_filtered.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Add Label Column Using LabelEncoder &  Sentence Splitting and Combine Text","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.tokenize import sent_tokenize\nimport nltk\nnltk.download('punkt')\n\n# Reload economics dataset\ndf = pd.read_csv('/kaggle/working/econ_filtered.csv')\n\n# Recreate 'primary_category' column\ndf['primary_category'] = df['categories'].str.split().str[0]\n\n# ✅ Encode label from primary_category\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['primary_category'])\n\nlabel_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(\"Label Mapping:\")\nprint(label_mapping)\n\n# Combine title + abstract into text\ndf['text'] = df['title'] + \". \" + df['abstract']\n\n# Tokenize text into list of sentences\ndf['sentences'] = df['text'].apply(sent_tokenize)\n\n# ✅ Now select only what you need for LNLF\ndf = df[['sentences', 'label']]\ndf.head(20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:48:20.711819Z","iopub.execute_input":"2025-05-10T19:48:20.712145Z","iopub.status.idle":"2025-05-10T19:48:22.001219Z","shell.execute_reply.started":"2025-05-10T19:48:20.712125Z","shell.execute_reply":"2025-05-10T19:48:22.000599Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Label Mapping:\n{'econ.EM': 0, 'econ.GN': 1, 'econ.TH': 2}\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            sentences  label\n0   [Fiscal stimulus as an optimal control problem...      1\n1   [Identification and Estimation of Multidimensi...      1\n2   [Comprehensive Time-Series Regression Models U...      1\n3   [On Game-Theoretic Risk Management (Part One) ...      1\n4   [On Game-Theoretic Risk Management (Part Two) ...      1\n5   [The Mittag-Leffler Fitting of the Phillips Cu...      1\n6   [Economic Development and Inequality: a comple...      1\n7   [Banks as Tanks: A Continuous-Time Model of Fi...      1\n8   [Unfolding the innovation system for the devel...      1\n9   [Technology networks: the autocatalytic origin...      1\n10  [Payoff Information and Learning in Signaling ...      2\n11  [Economic Complexity: \"Buttarla in caciara\" vs...      1\n12  [Fixed Effect Estimation of Large T Panel Data...      0\n13  [Bounds On Treatment Effects On Transitions., ...      0\n14  [Inference on Estimators defined by Mathematic...      0\n15  [Discrete Choice and Rational Inattention: a G...      0\n16  [Sharp bounds and testability of a Roy model o...      0\n17  [Zero-rating of Content and its Effect on the ...      0\n18  [Identification of hedonic equilibrium and non...      0\n19  [Inference for Impulse Responses under Model U...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentences</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[Fiscal stimulus as an optimal control problem...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Identification and Estimation of Multidimensi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Comprehensive Time-Series Regression Models U...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[On Game-Theoretic Risk Management (Part One) ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[On Game-Theoretic Risk Management (Part Two) ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[The Mittag-Leffler Fitting of the Phillips Cu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[Economic Development and Inequality: a comple...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[Banks as Tanks: A Continuous-Time Model of Fi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[Unfolding the innovation system for the devel...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[Technology networks: the autocatalytic origin...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>[Payoff Information and Learning in Signaling ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>[Economic Complexity: \"Buttarla in caciara\" vs...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>[Fixed Effect Estimation of Large T Panel Data...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>[Bounds On Treatment Effects On Transitions., ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>[Inference on Estimators defined by Mathematic...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>[Discrete Choice and Rational Inattention: a G...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>[Sharp bounds and testability of a Roy model o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>[Zero-rating of Content and its Effect on the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>[Identification of hedonic equilibrium and non...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>[Inference for Impulse Responses under Model U...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"#  Install & Load Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizerFast\ntokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:53:17.076178Z","iopub.execute_input":"2025-05-10T19:53:17.076690Z","iopub.status.idle":"2025-05-10T19:53:21.346240Z","shell.execute_reply.started":"2025-05-10T19:53:17.076657Z","shell.execute_reply":"2025-05-10T19:53:21.345400Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7da2c62c5652411a8816f91f1af2477c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc8c4638ce941459a69faab46424ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a8510e84e8407a8b54376084adf07e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3db34b73e44a53a0c10cb5376e34c5"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Custom Dataset Class for Longformer-LNLF","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizerFast\nimport torch\nfrom torch.utils.data import Dataset\n\n# Load tokenizer\ntokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")\n\n# Parameters tuned for P100\nMAX_SENT_LEN = 128\nMAX_SENTS = 12\n\nclass EconLongformerDataset(Dataset):\n    def __init__(self, df, tokenizer, max_sent_len=MAX_SENT_LEN, max_sents=MAX_SENTS):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_sent_len = max_sent_len\n        self.max_sents = max_sents\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        sentences = row['sentences'][:self.max_sents]\n        label = row['label']\n\n        input_ids = []\n        attention_masks = []\n\n        for sent in sentences:\n            encoded = self.tokenizer(\n                sent,\n                padding='max_length',\n                truncation=True,\n                max_length=self.max_sent_len,\n                return_tensors='pt'\n            )\n            input_ids.append(encoded['input_ids'].squeeze(0))\n            attention_masks.append(encoded['attention_mask'].squeeze(0))\n\n        # Pad sentence count to MAX_SENTS\n        while len(input_ids) < self.max_sents:\n            input_ids.append(torch.zeros(self.max_sent_len, dtype=torch.long))\n            attention_masks.append(torch.zeros(self.max_sent_len, dtype=torch.long))\n\n        return {\n            'input_ids': torch.stack(input_ids),            # shape: [max_sents, max_sent_len]\n            'attention_mask': torch.stack(attention_masks), # shape: [max_sents, max_sent_len]\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:17:40.504168Z","iopub.execute_input":"2025-05-10T20:17:40.504479Z","iopub.status.idle":"2025-05-10T20:17:40.803344Z","shell.execute_reply.started":"2025-05-10T20:17:40.504457Z","shell.execute_reply":"2025-05-10T20:17:40.802777Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# Create Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\n# Create dataset\ndataset = EconLongformerDataset(df, tokenizer)\n\n# Split for train/test\ntrain_size = int(0.80 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Dataloaders (batch_size is small due to Longformer memory)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1)\n\nprint(\" Dataset and DataLoaders ready.\")\nfrom torch.utils.data import DataLoader, random_split\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:18:56.005355Z","iopub.execute_input":"2025-05-10T20:18:56.005838Z","iopub.status.idle":"2025-05-10T20:18:56.020872Z","shell.execute_reply.started":"2025-05-10T20:18:56.005813Z","shell.execute_reply":"2025-05-10T20:18:56.020245Z"}},"outputs":[{"name":"stdout","text":" Dataset and DataLoaders ready.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Define the Longformer-LNLF Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import LongformerModel\n\nclass LongformerLNLFClassifier(nn.Module):\n    def __init__(self, num_labels, base_model='allenai/longformer-base-4096',\n                 max_sent_len=128, max_sents=12, hidden_size=768):\n        super(LongformerLNLFClassifier, self).__init__()\n\n        self.max_sents = max_sents\n        self.max_sent_len = max_sent_len\n        self.hidden_size = hidden_size\n\n        # Longformer encoder\n        self.longformer = LongformerModel.from_pretrained(base_model)\n        self.longformer.gradient_checkpointing_enable()\n\n        # Document-level transformer over sentence embeddings\n        self.doc_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4),\n            num_layers=1\n        )\n\n        # Adaptive attention controller\n        self.controller = nn.Sequential(\n            nn.Linear(hidden_size, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        B, S, T = input_ids.size()  # [batch, max_sents, max_len]\n\n        input_ids = input_ids.view(-1, T)           # [B*S, T]\n        attention_mask = attention_mask.view(-1, T) # [B*S, T]\n\n        # Sentence-level encoding\n        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n        cls_tokens = outputs.last_hidden_state[:, 0, :]  # [B*S, H]\n        cls_tokens = cls_tokens.view(B, S, self.hidden_size)\n\n        # Document-level transformer\n        doc_encoded = self.doc_encoder(cls_tokens)  # [B, S, H]\n\n        # Adaptive weights\n        weights = self.controller(doc_encoded).squeeze(-1)  # [B, S]\n        soft_weights = torch.sigmoid(weights)               # [B, S]\n        pooled = torch.sum(doc_encoded * soft_weights.unsqueeze(-1), dim=1) / \\\n                 (soft_weights.sum(dim=1, keepdim=True) + 1e-5)  # [B, H]\n\n        logits = self.classifier(pooled)\n\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits, labels)\n            return {\"loss\": loss, \"logits\": logits}\n        else:\n            return {\"logits\": logits}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:20:00.303686Z","iopub.execute_input":"2025-05-10T20:20:00.304046Z","iopub.status.idle":"2025-05-10T20:20:00.312485Z","shell.execute_reply.started":"2025-05-10T20:20:00.304024Z","shell.execute_reply":"2025-05-10T20:20:00.311937Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:10:42.132683Z","iopub.execute_input":"2025-05-10T20:10:42.133006Z","iopub.status.idle":"2025-05-10T20:10:42.136752Z","shell.execute_reply.started":"2025-05-10T20:10:42.132984Z","shell.execute_reply":"2025-05-10T20:10:42.136098Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate the model\nmodel = LongformerLNLFClassifier(\n    num_labels=len(df['label'].unique()),\n    max_sent_len=64,\n    max_sents=6\n).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training loop\nepochs = 2  # You can increase to 3–5 for better results\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0.0\n\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs['loss']\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n\n    print(f\" Epoch {epoch+1} | Avg Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:49:16.794464Z","iopub.execute_input":"2025-05-10T20:49:16.795265Z","iopub.status.idle":"2025-05-10T20:49:17.687737Z","shell.execute_reply.started":"2025-05-10T20:49:16.795234Z","shell.execute_reply":"2025-05-10T20:49:17.686871Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1374705082.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_sent_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m ).to(device)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 79.12 MiB is free. Process 5052 has 15.81 GiB memory in use. Of the allocated memory 15.24 GiB is allocated by PyTorch, and 291.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 79.12 MiB is free. Process 5052 has 15.81 GiB memory in use. Of the allocated memory 15.24 GiB is allocated by PyTorch, and 291.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":37},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nimport nltk\nnltk.download('punkt')\n\n# Combine title and abstract\ndf['text'] = df['title'] + \". \" + df['abstract']\n\n# Tokenize into sentences\ndf['sentences'] = df['text'].apply(sent_tokenize)\n\n# Final dataframe for LNLF model\ndf = df[['sentences', 'label']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:46:35.920116Z","iopub.execute_input":"2025-05-10T19:46:35.920970Z","iopub.status.idle":"2025-05-10T19:46:37.041199Z","shell.execute_reply.started":"2025-05-10T19:46:35.920936Z","shell.execute_reply":"2025-05-10T19:46:37.040309Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2827968541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Final dataframe for LNLF model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['label'] not in index\""],"ename":"KeyError","evalue":"\"['label'] not in index\"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"!pip install -q nltk\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\n\n# Load your saved economics dataset\ndf = pd.read_csv('/kaggle/working/econ_filtered.csv')\ndf.dropna(subset=['title', 'abstract'], inplace=True)\n\n# Combine title + abstract\ndf['text'] = df['title'] + \". \" + df['abstract']\n\n# Split into sentences\ndf['sentences'] = df['text'].apply(lambda x: sent_tokenize(x))\n\n# Keep label and sentence data\ndf = df[['sentences', 'label']]\nprint(\" Sentence tokenization complete.\")\nprint(df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:46:10.182655Z","iopub.execute_input":"2025-05-10T19:46:10.183334Z","iopub.status.idle":"2025-05-10T19:46:14.319943Z","shell.execute_reply.started":"2025-05-10T19:46:10.183312Z","shell.execute_reply":"2025-05-10T19:46:14.318913Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3676267696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Keep label and sentence data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Sentence tokenization complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['label'] not in index\""],"ename":"KeyError","evalue":"\"['label'] not in index\"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"df.to_pickle('/kaggle/working/econ_sentences.pkl')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/econ_filtered.csv')\nprint(df.columns)  # Confirm 'label' exists\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T19:44:21.035940Z","iopub.execute_input":"2025-05-10T19:44:21.036257Z","iopub.status.idle":"2025-05-10T19:44:21.131841Z","shell.execute_reply.started":"2025-05-10T19:44:21.036230Z","shell.execute_reply":"2025-05-10T19:44:21.131088Z"}},"outputs":[{"name":"stdout","text":"Index(['id', 'title', 'abstract', 'categories', 'primary_category'], dtype='object')\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}