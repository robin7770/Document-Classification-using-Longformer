{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11764513,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom collections import Counter\nimport os\n\n# Step 1: Load the arXiv dataset\ninput_path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\ndata = []\nprint(\"Loading arXiv JSON lines dataset...\")\nwith open(input_path, 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nprint(f\"Total papers loaded: {len(data)}\")\n\n# Step 2: Convert to DataFrame\ndf = pd.DataFrame(data)\nprint(\"Available columns:\")\nprint(df.columns.tolist())\n\n# Step 3: Count papers per category\ncategory_counts = Counter()\nfor cats in df['categories']:\n    for cat in cats.split():\n        category_counts[cat] += 1\n\ncategory_df = pd.DataFrame(category_counts.items(), columns=['Category', 'Count'])\ncategory_df = category_df.sort_values('Count', ascending=False)\n\n# Step 4: Filter categories with >5,000 papers\npopular_categories = category_df[category_df['Count'] > 5000]['Category'].tolist()\nprint(f\"\\nCategories with > 5,000 papers: {len(popular_categories)}\")\nprint(category_df[category_df['Category'].isin(popular_categories)].head(20))\n\n# Step 5: Filter papers that have at least one popular category\ndef has_popular_category(category_str):\n    return any(cat in popular_categories for cat in category_str.split())\n\nfiltered_df = df[df['categories'].apply(has_popular_category)]\nprint(f\"\\nFiltered papers: {len(filtered_df)}\")\n\n# Step 6: Keep only relevant columns and enrich text\nfiltered_df = filtered_df[['title', 'abstract', 'categories']]\nfiltered_df = filtered_df.dropna()\nfiltered_df['title_abstract'] = filtered_df['title'] + \" \" + filtered_df['abstract']\n\n# Step 7: Save outputs\noutput_dir = '/kaggle/working'\nos.makedirs(output_dir, exist_ok=True)\n\nfiltered_csv_path = os.path.join(output_dir, 'filtered_arxiv_papers.csv')\nstats_csv_path = os.path.join(output_dir, 'category_statistics.csv')\n\nfiltered_df.to_csv(filtered_csv_path, index=False)\ncategory_df.to_csv(stats_csv_path, index=False)\n\n# Step 8: Print summary\nprint(\"\\n Processing Complete\")\nprint(f\"Original total papers: {len(df)}\")\nprint(f\"Filtered papers: {len(filtered_df)}\")\nprint(f\"Filtered dataset saved to: {filtered_csv_path}\")\nprint(f\"Category statistics saved to: {stats_csv_path}\")\nprint(\"\\nSample rows:\")\nprint(filtered_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:34:44.745453Z","iopub.execute_input":"2025-05-12T18:34:44.745807Z","iopub.status.idle":"2025-05-12T18:39:57.819328Z","shell.execute_reply.started":"2025-05-12T18:34:44.745785Z","shell.execute_reply":"2025-05-12T18:39:57.818520Z"}},"outputs":[{"name":"stdout","text":"Loading arXiv JSON lines dataset...\nTotal papers loaded: 2730173\nAvailable columns:\n['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n\nCategories with > 5,000 papers: 128\n               Category   Count\n96                cs.LG  215824\n0                hep-ph  187339\n13               hep-th  173525\n27             quant-ph  161515\n114               cs.CV  154368\n42                cs.AI  124844\n7                 gr-qc  113475\n9              astro-ph  105380\n8     cond-mat.mtrl-sci   99501\n6     cond-mat.mes-hall   95273\n34              math.MP   83934\n33              math-ph   83934\n126               cs.CL   83033\n20      cond-mat.str-el   77684\n21   cond-mat.stat-mech   76808\n136         astro-ph.CO   71512\n1               math.CO   71159\n110             stat.ML   70661\n144         astro-ph.GA   69844\n66              math.AP   67149\n\nFiltered papers: 2702634\n\n Processing Complete\nOriginal total papers: 2730173\nFiltered papers: 2702634\nFiltered dataset saved to: /kaggle/working/filtered_arxiv_papers.csv\nCategory statistics saved to: /kaggle/working/category_statistics.csv\n\nSample rows:\n                                               title  \\\n0  Calculation of prompt diphoton production cros...   \n1           Sparsity-certifying Graph Decompositions   \n2  The evolution of the Earth-Moon system based o...   \n3  A determinant of Stirling cycle numbers counts...   \n4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n\n                                            abstract       categories  \\\n0    A fully differential calculation in perturba...           hep-ph   \n1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG   \n2    The evolution of Earth-Moon system is descri...   physics.gen-ph   \n3    We show that a determinant of Stirling cycle...          math.CO   \n4    In this paper we show how to compute the $\\L...  math.CA math.FA   \n\n                                      title_abstract  \n0  Calculation of prompt diphoton production cros...  \n1  Sparsity-certifying Graph Decompositions   We ...  \n2  The evolution of the Earth-Moon system based o...  \n3  A determinant of Stirling cycle numbers counts...  \n4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\n\n# File paths\nfile_path = '/kaggle/working/filtered_arxiv_papers.csv'\nstats_path = '/kaggle/working/category_statistics.csv'\n\n# Check file\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\n# Load dataset\nprint(\"Loading filtered_arxiv_papers.csv...\")\ndf = pd.read_csv(file_path)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Enrich text\nif 'title_abstract' not in df.columns:\n    print(\"Enriching text column with title + abstract...\")\n    df['title_abstract'] = df['title'].astype(str) + \" \" + df['abstract'].astype(str)\n\n# Load and filter category stats for math.* only\nprint(\"Loading category statistics...\")\nstats_df = pd.read_csv(stats_path)\nstats_df = stats_df[stats_df['Category'].str.startswith('math.')]\nstats_df = stats_df.sort_values('Count', ascending=False)\n\ntop_categories = stats_df['Category'].tolist()\nprint(f\"\\nUsing ALL {len(top_categories)} math categories:\")\nprint(top_categories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:42:55.815118Z","iopub.execute_input":"2025-05-12T18:42:55.815834Z","iopub.status.idle":"2025-05-12T18:44:06.578423Z","shell.execute_reply.started":"2025-05-12T18:42:55.815809Z","shell.execute_reply":"2025-05-12T18:44:06.577627Z"}},"outputs":[{"name":"stdout","text":"Checking for /kaggle/working/filtered_arxiv_papers.csv...\nLoading filtered_arxiv_papers.csv...\nLoaded 2702634 rows\nColumns: ['title', 'abstract', 'categories', 'title_abstract']\nLoading category statistics...\n\nUsing ALL 32 math categories:\n['math.MP', 'math.CO', 'math.AP', 'math.PR', 'math.AG', 'math.OC', 'math.IT', 'math.NT', 'math.DG', 'math.NA', 'math.DS', 'math.FA', 'math.RT', 'math.ST', 'math.GT', 'math.GR', 'math.CA', 'math.QA', 'math.RA', 'math.CV', 'math.AT', 'math.LO', 'math.AC', 'math.OA', 'math.MG', 'math.SP', 'math.SG', 'math.CT', 'math.KT', 'math.GN', 'math.GM', 'math.HO']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Map labels\nlabel_map = {cat: idx for idx, cat in enumerate(top_categories)}\n\n# Extract primary math category\ndef extract_primary_category(cat_str):\n    if pd.isna(cat_str):\n        return None\n    for cat in cat_str.split():\n        if cat.startswith('math.') and cat in label_map:\n            return cat\n    return None\n\n# Apply extraction\nprint(\"Mapping categories to labels...\")\ndf['category'] = df['categories'].apply(extract_primary_category)\ndf['label'] = df['category'].map(label_map)\n\n# Filter valid rows\ndf = df[df['label'].notnull()].copy()\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:45:00.728678Z","iopub.execute_input":"2025-05-12T18:45:00.728964Z","iopub.status.idle":"2025-05-12T18:45:03.879004Z","shell.execute_reply.started":"2025-05-12T18:45:00.728944Z","shell.execute_reply":"2025-05-12T18:45:03.878354Z"}},"outputs":[{"name":"stdout","text":"Mapping categories to labels...\n                                                     title  \\\n1                 Sparsity-certifying Graph Decompositions   \n3        A determinant of Stirling cycle numbers counts...   \n4        From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n9        Partial cubes: structures, characterizations, ...   \n10       Computing genus 2 Hilbert-Siegel modular forms...   \n...                                                    ...   \n2702530  Yang-Baxter Algebra for the n-Harmonic Oscilla...   \n2702543  Integrable deformations of oscillator chains f...   \n2702544  A note on real forms of the complex N=4 supers...   \n2702558  Real forms of the complex twisted N=2 supersym...   \n2702563  Vector NLS hierarchy solitons revisited: dress...   \n\n                                                  abstract  \\\n1          We describe a new algorithm, the $(k,\\ell)$-...   \n3          We show that a determinant of Stirling cycle...   \n4          In this paper we show how to compute the $\\L...   \n9          Partial cubes are isometric subgraphs of hyp...   \n10         In this paper we present an algorithm for co...   \n...                                                    ...   \n2702530    Using a rational R-matrix associated with th...   \n2702543    A family of completely integrable nonlinear ...   \n2702544    Three inequivalent real forms of the complex...   \n2702558    Three nonequivalent real forms of the comple...   \n2702563    We discuss some algebraic aspects of the int...   \n\n                                      categories  \\\n1                                  math.CO cs.CG   \n3                                        math.CO   \n4                                math.CA math.FA   \n9                                        math.CO   \n10                               math.NT math.AG   \n...                                          ...   \n2702530         solv-int math-ph math.MP nlin.SI   \n2702543                 solv-int math.QA nlin.SI   \n2702544  solv-int hep-th math-ph math.MP nlin.SI   \n2702558  solv-int hep-th math-ph math.MP nlin.SI   \n2702563  solv-int hep-th math-ph math.MP nlin.SI   \n\n                                            title_abstract category  label  \n1        Sparsity-certifying Graph Decompositions   We ...  math.CO    1.0  \n3        A determinant of Stirling cycle numbers counts...  math.CO    1.0  \n4        From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  math.CA   16.0  \n9        Partial cubes: structures, characterizations, ...  math.CO    1.0  \n10       Computing genus 2 Hilbert-Siegel modular forms...  math.NT    7.0  \n...                                                    ...      ...    ...  \n2702530  Yang-Baxter Algebra for the n-Harmonic Oscilla...  math.MP    0.0  \n2702543  Integrable deformations of oscillator chains f...  math.QA   17.0  \n2702544  A note on real forms of the complex N=4 supers...  math.MP    0.0  \n2702558  Real forms of the complex twisted N=2 supersym...  math.MP    0.0  \n2702563  Vector NLS hierarchy solitons revisited: dress...  math.MP    0.0  \n\n[687785 rows x 6 columns]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\n\n# File paths\nfile_path = '/kaggle/working/filtered_arxiv_papers.csv'\nstats_path = '/kaggle/working/category_statistics.csv'\noutput_path = '/kaggle/working/math_dataset_labeled.csv'\nlabel_map_path = '/kaggle/working/label_map.csv'\n\n# Check file existence\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\n# Load the dataset\nprint(\"Loading filtered_arxiv_papers.csv...\")\ndf = pd.read_csv(file_path)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Enrich title_abstract if not present\nif 'title_abstract' not in df.columns:\n    print(\"Creating 'title_abstract' from title + abstract...\")\n    df['title'] = df['title'].fillna('')\n    df['abstract'] = df['abstract'].fillna('')\n    df['title_abstract'] = df['title'] + \" \" + df['abstract']\n\n# Load category stats and filter math.* categories\nprint(\"Loading and filtering category statistics...\")\nstats_df = pd.read_csv(stats_path)\nstats_df = stats_df[stats_df['Category'].str.startswith('math.')]\nstats_df = stats_df.sort_values('Count', ascending=False)\n\ntop_categories = stats_df['Category'].tolist()\nprint(f\"\\nUsing ALL {len(top_categories)} math categories:\")\nprint(top_categories)\n\n# Create label mapping\nlabel_map = {cat: idx for idx, cat in enumerate(top_categories)}\n\n# Function to extract primary math category\ndef extract_primary_category(cat_str):\n    if pd.isna(cat_str):\n        return None\n    for cat in cat_str.split():\n        if cat.startswith('math.') and cat in label_map:\n            return cat\n    return None\n\n# Apply category extraction and label mapping\nprint(\"Mapping categories to labels...\")\ndf['category'] = df['categories'].apply(extract_primary_category)\ndf['label'] = df['category'].map(label_map)\n\n# Filter out rows without math category\ndf = df[df['label'].notnull()].copy()\n\n# Save label map to CSV\npd.Series(label_map).to_csv(label_map_path, header=['Label'], index_label='Category')\nprint(f\"Saved label mapping to {label_map_path}\")\n\n# Drop unneeded columns\ndf = df.drop(columns=['title','abstract'])\n\n# Save final labeled dataset\ndf.to_csv(output_path, index=False)\nprint(f\"Saved cleaned dataset to {output_path}\")\n\n# Reload and print category distribution\nprint(\"\\n--- Category Distribution ---\")\ndf_loaded = pd.read_csv(output_path)\ncategory_counts = df_loaded['category'].value_counts()\n\nfor category, count in category_counts.items():\n    label = df_loaded[df_loaded['category'] == category]['label'].iloc[0]\n    print(f\"Category: {category} | Label: {label} | Papers: {count}\")\n\nprint(\"\\nTotal unique categories:\", df_loaded['category'].nunique())\nprint(\"Total number of papers:\", len(df_loaded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:45:08.095147Z","iopub.execute_input":"2025-05-12T18:45:08.095456Z","iopub.status.idle":"2025-05-12T18:46:45.983807Z","shell.execute_reply.started":"2025-05-12T18:45:08.095432Z","shell.execute_reply":"2025-05-12T18:46:45.983017Z"}},"outputs":[{"name":"stdout","text":"Checking for /kaggle/working/filtered_arxiv_papers.csv...\nLoading filtered_arxiv_papers.csv...\nLoaded 2702634 rows\nColumns: ['title', 'abstract', 'categories', 'title_abstract']\nLoading and filtering category statistics...\n\nUsing ALL 32 math categories:\n['math.MP', 'math.CO', 'math.AP', 'math.PR', 'math.AG', 'math.OC', 'math.IT', 'math.NT', 'math.DG', 'math.NA', 'math.DS', 'math.FA', 'math.RT', 'math.ST', 'math.GT', 'math.GR', 'math.CA', 'math.QA', 'math.RA', 'math.CV', 'math.AT', 'math.LO', 'math.AC', 'math.OA', 'math.MG', 'math.SP', 'math.SG', 'math.CT', 'math.KT', 'math.GN', 'math.GM', 'math.HO']\nMapping categories to labels...\nSaved label mapping to /kaggle/working/label_map.csv\nSaved cleaned dataset to /kaggle/working/math_dataset_labeled.csv\n\n--- Category Distribution ---\nCategory: math.AP | Label: 2.0 | Papers: 56123\nCategory: math.CO | Label: 1.0 | Papers: 55201\nCategory: math.MP | Label: 0.0 | Papers: 51329\nCategory: math.OC | Label: 5.0 | Papers: 46835\nCategory: math.IT | Label: 6.0 | Papers: 45816\nCategory: math.PR | Label: 3.0 | Papers: 45218\nCategory: math.AG | Label: 4.0 | Papers: 41155\nCategory: math.NA | Label: 9.0 | Papers: 36557\nCategory: math.NT | Label: 7.0 | Papers: 34638\nCategory: math.DG | Label: 8.0 | Papers: 33981\nCategory: math.DS | Label: 10.0 | Papers: 29312\nCategory: math.FA | Label: 11.0 | Papers: 22366\nCategory: math.ST | Label: 13.0 | Papers: 20640\nCategory: math.CA | Label: 16.0 | Papers: 16852\nCategory: math.GT | Label: 14.0 | Papers: 16660\nCategory: math.RT | Label: 12.0 | Papers: 16076\nCategory: math.GR | Label: 15.0 | Papers: 15367\nCategory: math.QA | Label: 17.0 | Papers: 11660\nCategory: math.RA | Label: 18.0 | Papers: 11492\nCategory: math.LO | Label: 21.0 | Papers: 11403\nCategory: math.CV | Label: 19.0 | Papers: 11207\nCategory: math.AT | Label: 20.0 | Papers: 10277\nCategory: math.AC | Label: 22.0 | Papers: 9148\nCategory: math.OA | Label: 23.0 | Papers: 8688\nCategory: math.MG | Label: 24.0 | Papers: 6677\nCategory: math.CT | Label: 27.0 | Papers: 5183\nCategory: math.SG | Label: 26.0 | Papers: 5071\nCategory: math.SP | Label: 25.0 | Papers: 4997\nCategory: math.GN | Label: 29.0 | Papers: 3512\nCategory: math.KT | Label: 28.0 | Papers: 2626\nCategory: math.HO | Label: 31.0 | Papers: 1431\nCategory: math.GM | Label: 30.0 | Papers: 287\n\nTotal unique categories: 32\nTotal number of papers: 687785\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"\\nTrain label distribution:\")\nprint(train_df['label'].value_counts().sort_index())\n\nprint(\"\\nVal label distribution:\")\nprint(val_df['label'].value_counts().sort_index())\n\nprint(\"\\nTest label distribution:\")\nprint(test_df['label'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:02:18.572814Z","iopub.execute_input":"2025-05-12T19:02:18.573340Z","iopub.status.idle":"2025-05-12T19:02:18.599981Z","shell.execute_reply.started":"2025-05-12T19:02:18.573317Z","shell.execute_reply":"2025-05-12T19:02:18.599320Z"}},"outputs":[{"name":"stdout","text":"\nTrain label distribution:\nlabel\n0     2400\n1     2400\n2     2400\n3     2400\n4     2400\n5     2400\n6     2400\n7     2400\n8     2400\n9     2400\n10    2400\n11    2400\n12    2400\n13    2400\n14    2400\n15    2400\n16    2400\n17    2400\n18    2400\n19    2400\n20    2400\n21    2400\n22    2400\n23    2400\n24    2400\n25    2400\n26    2400\n27    2400\n28    2400\nName: count, dtype: int64\n\nVal label distribution:\nlabel\n0     300\n1     300\n2     300\n3     300\n4     300\n5     300\n6     300\n7     300\n8     300\n9     300\n10    300\n11    300\n12    300\n13    300\n14    300\n15    300\n16    300\n17    300\n18    300\n19    300\n20    300\n21    300\n22    300\n23    300\n24    300\n25    300\n26    300\n27    300\n28    300\nName: count, dtype: int64\n\nTest label distribution:\nlabel\n0     300\n1     300\n2     300\n3     300\n4     300\n5     300\n6     300\n7     300\n8     300\n9     300\n10    300\n11    300\n12    300\n13    300\n14    300\n15    300\n16    300\n17    300\n18    300\n19    300\n20    300\n21    300\n22    300\n23    300\n24    300\n25    300\n26    300\n27    300\n28    300\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/working/math_dataset_labeled.csv')\n\n# Check the columns\nprint(df.columns)\n\n# Balance parameters\nrows_per_category = 3000\ntop_k = 10\nprint(f\"\\nBalancing: {rows_per_category} rows per category for top {top_k} categories...\")\n\n# Get top K categories\ntop_categories = df['category'].value_counts().head(top_k).index.tolist()\n\n# Collect balanced data\nfiltered_list = []\nskipped = []\n\nfor cat in top_categories:\n    cat_df = df[df['category'] == cat]\n    available = len(cat_df)\n    if available >= rows_per_category:\n        filtered_list.append(cat_df.sample(n=rows_per_category, random_state=42))\n    else:\n        skipped.append((cat, available))\n        print(f\"⚠ Skipping {cat}: only {available} papers (needs ≥ {rows_per_category})\")\n\n# Combine into a new DataFrame\nif not filtered_list:\n    raise ValueError(\"No categories had enough papers to sample from!\")\n\nbalanced_df = pd.concat(filtered_list).reset_index(drop=True)\n\n# Create label map\nused_categories = sorted(balanced_df['category'].unique())\nlabel_map = {cat: i for i, cat in enumerate(used_categories)}\nbalanced_df['label'] = balanced_df['category'].map(label_map)\n\n# Print label mapping\nprint(\"\\nLabel mapping:\")\nfor cat, idx in label_map.items():\n    count = (balanced_df['category'] == cat).sum()\n    print(f\" Category: {cat:<20} | Label: {idx:<2} | Papers: {count}\")\n\n# Split into train/val/test (80/10/10)\nprint(\"\\nSplitting dataset...\")\ntrain_df, temp_df = train_test_split(\n    balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42\n)\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n)\n\n# Save CSVs\noutput_dir = '/kaggle/working/'\ntrain_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\nval_df.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\ntest_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n\n# Summary\nprint(f\"\\nSaved train/val/test splits:\")\nprint(f\" Train size: {len(train_df)}\")\nprint(f\" Val size:   {len(val_df)}\")\nprint(f\" Test size:  {len(test_df)}\")\n\nprint(\"\\nTrain label distribution:\")\nprint(train_df['label'].value_counts().sort_index())\n\nprint(\"\\nVal label distribution:\")\nprint(val_df['label'].value_counts().sort_index())\n\nprint(\"\\nTest label distribution:\")\nprint(test_df['label'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:03:56.201996Z","iopub.execute_input":"2025-05-12T19:03:56.202531Z","iopub.status.idle":"2025-05-12T19:04:06.733158Z","shell.execute_reply.started":"2025-05-12T19:03:56.202510Z","shell.execute_reply":"2025-05-12T19:04:06.732347Z"}},"outputs":[{"name":"stdout","text":"Index(['categories', 'title_abstract', 'category', 'label'], dtype='object')\n\nBalancing: 3000 rows per category for top 10 categories...\n\nLabel mapping:\n Category: math.AG              | Label: 0  | Papers: 3000\n Category: math.AP              | Label: 1  | Papers: 3000\n Category: math.CO              | Label: 2  | Papers: 3000\n Category: math.DG              | Label: 3  | Papers: 3000\n Category: math.IT              | Label: 4  | Papers: 3000\n Category: math.MP              | Label: 5  | Papers: 3000\n Category: math.NA              | Label: 6  | Papers: 3000\n Category: math.NT              | Label: 7  | Papers: 3000\n Category: math.OC              | Label: 8  | Papers: 3000\n Category: math.PR              | Label: 9  | Papers: 3000\n\nSplitting dataset...\n\nSaved train/val/test splits:\n Train size: 24000\n Val size:   3000\n Test size:  3000\n\nTrain label distribution:\nlabel\n0    2400\n1    2400\n2    2400\n3    2400\n4    2400\n5    2400\n6    2400\n7    2400\n8    2400\n9    2400\nName: count, dtype: int64\n\nVal label distribution:\nlabel\n0    300\n1    300\n2    300\n3    300\n4    300\n5    300\n6    300\n7    300\n8    300\n9    300\nName: count, dtype: int64\n\nTest label distribution:\nlabel\n0    300\n1    300\n2    300\n3    300\n4    300\n5    300\n6    300\n7    300\n8    300\n9    300\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import LongformerTokenizerFast\nimport torch\nimport pickle\nimport logging\nimport os\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/step2_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)  # Fixed _name_ to __name__\n\n# Define input/output directories\ninput_dir = '/kaggle/working/'\noutput_dir = '/kaggle/working/'\n\n# Verify train.csv and val.csv exist\ntrain_path = os.path.join(input_dir, 'train.csv')\nval_path = os.path.join(input_dir, 'val.csv')\n\nprint(f\"Checking for train.csv at {train_path}...\")\nprint(f\"Checking for val.csv at {val_path}...\")\nif not os.path.exists(train_path) or not os.path.exists(val_path):\n    logger.error(\"train.csv or val.csv not found in /kaggle/working/.\")\n    raise FileNotFoundError(\"train.csv or val.csv not found. Please run Step 1 first.\")\n\n# Load datasets\nprint(\"Loading datasets...\")\nlogger.info(\"Loading datasets...\")\ntrain_df = pd.read_csv(train_path)\nval_df = pd.read_csv(val_path)\nprint(f\"Loaded {len(train_df)} rows in train set\")\nprint(f\"Loaded {len(val_df)} rows in val set\")\n\n# Load tokenizer\nprint(\"Loading Longformer tokenizer...\")\nlogger.info(\"Loading Longformer tokenizer...\")\ntry:\n    tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n    tokenizer.model_max_length = 1024  # Adjust to match your previous setup\nexcept Exception as e:\n    logger.error(f\"Error loading tokenizer: {e}\")\n    raise\n\n# Tokenization function\ndef tokenize_data(df, max_length=1024):\n    texts = df['title_abstract'].tolist()\n    labels = df['label'].tolist()\n    encodings = tokenizer(\n        texts,\n        truncation=True,\n        padding=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    return {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': torch.tensor(labels)\n    }\n\n# Tokenize in batches\nbatch_size = 100\ntrain_tokenized = []\nval_tokenized = []\n\nprint(\"Tokenizing training data...\")\nlogger.info(\"Tokenizing training data...\")\nfor i in range(0, len(train_df), batch_size):\n    batch_df = train_df[i:i + batch_size]\n    tokenized_batch = tokenize_data(batch_df)\n    train_tokenized.append(tokenized_batch)\n    print(f\"Tokenized train batch {i//batch_size + 1}/{len(train_df)//batch_size + 1}\")\n    logger.info(f\"Tokenized train batch {i//batch_size + 1}/{len(train_df)//batch_size + 1}\")\n\nprint(\"Tokenizing validation data...\")\nlogger.info(\"Tokenizing validation data...\")\nfor i in range(0, len(val_df), batch_size):\n    batch_df = val_df[i:i + batch_size]\n    tokenized_batch = tokenize_data(batch_df)\n    val_tokenized.append(tokenized_batch)\n    print(f\"Tokenized val batch {i//batch_size + 1}/{len(val_df)//batch_size + 1}\")\n    logger.info(f\"Tokenized val batch {i//batch_size + 1}/{len(val_df)//batch_size + 1}\")\n\n# Save tokenized datasets\nprint(f\"Saving tokenized datasets to {output_dir}...\")\nlogger.info(f\"Saving tokenized datasets to {output_dir}...\")\nwith open(os.path.join(output_dir, 'train_tokenized.pkl'), 'wb') as f:\n    pickle.dump(train_tokenized, f)\nwith open(os.path.join(output_dir, 'val_tokenized.pkl'), 'wb') as f:\n    pickle.dump(val_tokenized, f)\n\nprint(f\"Train tokenized: {len(train_tokenized)} batches, Val tokenized: {len(val_tokenized)} batches\")\nlogger.info(f\"Train tokenized: {len(train_tokenized)} batches, Val tokenized: {len(val_tokenized)} batches\")\nprint(\"Step 2 complete.\")\nlogger.info(\"Step 2 complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:11:19.241516Z","iopub.execute_input":"2025-05-12T19:11:19.242186Z","iopub.status.idle":"2025-05-12T19:11:32.730042Z","shell.execute_reply.started":"2025-05-12T19:11:19.242160Z","shell.execute_reply":"2025-05-12T19:11:32.729372Z"}},"outputs":[{"name":"stdout","text":"Checking for train.csv at /kaggle/working/train.csv...\nChecking for val.csv at /kaggle/working/val.csv...\nLoading datasets...\n","output_type":"stream"},{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/config.json\nModel config LongformerConfig {\n  \"attention_mode\": \"longformer\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"attention_window\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"ignore_attention_mask\": false,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 4098,\n  \"model_type\": \"longformer\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"onnx_export\": false,\n  \"pad_token_id\": 1,\n  \"sep_token_id\": 2,\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Loaded 24000 rows in train set\nLoaded 3000 rows in val set\nLoading Longformer tokenizer...\nTokenizing training data...\nTokenized train batch 1/241\nTokenized train batch 2/241\nTokenized train batch 3/241\nTokenized train batch 4/241\nTokenized train batch 5/241\nTokenized train batch 6/241\nTokenized train batch 7/241\nTokenized train batch 8/241\nTokenized train batch 9/241\nTokenized train batch 10/241\nTokenized train batch 11/241\nTokenized train batch 12/241\nTokenized train batch 13/241\nTokenized train batch 14/241\nTokenized train batch 15/241\nTokenized train batch 16/241\nTokenized train batch 17/241\nTokenized train batch 18/241\nTokenized train batch 19/241\nTokenized train batch 20/241\nTokenized train batch 21/241\nTokenized train batch 22/241\nTokenized train batch 23/241\nTokenized train batch 24/241\nTokenized train batch 25/241\nTokenized train batch 26/241\nTokenized train batch 27/241\nTokenized train batch 28/241\nTokenized train batch 29/241\nTokenized train batch 30/241\nTokenized train batch 31/241\nTokenized train batch 32/241\nTokenized train batch 33/241\nTokenized train batch 34/241\nTokenized train batch 35/241\nTokenized train batch 36/241\nTokenized train batch 37/241\nTokenized train batch 38/241\nTokenized train batch 39/241\nTokenized train batch 40/241\nTokenized train batch 41/241\nTokenized train batch 42/241\nTokenized train batch 43/241\nTokenized train batch 44/241\nTokenized train batch 45/241\nTokenized train batch 46/241\nTokenized train batch 47/241\nTokenized train batch 48/241\nTokenized train batch 49/241\nTokenized train batch 50/241\nTokenized train batch 51/241\nTokenized train batch 52/241\nTokenized train batch 53/241\nTokenized train batch 54/241\nTokenized train batch 55/241\nTokenized train batch 56/241\nTokenized train batch 57/241\nTokenized train batch 58/241\nTokenized train batch 59/241\nTokenized train batch 60/241\nTokenized train batch 61/241\nTokenized train batch 62/241\nTokenized train batch 63/241\nTokenized train batch 64/241\nTokenized train batch 65/241\nTokenized train batch 66/241\nTokenized train batch 67/241\nTokenized train batch 68/241\nTokenized train batch 69/241\nTokenized train batch 70/241\nTokenized train batch 71/241\nTokenized train batch 72/241\nTokenized train batch 73/241\nTokenized train batch 74/241\nTokenized train batch 75/241\nTokenized train batch 76/241\nTokenized train batch 77/241\nTokenized train batch 78/241\nTokenized train batch 79/241\nTokenized train batch 80/241\nTokenized train batch 81/241\nTokenized train batch 82/241\nTokenized train batch 83/241\nTokenized train batch 84/241\nTokenized train batch 85/241\nTokenized train batch 86/241\nTokenized train batch 87/241\nTokenized train batch 88/241\nTokenized train batch 89/241\nTokenized train batch 90/241\nTokenized train batch 91/241\nTokenized train batch 92/241\nTokenized train batch 93/241\nTokenized train batch 94/241\nTokenized train batch 95/241\nTokenized train batch 96/241\nTokenized train batch 97/241\nTokenized train batch 98/241\nTokenized train batch 99/241\nTokenized train batch 100/241\nTokenized train batch 101/241\nTokenized train batch 102/241\nTokenized train batch 103/241\nTokenized train batch 104/241\nTokenized train batch 105/241\nTokenized train batch 106/241\nTokenized train batch 107/241\nTokenized train batch 108/241\nTokenized train batch 109/241\nTokenized train batch 110/241\nTokenized train batch 111/241\nTokenized train batch 112/241\nTokenized train batch 113/241\nTokenized train batch 114/241\nTokenized train batch 115/241\nTokenized train batch 116/241\nTokenized train batch 117/241\nTokenized train batch 118/241\nTokenized train batch 119/241\nTokenized train batch 120/241\nTokenized train batch 121/241\nTokenized train batch 122/241\nTokenized train batch 123/241\nTokenized train batch 124/241\nTokenized train batch 125/241\nTokenized train batch 126/241\nTokenized train batch 127/241\nTokenized train batch 128/241\nTokenized train batch 129/241\nTokenized train batch 130/241\nTokenized train batch 131/241\nTokenized train batch 132/241\nTokenized train batch 133/241\nTokenized train batch 134/241\nTokenized train batch 135/241\nTokenized train batch 136/241\nTokenized train batch 137/241\nTokenized train batch 138/241\nTokenized train batch 139/241\nTokenized train batch 140/241\nTokenized train batch 141/241\nTokenized train batch 142/241\nTokenized train batch 143/241\nTokenized train batch 144/241\nTokenized train batch 145/241\nTokenized train batch 146/241\nTokenized train batch 147/241\nTokenized train batch 148/241\nTokenized train batch 149/241\nTokenized train batch 150/241\nTokenized train batch 151/241\nTokenized train batch 152/241\nTokenized train batch 153/241\nTokenized train batch 154/241\nTokenized train batch 155/241\nTokenized train batch 156/241\nTokenized train batch 157/241\nTokenized train batch 158/241\nTokenized train batch 159/241\nTokenized train batch 160/241\nTokenized train batch 161/241\nTokenized train batch 162/241\nTokenized train batch 163/241\nTokenized train batch 164/241\nTokenized train batch 165/241\nTokenized train batch 166/241\nTokenized train batch 167/241\nTokenized train batch 168/241\nTokenized train batch 169/241\nTokenized train batch 170/241\nTokenized train batch 171/241\nTokenized train batch 172/241\nTokenized train batch 173/241\nTokenized train batch 174/241\nTokenized train batch 175/241\nTokenized train batch 176/241\nTokenized train batch 177/241\nTokenized train batch 178/241\nTokenized train batch 179/241\nTokenized train batch 180/241\nTokenized train batch 181/241\nTokenized train batch 182/241\nTokenized train batch 183/241\nTokenized train batch 184/241\nTokenized train batch 185/241\nTokenized train batch 186/241\nTokenized train batch 187/241\nTokenized train batch 188/241\nTokenized train batch 189/241\nTokenized train batch 190/241\nTokenized train batch 191/241\nTokenized train batch 192/241\nTokenized train batch 193/241\nTokenized train batch 194/241\nTokenized train batch 195/241\nTokenized train batch 196/241\nTokenized train batch 197/241\nTokenized train batch 198/241\nTokenized train batch 199/241\nTokenized train batch 200/241\nTokenized train batch 201/241\nTokenized train batch 202/241\nTokenized train batch 203/241\nTokenized train batch 204/241\nTokenized train batch 205/241\nTokenized train batch 206/241\nTokenized train batch 207/241\nTokenized train batch 208/241\nTokenized train batch 209/241\nTokenized train batch 210/241\nTokenized train batch 211/241\nTokenized train batch 212/241\nTokenized train batch 213/241\nTokenized train batch 214/241\nTokenized train batch 215/241\nTokenized train batch 216/241\nTokenized train batch 217/241\nTokenized train batch 218/241\nTokenized train batch 219/241\nTokenized train batch 220/241\nTokenized train batch 221/241\nTokenized train batch 222/241\nTokenized train batch 223/241\nTokenized train batch 224/241\nTokenized train batch 225/241\nTokenized train batch 226/241\nTokenized train batch 227/241\nTokenized train batch 228/241\nTokenized train batch 229/241\nTokenized train batch 230/241\nTokenized train batch 231/241\nTokenized train batch 232/241\nTokenized train batch 233/241\nTokenized train batch 234/241\nTokenized train batch 235/241\nTokenized train batch 236/241\nTokenized train batch 237/241\nTokenized train batch 238/241\nTokenized train batch 239/241\nTokenized train batch 240/241\nTokenizing validation data...\nTokenized val batch 1/31\nTokenized val batch 2/31\nTokenized val batch 3/31\nTokenized val batch 4/31\nTokenized val batch 5/31\nTokenized val batch 6/31\nTokenized val batch 7/31\nTokenized val batch 8/31\nTokenized val batch 9/31\nTokenized val batch 10/31\nTokenized val batch 11/31\nTokenized val batch 12/31\nTokenized val batch 13/31\nTokenized val batch 14/31\nTokenized val batch 15/31\nTokenized val batch 16/31\nTokenized val batch 17/31\nTokenized val batch 18/31\nTokenized val batch 19/31\nTokenized val batch 20/31\nTokenized val batch 21/31\nTokenized val batch 22/31\nTokenized val batch 23/31\nTokenized val batch 24/31\nTokenized val batch 25/31\nTokenized val batch 26/31\nTokenized val batch 27/31\nTokenized val batch 28/31\nTokenized val batch 29/31\nTokenized val batch 30/31\nSaving tokenized datasets to /kaggle/working/...\nTrain tokenized: 240 batches, Val tokenized: 30 batches\nStep 2 complete.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\nimport glob\nimport re\nimport torch\nimport numpy as np\nimport pickle\nimport gc\nimport logging\nimport time\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom transformers import (\n    LongformerForSequenceClassification,\n    LongformerTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# ----------------------------\n# Setup\n# ----------------------------\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\ntorch.backends.cuda.matmul.allow_tf32 = True\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/training_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ninput_dir = '/kaggle/working/'\ntrain_tokenized_path = os.path.join(input_dir, 'train_tokenized.pkl')\nval_tokenized_path = os.path.join(input_dir, 'val_tokenized.pkl')\nresults_dir = os.path.join(input_dir, 'results')\nos.makedirs(results_dir, exist_ok=True)\n\n# ----------------------------\n# Load Checkpoint\n# ----------------------------\ndef get_latest_checkpoint(results_dir):\n    checkpoint_dirs = glob.glob(os.path.join(results_dir, 'checkpoint-*'))\n    if not checkpoint_dirs:\n        return None\n    checkpoint_nums = [int(re.search(r'checkpoint-(\\d+)', d).group(1)) for d in checkpoint_dirs]\n    return os.path.join(results_dir, f'checkpoint-{max(checkpoint_nums)}')\n\ncheckpoint_path = get_latest_checkpoint(results_dir)\nprint(f\"Checkpoint: {checkpoint_path}\" if checkpoint_path else \"No checkpoints found.\")\n\n# ----------------------------\n# Load Tokenized Data\n# ----------------------------\ndef flatten_batches(batched_data):\n    flat_data = []\n    for batch in batched_data:\n        for i in range(len(batch['input_ids'])):\n            item = {\n                'input_ids': batch['input_ids'][i][:1024],\n                'attention_mask': batch['attention_mask'][i][:1024],\n                'labels': int(batch['labels'][i])\n            }\n            flat_data.append(item)\n    return flat_data\n\nwith open(train_tokenized_path, 'rb') as f:\n    train_tokenized = pickle.load(f)\nwith open(val_tokenized_path, 'rb') as f:\n    val_tokenized = pickle.load(f)\n\ntrain_dataset = Dataset.from_list(flatten_batches(train_tokenized))\nval_dataset = Dataset.from_list(flatten_batches(val_tokenized))\n\n# ----------------------------\n# Cast label to int64\n# ----------------------------\nfrom datasets import Value\n\ntrain_dataset = train_dataset.cast_column(\"labels\", Value(\"int64\"))\nval_dataset = val_dataset.cast_column(\"labels\", Value(\"int64\"))\n\n\n# Confirm label range\ntrain_labels = [x['labels'] for x in train_dataset]\nval_labels = [x['labels'] for x in val_dataset]\nprint(\"Train label min/max:\", min(train_labels), max(train_labels))\nprint(\"Val label min/max:\", min(val_labels), max(val_labels))\n\n# ----------------------------\n# Tokenizer & Model\n# ----------------------------\ntokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\ntokenizer.model_max_length = 1024\n\n# ✅ Always load from base model to avoid mismatch errors\nmodel = LongformerForSequenceClassification.from_pretrained(\n    'allenai/longformer-base-4096',\n    num_labels=10\n)\ncheckpoint_path = None  # ✅ prevent resuming from mismatched checkpoints\n\n\n# Freeze all layers except classifier head\nfor name, param in model.named_parameters():\n    if not name.startswith(\"classifier\"):\n        param.requires_grad = False\ntrainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\nlogger.info(f\"Trainable parameters: {trainable_params}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# ----------------------------\n# Compute Metrics\n# ----------------------------\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    return {\n        'accuracy': (preds == labels).mean(),\n        'precision_weighted': precision,\n        'recall_weighted': recall,\n        'f1_weighted': f1\n    }\n\n# ----------------------------\n# Training Arguments\n# ----------------------------\ntraining_args = TrainingArguments(\n    output_dir=results_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=200,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    logging_dir=os.path.join(input_dir, 'logs'),\n    logging_steps=10,\n    logging_first_step=True,\n    eval_strategy='steps',\n    eval_steps=500,\n    save_strategy='epoch',\n    load_best_model_at_end=False,\n    fp16=True,\n    report_to='none',\n    log_level=\"info\",\n    disable_tqdm=False\n)\n\n# ----------------------------\n# Trainer\n# ----------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer, padding=True),\n)\n\n# ----------------------------\n# Train\n# ----------------------------\nprint(\"Starting training...\")\nstart_time = time.time()\ntrainer.train(resume_from_checkpoint=checkpoint_path)\nend_time = time.time()\n\n# ----------------------------\n# Save Model & Evaluation\n# ----------------------------\nfinal_model_path = os.path.join(input_dir, 'final_model')\ntrainer.save_model(final_model_path)\nprint(f\"Model saved to {final_model_path}\")\nprint(f\"Training completed in {(end_time - start_time)/60:.2f} minutes.\")\n\nmetrics = trainer.evaluate()\npd.DataFrame([metrics]).to_csv(os.path.join(input_dir, \"final_eval_metrics.csv\"), index=False)\nprint(\"Metrics saved to final_eval_metrics.csv\")\n\n# ----------------------------\n# Cleanup\n# ----------------------------\ndel model, trainer\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:16:00.603316Z","iopub.execute_input":"2025-05-12T19:16:00.603623Z","iopub.status.idle":"2025-05-12T19:16:19.202134Z","shell.execute_reply.started":"2025-05-12T19:16:00.603589Z","shell.execute_reply":"2025-05-12T19:16:19.201131Z"}},"outputs":[{"name":"stdout","text":"No checkpoints found.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/24000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992e1527c231437daad4f7a967933a39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591235513b5f4e7da1a63698518c0984"}},"metadata":{}},{"name":"stdout","text":"Train label min/max: 0 9\nVal label min/max: 0 9\n","output_type":"stream"},{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/config.json\nModel config LongformerConfig {\n  \"attention_mode\": \"longformer\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"attention_window\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"ignore_attention_mask\": false,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 4098,\n  \"model_type\": \"longformer\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"onnx_export\": false,\n  \"pad_token_id\": 1,\n  \"sep_token_id\": 2,\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/config.json\nModel config LongformerConfig {\n  \"attention_mode\": \"longformer\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"attention_window\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\"\n  },\n  \"ignore_attention_mask\": false,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 4098,\n  \"model_type\": \"longformer\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"onnx_export\": false,\n  \"pad_token_id\": 1,\n  \"sep_token_id\": 2,\n  \"transformers_version\": \"4.51.1\",\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allenai--longformer-base-4096/snapshots/301e6a42cb0d9976a6d6a26a079fef81c18aa895/pytorch_model.bin\nAttempting to create safetensors variant\nSafetensors PR exists\nSome weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1638058487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3696\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3697\u001b[0m                 )\n\u001b[0;32m-> 3698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":27}]}