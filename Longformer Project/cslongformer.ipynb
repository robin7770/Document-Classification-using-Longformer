{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11764513,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom collections import Counter\nimport os\n\n# Step 1: Load the arXiv dataset\ninput_path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\ndata = []\nprint(\"Loading arXiv JSON lines dataset...\")\nwith open(input_path, 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nprint(f\"Total papers loaded: {len(data)}\")\n\n# Step 2: Convert to DataFrame\ndf = pd.DataFrame(data)\nprint(\"Available columns:\")\nprint(df.columns.tolist())\n\n# Step 3: Count papers per category\ncategory_counts = Counter()\nfor cats in df['categories']:\n    for cat in cats.split():\n        category_counts[cat] += 1\n\ncategory_df = pd.DataFrame(category_counts.items(), columns=['Category', 'Count'])\ncategory_df = category_df.sort_values('Count', ascending=False)\n\n# Step 4: Filter categories with >5,000 papers\npopular_categories = category_df[category_df['Count'] > 5000]['Category'].tolist()\nprint(f\"\\nCategories with > 5,000 papers: {len(popular_categories)}\")\nprint(category_df[category_df['Category'].isin(popular_categories)].head(20))\n\n# Step 5: Filter papers that have at least one popular category\ndef has_popular_category(category_str):\n    return any(cat in popular_categories for cat in category_str.split())\n\nfiltered_df = df[df['categories'].apply(has_popular_category)]\nprint(f\"\\nFiltered papers: {len(filtered_df)}\")\n\n# Step 6: Keep only relevant columns and enrich text\nfiltered_df = filtered_df[['title', 'abstract', 'categories']]\nfiltered_df = filtered_df.dropna()\nfiltered_df['title_abstract'] = filtered_df['title'] + \" \" + filtered_df['abstract']\n\n# Step 7: Save outputs\noutput_dir = '/kaggle/working'\nos.makedirs(output_dir, exist_ok=True)\n\nfiltered_csv_path = os.path.join(output_dir, 'filtered_arxiv_papers.csv')\nstats_csv_path = os.path.join(output_dir, 'category_statistics.csv')\n\nfiltered_df.to_csv(filtered_csv_path, index=False)\ncategory_df.to_csv(stats_csv_path, index=False)\n\n# Step 8: Print summary\nprint(\"\\n Processing Complete\")\nprint(f\"Original total papers: {len(df)}\")\nprint(f\"Filtered papers: {len(filtered_df)}\")\nprint(f\"Filtered dataset saved to: {filtered_csv_path}\")\nprint(f\"Category statistics saved to: {stats_csv_path}\")\nprint(\"\\nSample rows:\")\nprint(filtered_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T12:34:55.389104Z","iopub.execute_input":"2025-05-12T12:34:55.389727Z"}},"outputs":[{"name":"stdout","text":"Loading arXiv JSON lines dataset...\nTotal papers loaded: 2730173\nAvailable columns:\n['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n\nCategories with > 5,000 papers: 128\n               Category   Count\n96                cs.LG  215824\n0                hep-ph  187339\n13               hep-th  173525\n27             quant-ph  161515\n114               cs.CV  154368\n42                cs.AI  124844\n7                 gr-qc  113475\n9              astro-ph  105380\n8     cond-mat.mtrl-sci   99501\n6     cond-mat.mes-hall   95273\n34              math.MP   83934\n33              math-ph   83934\n126               cs.CL   83033\n20      cond-mat.str-el   77684\n21   cond-mat.stat-mech   76808\n136         astro-ph.CO   71512\n1               math.CO   71159\n110             stat.ML   70661\n144         astro-ph.GA   69844\n66              math.AP   67149\n\nFiltered papers: 2702634\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\n\n# File paths\nfile_path = '/kaggle/working/filtered_arxiv_papers.csv'\nstats_path = '/kaggle/working/category_statistics.csv'\n\n# Check file\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\n# Load dataset\nprint(\"Loading filtered_arxiv_papers.csv...\")\ndf = pd.read_csv(file_path)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Enrich text\nif 'title_abstract' not in df.columns:\n    print(\"Enriching text column with title + abstract...\")\n    df['title_abstract'] = df['title'].astype(str) + \" \" + df['abstract'].astype(str)\n\n# Load and filter category stats for cs.* only\nprint(\"Loading category statistics...\")\nstats_df = pd.read_csv(stats_path)\nstats_df = stats_df[stats_df['Category'].str.startswith('cs.')]\nstats_df = stats_df.sort_values('Count', ascending=False)\n\ntop_categories = stats_df['Category'].tolist()\nprint(f\"\\nUsing ALL {len(top_categories)} cs categories:\")\nprint(top_categories)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport csv\n\n# File paths\nfile_path = '/kaggle/working/filtered_arxiv_papers.csv'\nstats_path = '/kaggle/working/category_statistics.csv'\noutput_path = '/kaggle/working/cs_top10_dataset_labeled_balanced.csv'\nlabel_map_path = '/kaggle/working/label_map_cs_top10.csv'\n\n# Check file existence\nprint(f\"Checking for {file_path}...\")\nif not os.path.exists(file_path):\n    raise FileNotFoundError(f\"File not found: {file_path}\")\n\n# Load the dataset\nprint(\"Loading filtered_arxiv_papers.csv...\")\ndf = pd.read_csv(file_path)\nprint(f\"Loaded {len(df)} rows\")\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Enrich title_abstract if not present\nif 'title_abstract' not in df.columns:\n    print(\"Creating 'title_abstract' from title + abstract...\")\n    df['title'] = df['title'].fillna('')\n    df['abstract'] = df['abstract'].fillna('')\n    df['title_abstract'] = df['title'] + \" \" + df['abstract']\n\n# Load category stats and filter top 10 cs.* categories\nprint(\"Loading and filtering category statistics...\")\nstats_df = pd.read_csv(stats_path)\nstats_df = stats_df[stats_df['Category'].str.startswith('cs.')]\nstats_df = stats_df.sort_values('Count', ascending=False)\ntop_categories = stats_df['Category'].head(10).tolist()\n\nprint(f\"\\nUsing Top 10 cs categories:\")\nprint(top_categories)\n\n# Create label mapping\nlabel_map = {cat: idx for idx, cat in enumerate(top_categories)}\n\n# Function to extract primary cs category\ndef extract_primary_category(cat_str):\n    if pd.isna(cat_str):\n        return None\n    for cat in cat_str.split():\n        if cat.startswith('cs.') and cat in label_map:\n            return cat\n    return None\n\n# Apply category extraction and label mapping\nprint(\"Mapping categories to labels...\")\ndf['category'] = df['categories'].apply(extract_primary_category)\ndf['label'] = df['category'].map(label_map)\n\n# Filter to keep only rows from top 10 cs categories\ndf = df[df['label'].notnull()].copy()\n\n# Balance: keep 3000 rows per category\nprint(\"\\nBalancing to 3000 papers per category...\")\nbalanced_dfs = []\nskipped = []\n\nfor cat in top_categories:\n    cat_df = df[df['category'] == cat]\n    if len(cat_df) >= 3000:\n        balanced_dfs.append(cat_df.sample(n=3000, random_state=42))\n    else:\n        skipped.append((cat, len(cat_df)))\n        print(f\"⚠ Skipped {cat} — only {len(cat_df)} papers (needs 3000)\")\n\nif not balanced_dfs:\n    raise ValueError(\"No category has enough samples!\")\n\ndf_balanced = pd.concat(balanced_dfs).reset_index(drop=True)\n\n# Save label map to CSV\npd.Series(label_map).to_csv(label_map_path, header=['Label'], index_label='Category')\nprint(f\"Saved label mapping to {label_map_path}\")\n\n# Drop unneeded columns\ndf_balanced = df_balanced.drop(columns=['title', 'abstract'])\n\n# Save final balanced dataset\ndf_balanced.to_csv(output_path, index=False)\nprint(f\"Saved cleaned and balanced dataset to {output_path}\")\n\n# Reload and print category distribution\nprint(\"\\n--- Category Distribution ---\")\ndf_loaded = pd.read_csv(output_path)\ncategory_counts = df_loaded['category'].value_counts()\n\nfor category, count in category_counts.items():\n    label = df_loaded[df_loaded['category'] == category]['label'].iloc[0]\n    print(f\"Category: {category} | Label: {label} | Papers: {count}\")\n\nprint(\"\\nTotal unique categories:\", df_loaded['category'].nunique())\nprint(\"Total number of papers:\", len(df_loaded))  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Load the balanced dataset\ninput_path = '/kaggle/working/cs_top10_dataset_labeled_balanced.csv'\ndf = pd.read_csv(input_path)\n\nprint(f\"Loaded {len(df)} rows from balanced dataset\")\nprint(df['label'].value_counts())\n\n# Split: 80% train, 10% val, 10% test\ntrain_df, temp_df = train_test_split(\n    df, test_size=0.2, stratify=df['label'], random_state=42\n)\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n)\n\n# Save splits\noutput_dir = '/kaggle/working/'\ntrain_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\nval_df.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\ntest_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n\n# Final report\nprint(\"\\nSaved splits:\")\nprint(f\" Train: {len(train_df)}\")\nprint(f\" Val:   {len(val_df)}\")\nprint(f\" Test:  {len(test_df)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import LongformerTokenizerFast\nimport torch\nimport pickle\nimport logging\nimport os\nimport re\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/cs_step2_longformer_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Input/output\ninput_dir = '/kaggle/working/'\noutput_dir = '/kaggle/working/'\n\n# Auto-locate CSVs\ntrain_path, val_path = None, None\nfor root, _, files in os.walk(input_dir):\n    for file in files:\n        if file == 'train.csv':\n            train_path = os.path.join(root, file)\n        elif file == 'val.csv':\n            val_path = os.path.join(root, file)\n    if train_path and val_path:\n        break\n\nif not train_path or not val_path:\n    logger.error(\"Missing train.csv or val.csv.\")\n    raise FileNotFoundError(\"train.csv or val.csv not found in /kaggle/working/\")\n\nlogger.info(f\"Found train: {train_path}\")\nlogger.info(f\"Found val: {val_path}\")\n\n# Load Longformer tokenizer\nlogger.info(\"Loading Longformer tokenizer...\")\ntokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n\n# Load data\ntrain_df = pd.read_csv(train_path)\nval_df = pd.read_csv(val_path)\n\n# Fill missing text fields\ntrain_df['title_abstract'] = train_df['title_abstract'].fillna(\"No content\")\nval_df['title_abstract'] = val_df['title_abstract'].fillna(\"No content\")\n\n# Clean text: remove special characters and lowercase\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]', ' ', text)     # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n    return text.lower()\n\nlogger.info(\"Cleaning special characters from text...\")\ntrain_df['title_abstract'] = train_df['title_abstract'].astype(str).apply(clean_text)\nval_df['title_abstract'] = val_df['title_abstract'].astype(str).apply(clean_text)\n\n# Tokenization function for Longformer\ndef tokenize_data(df, max_length=4096):\n    texts = df['title_abstract'].tolist()\n    labels = df['label'].astype(int).tolist()\n    encodings = tokenizer(\n        texts,\n        truncation=True,\n        padding='max_length',\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    return {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': torch.tensor(labels)\n    }\n\n# Batch tokenize\nbatch_size = 100\ntrain_tokenized, val_tokenized = [], []\n\nlogger.info(\"Tokenizing training data...\")\nfor i in range(0, len(train_df), batch_size):\n    batch_df = train_df[i:i+batch_size]\n    tokenized = tokenize_data(batch_df)\n    train_tokenized.append(tokenized)\n    logger.info(f\"Train batch {i//batch_size + 1}/{(len(train_df)-1)//batch_size + 1}\")\n\nlogger.info(\"Tokenizing validation data...\")\nfor i in range(0, len(val_df), batch_size):\n    batch_df = val_df[i:i+batch_size]\n    tokenized = tokenize_data(batch_df)\n    val_tokenized.append(tokenized)\n    logger.info(f\"Val batch {i//batch_size + 1}/{(len(val_df)-1)//batch_size + 1}\")\n\n# Save tokenized data\nwith open(os.path.join(output_dir, 'cs_train_tokenized.pkl'), 'wb') as f:\n    pickle.dump(train_tokenized, f)\nwith open(os.path.join(output_dir, 'cs_val_tokenized.pkl'), 'wb') as f:\n    pickle.dump(val_tokenized, f)\n\nlogger.info(f\"Saved: cs_train_tokenized.pkl ({len(train_tokenized)} batches), cs_val_tokenized.pkl ({len(val_tokenized)} batches)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport torch\nimport gc\nimport glob\nimport logging\nfrom datasets import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    LongformerTokenizerFast,\n    LongformerForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# Setup\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/cs_training_log_longformer.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# File paths\ninput_dir = '/kaggle/working/'\ntrain_tokenized_path = os.path.join(input_dir, 'cs_train_tokenized.pkl')\nval_tokenized_path = os.path.join(input_dir, 'cs_val_tokenized.pkl')\nresults_dir = os.path.join(input_dir, 'cs_results_longformer')\nos.makedirs(results_dir, exist_ok=True)\n\n# Tokenizer\ntokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n\n# Load tokenized data\nwith open(train_tokenized_path, 'rb') as f:\n    train_tokenized = pickle.load(f)\nwith open(val_tokenized_path, 'rb') as f:\n    val_tokenized = pickle.load(f)\n\n# Convert tokenized data to Huggingface Dataset\ndef convert_to_dataset(tokenized_data):\n    input_ids, attention_mask, labels = [], [], []\n    for batch in tokenized_data:\n        input_ids.extend(batch['input_ids'])\n        attention_mask.extend(batch['attention_mask'])\n        batch_labels = batch['labels'].tolist() if isinstance(batch['labels'], torch.Tensor) else batch['labels']\n        labels.extend(batch_labels)\n    dataset = Dataset.from_dict({\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'labels': labels\n    })\n    return dataset.map(lambda x: {'labels': int(x['labels'])})\n\n# Prepare datasets\ntrain_dataset = convert_to_dataset(train_tokenized)\nval_dataset = convert_to_dataset(val_tokenized)\ndel train_tokenized, val_tokenized\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Class weights\ntrain_labels = np.array(train_dataset['labels'])\nnum_labels = len(np.unique(train_labels))\nclass_weights_tensor = torch.tensor(\n    compute_class_weight(class_weight='balanced', classes=np.arange(num_labels), y=train_labels),\n    dtype=torch.float\n)\n\n# Load Longformer model\ncheckpoint_dirs = glob.glob(os.path.join(results_dir, 'checkpoint-*'))\ncheckpoint_path = max(checkpoint_dirs, key=os.path.getmtime) if checkpoint_dirs else None\nmodel = LongformerForSequenceClassification.from_pretrained(\n    checkpoint_path if checkpoint_path else 'allenai/longformer-base-4096',\n    num_labels=num_labels\n)\nmodel.to(torch.device('cuda'))\n\n# Custom trainer with class weights\n# Custom trainer with class weights\nclass WeightedTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=results_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    warmup_steps=500,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    logging_dir=os.path.join(input_dir, 'cs_logs_longformer'),\n    logging_steps=10,\n    save_strategy='epoch',\n    eval_strategy='epoch',\n    save_total_limit=2,\n    fp16=True,\n    report_to='none'\n)\n\n# Train\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=lambda pred: {'accuracy': (np.argmax(pred.predictions, axis=1) == pred.label_ids).mean()},\n    data_collator=DataCollatorWithPadding(tokenizer),\n    class_weights=class_weights_tensor\n)\n\nlogger.info(\"Starting Longformer training for CS top-10...\")\ntrainer.train(resume_from_checkpoint=checkpoint_path if checkpoint_path else None)\n\n# Save final model\nfinal_model_path = os.path.join(input_dir, 'cs_final_model_longformer')\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\n# Evaluate\nmetrics = trainer.evaluate()\npd.DataFrame([metrics]).to_csv(os.path.join(input_dir, \"cs_final_eval_metrics_longformer.csv\"), index=False)\nlogger.info(f\"Training completed. Model saved to {final_model_path}. Metrics saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}