{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11714990,"sourceType":"datasetVersion","datasetId":7353519}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport os\n\n# Path to your CSV\nfile_path = '/kaggle/input/physics/physics_papers.csv'\n\n# Load CSV\nprint(\"Loading dataset...\")\ndf = pd.read_csv(file_path, quoting=csv.QUOTE_ALL, on_bad_lines='warn', engine='python')\n\n# Extract physics.* subcategories\nphysics_prefix = 'physics.'\n\ndef extract_physics_subcategories(categories_series):\n    physics_subcats = set()\n    for entry in categories_series.dropna():\n        for cat in entry.split():\n            if cat.startswith(physics_prefix):\n                physics_subcats.add(cat[len(physics_prefix):])\n    return sorted(physics_subcats)\n\n# Apply\nprint(\"Extracting physics subcategories...\")\nphysics_subcategories = extract_physics_subcategories(df['categories'])\n\n# Display results\nprint(f\"Found {len(physics_subcategories)} physics subcategories:\")\nfor cat in physics_subcategories:\n    print(cat)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:40:54.098002Z","iopub.execute_input":"2025-05-09T09:40:54.098759Z","iopub.status.idle":"2025-05-09T09:41:08.902235Z","shell.execute_reply.started":"2025-05-09T09:40:54.098731Z","shell.execute_reply":"2025-05-09T09:41:08.901444Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nExtracting physics subcategories...\nFound 22 physics subcategories:\nacc-ph\nao-ph\napp-ph\natm-clus\natom-ph\nbio-ph\nchem-ph\nclass-ph\ncomp-ph\ndata-an\ned-ph\nflu-dyn\ngen-ph\ngeo-ph\nhist-ph\nins-det\nmed-ph\noptics\nplasm-ph\npop-ph\nsoc-ph\nspace-ph\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport os\nimport json\n\n# Path to your CSV\nfile_path = '/kaggle/input/physics/physics_papers.csv'\n\n# Load CSV\nprint(\"Loading dataset...\")\ndf = pd.read_csv(file_path, quoting=csv.QUOTE_ALL, on_bad_lines='warn', engine='python')\n\n# Enrich text column with title + abstract\ndf['text'] = df['title'].astype(str) + \" \" + df['abstract'].astype(str)\n\n# Extract physics.* subcategories\nphysics_prefix = 'physics.'\n\ndef extract_physics_subcategories(categories_series):\n    physics_subcats = set()\n    for entry in categories_series.dropna():\n        for cat in entry.split():\n            if cat.startswith(physics_prefix):\n                physics_subcats.add(cat[len(physics_prefix):])\n    return sorted(physics_subcats)\n\n# Apply\nprint(\"Extracting physics subcategories...\")\nphysics_subcategories = extract_physics_subcategories(df['categories'])\n\n# Display all subcategories\nprint(f\"Found {len(physics_subcategories)} physics subcategories:\")\nfor cat in physics_subcategories:\n    print(cat)\n\n# Extract primary physics category from 'categories' column\ndef extract_primary_category(cat_string):\n    if pd.isna(cat_string):\n        return None\n    for cat in cat_string.split():\n        if cat.startswith(physics_prefix):\n            return cat[len(physics_prefix):]\n    return None\n\n# Create 'category' column based on the 'categories' column\nprint(\"Creating 'category' column...\")\ndf['category'] = df['categories'].apply(extract_primary_category)\n\n# Count categories (no minimum row threshold)\ncategory_counts = df['category'].value_counts()\nvalid_categories = category_counts.index.tolist()\n\nprint(f\"\\nUsing {len(valid_categories)} physics subcategories:\")\nprint(valid_categories)\n\n# Filter to valid categories only\ndf = df[df['category'].isin(valid_categories)].copy()\n\n# Assign integer labels\nlabel_map = {cat: i for i, cat in enumerate(valid_categories)}\ndf['label'] = df['category'].map(label_map)\n\n# Select up to 10,000 rows per category\nmax_rows_per_category = 10000\nprint(f\"\\nSampling up to {max_rows_per_category} rows per subcategory...\")\nbalanced_df = pd.concat([\n    df[df['category'] == cat].sample(n=min(len(df[df['category'] == cat]), max_rows_per_category), random_state=42)\n    for cat in valid_categories\n], ignore_index=True)\n\nprint(f\"\\nFinal dataset size: {len(balanced_df)}\")\nprint(\"Label distribution:\")\nprint(balanced_df['label'].value_counts())\n\n# Save results\noutput_dir = '/kaggle/working/'\n\n# Saving the processed dataset\nbalanced_df.to_csv(os.path.join(output_dir, 'balanced_physics_papers.csv'), index=False)\n\n# Save label mapping\nlabel_map_path = os.path.join(output_dir, 'label_map.json')\nwith open(label_map_path, 'w') as f:\n    json.dump(label_map, f, indent=2)\n\nprint(f\"\\nSaved balanced dataset to: {output_dir}/balanced_physics_papers.csv\")\nprint(f\"Saved label map to: {label_map_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:41:44.560317Z","iopub.execute_input":"2025-05-09T09:41:44.560705Z","iopub.status.idle":"2025-05-09T09:42:06.322377Z","shell.execute_reply.started":"2025-05-09T09:41:44.560678Z","shell.execute_reply":"2025-05-09T09:42:06.321600Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nExtracting physics subcategories...\nFound 22 physics subcategories:\nacc-ph\nao-ph\napp-ph\natm-clus\natom-ph\nbio-ph\nchem-ph\nclass-ph\ncomp-ph\ndata-an\ned-ph\nflu-dyn\ngen-ph\ngeo-ph\nhist-ph\nins-det\nmed-ph\noptics\nplasm-ph\npop-ph\nsoc-ph\nspace-ph\nCreating 'category' column...\n\nUsing 22 physics subcategories:\n['optics', 'flu-dyn', 'soc-ph', 'atom-ph', 'chem-ph', 'ins-det', 'comp-ph', 'app-ph', 'plasm-ph', 'bio-ph', 'gen-ph', 'data-an', 'class-ph', 'acc-ph', 'med-ph', 'ao-ph', 'geo-ph', 'hist-ph', 'space-ph', 'ed-ph', 'atm-clus', 'pop-ph']\n\nSampling up to 10000 rows per subcategory...\n\nFinal dataset size: 165688\nLabel distribution:\nlabel\n0     10000\n1     10000\n2     10000\n3     10000\n4     10000\n5     10000\n6     10000\n7     10000\n8     10000\n9     10000\n10     9291\n11     7705\n12     7077\n13     7002\n14     6551\n15     5760\n16     5653\n17     4724\n18     4601\n19     3620\n20     1970\n21     1734\nName: count, dtype: int64\n\nSaved balanced dataset to: /kaggle/working//balanced_physics_papers.csv\nSaved label map to: /kaggle/working/label_map.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport os\nimport json\nimport numpy as np\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom transformers import LongformerTokenizerFast\nfrom datasets import Dataset\nimport logging\nfrom collections import Counter\nimport torch\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.FileHandler('/kaggle/working/preprocessing_log.txt'), logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\n\n# Load CSV\nfile_path = '/kaggle/input/physics/physics_papers.csv'\nprint(\"Loading dataset...\")\ndf = pd.read_csv(file_path, quoting=csv.QUOTE_ALL, on_bad_lines='warn', engine='python')\n\n# Combine title and abstract\ndf['title_abstract'] = df['title'].astype(str) + \" \" + df['abstract'].astype(str)\n\n# Extract physics.* subcategories\nphysics_prefix = 'physics.'\n\ndef extract_physics_subcategories(categories_series):\n    physics_subcats = set()\n    for entry in categories_series.dropna():\n        for cat in entry.split():\n            if cat.startswith(physics_prefix):\n                physics_subcats.add(cat[len(physics_prefix):])\n    return sorted(physics_subcats)\n\nprint(\"Extracting physics subcategories...\")\nphysics_subcategories = extract_physics_subcategories(df['categories'])\n\nprint(f\"Found {len(physics_subcategories)} physics subcategories:\")\nfor cat in physics_subcategories:\n    print(cat)\n\n# Extract primary physics category\ndef extract_primary_category(cat_string):\n    if pd.isna(cat_string):\n        return None\n    for cat in cat_string.split():\n        if cat.startswith(physics_prefix):\n            return cat[len(physics_prefix):]\n    return None\n\ndf['category'] = df['categories'].apply(extract_primary_category)\n\n# Filter to physics subcategories\ndf = df[df['category'].isin(physics_subcategories)].copy()\n\n# Label mapping\nlabel_map = {cat: i for i, cat in enumerate(physics_subcategories)}\ndf['label'] = df['category'].map(label_map)\n\n# Balance data: up to 10,000 per category\nmax_rows_per_category = 10000\nprint(f\"\\nSampling up to {max_rows_per_category} rows per subcategory...\")\nbalanced_df = pd.concat([\n    df[df['category'] == cat].sample(n=min(len(df[df['category'] == cat]), max_rows_per_category), random_state=42)\n    for cat in physics_subcategories\n], ignore_index=True)\n\nprint(f\"\\nFinal dataset size: {len(balanced_df)}\")\nprint(\"Label distribution:\")\nprint(balanced_df['label'].value_counts())\n\n# Save label map\noutput_dir = '/kaggle/working/'\nlabel_map_path = os.path.join(output_dir, 'label_map.json')\nwith open(label_map_path, 'w') as f:\n    json.dump(label_map, f, indent=2)\n\n# Train/Val/Test split\ntrain_df, temp_df = train_test_split(balanced_df, test_size=0.3, stratify=balanced_df['label'], random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n\nprint(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n\n# Save test set for evaluation\ntest_df.to_csv(os.path.join(output_dir, 'test_physics_papers.csv'), index=False)\n\n# Tokenizer setup\ntokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n\ndef tokenize_batch(texts, batch_size=32):\n    tokenized = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        encodings = tokenizer(\n            batch_texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=1024,\n            return_tensors='pt'\n        )\n        tokenized.append({\n            'input_ids': encodings['input_ids'],\n            'attention_mask': encodings['attention_mask']\n        })\n    return tokenized\n\n# Tokenize\nprint(\"Tokenizing train and val sets...\")\ntrain_texts = train_df['title_abstract']\nval_texts = val_df['title_abstract']\ntrain_labels = train_df['label']\nval_labels = val_df['label']\n\ntrain_tokenized = tokenize_batch(train_texts)\nval_tokenized = tokenize_batch(val_texts)\n\n# Add labels\nfor batch, labels in zip(train_tokenized, [train_labels[i:i+32] for i in range(0, len(train_labels), 32)]):\n    batch['labels'] = torch.tensor(labels.values, dtype=torch.long)\nfor batch, labels in zip(val_tokenized, [val_labels[i:i+32] for i in range(0, len(val_labels), 32)]):\n    batch['labels'] = torch.tensor(labels.values, dtype=torch.long)\n\n# Save tokenized data\nwith open(os.path.join(output_dir, 'traintokenized.pkl'), 'wb') as f:\n    pickle.dump(train_tokenized, f)\nwith open(os.path.join(output_dir, 'valtokenized.pkl'), 'wb') as f:\n    pickle.dump(val_tokenized, f)\n\nprint(\"Preprocessing complete. Tokenized datasets saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:42:33.362816Z","iopub.execute_input":"2025-05-09T09:42:33.363393Z","iopub.status.idle":"2025-05-09T09:44:38.626936Z","shell.execute_reply.started":"2025-05-09T09:42:33.363366Z","shell.execute_reply":"2025-05-09T09:44:38.626089Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nExtracting physics subcategories...\nFound 22 physics subcategories:\nacc-ph\nao-ph\napp-ph\natm-clus\natom-ph\nbio-ph\nchem-ph\nclass-ph\ncomp-ph\ndata-an\ned-ph\nflu-dyn\ngen-ph\ngeo-ph\nhist-ph\nins-det\nmed-ph\noptics\nplasm-ph\npop-ph\nsoc-ph\nspace-ph\n\nSampling up to 10000 rows per subcategory...\n\nFinal dataset size: 165688\nLabel distribution:\nlabel\n4     10000\n2     10000\n5     10000\n11    10000\n8     10000\n6     10000\n18    10000\n20    10000\n17    10000\n15    10000\n12     9291\n9      7705\n7      7077\n0      7002\n16     6551\n1      5760\n13     5653\n14     4724\n21     4601\n10     3620\n3      1970\n19     1734\nName: count, dtype: int64\nTrain size: 115981, Val size: 24853, Test size: 24854\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81700d9d9004f10af5bbdf63aa0d162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87cdcff213834b46bcfa35e27fd102dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130c14f32d124ce9ae06d8745e82d7f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e25b9bb4132446c84e70afb5429794c"}},"metadata":{}},{"name":"stdout","text":"Tokenizing train and val sets...\nPreprocessing complete. Tokenized datasets saved.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport os\nimport json\nimport numpy as np\nimport pickle\nimport torch\nimport gc\nimport glob\nimport re\nimport shutil\nimport logging\nfrom datasets import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    LongformerTokenizerFast,\n    LongformerForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# Setup\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/training_log.txt'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# File paths\ninput_dir = '/kaggle/working/'\ntrain_tokenized_path = os.path.join(input_dir, 'traintokenized.pkl')\nval_tokenized_path = os.path.join(input_dir, 'valtokenized.pkl')\nresults_dir = os.path.join(input_dir, 'results')\nos.makedirs(results_dir, exist_ok=True)\n\n# Tokenizer\ntokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\ntokenizer.model_max_length = 512\n\n# Load tokenized data\nwith open(train_tokenized_path, 'rb') as f:\n    train_tokenized = pickle.load(f)\nwith open(val_tokenized_path, 'rb') as f:\n    val_tokenized = pickle.load(f)\n\n# Adaptive attention masks\ndef generate_adaptive_attention_masks(input_ids, tokenizer, important_tokens=['<s>']):\n    attention_masks = []\n    important_token_ids = tokenizer.convert_tokens_to_ids(important_tokens)\n    for seq in input_ids:\n        mask = torch.zeros(len(seq), dtype=torch.long)\n        for token_id in important_token_ids:\n            mask |= (torch.tensor(seq) == token_id)\n        attention_masks.append(mask.tolist())\n    return attention_masks\n\n# Convert tokenized data to Huggingface Dataset\ndef convert_to_dataset(tokenized_data):\n    input_ids, attention_mask, labels = [], [], []\n\n    for batch in tokenized_data:\n        input_ids.extend(batch['input_ids'])\n        attention_mask.extend(batch['attention_mask'])\n        batch_labels = batch['labels'].tolist() if isinstance(batch['labels'], torch.Tensor) else batch['labels']\n        labels.extend(batch_labels)\n\n    global_attention_mask = generate_adaptive_attention_masks(input_ids, tokenizer)\n\n    dataset = Dataset.from_dict({\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'global_attention_mask': global_attention_mask,\n        'labels': labels\n    })\n    return dataset.map(lambda x: {'labels': int(x['labels'])})\n\n# Prepare datasets\ntrain_dataset = convert_to_dataset(train_tokenized)\nval_dataset = convert_to_dataset(val_tokenized)\ndel train_tokenized, val_tokenized\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Class weights\ntrain_labels = np.array(train_dataset['labels'])\nnum_labels = len(np.unique(train_labels))\nclass_weights_tensor = torch.tensor(\n    compute_class_weight(class_weight='balanced', classes=np.arange(num_labels), y=train_labels),\n    dtype=torch.float\n)\n\n# Load model\ncheckpoint_dirs = glob.glob(os.path.join(results_dir, 'checkpoint-*'))\ncheckpoint_path = max(checkpoint_dirs, key=os.path.getmtime) if checkpoint_dirs else None\nmodel = LongformerForSequenceClassification.from_pretrained(\n    checkpoint_path if checkpoint_path else 'allenai/longformer-base-4096',\n    num_labels=num_labels,\n    attention_window=256,\n    ignore_mismatched_sizes=True\n)\nmodel.gradient_checkpointing_enable()\nmodel.to(torch.device('cuda'))\n\n# Custom trainer with class weights\nclass WeightedTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=results_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir=os.path.join(input_dir, 'logs'),\n    logging_steps=10,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=2,\n    fp16=True,\n    report_to='none'\n)\n\n# Train\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=lambda pred: {'accuracy': (np.argmax(pred.predictions, axis=1) == pred.label_ids).mean()},\n    data_collator=DataCollatorWithPadding(tokenizer),\n    class_weights=class_weights_tensor\n)\n\nlogger.info(\"Starting training...\")\ntrainer.train(resume_from_checkpoint=checkpoint_path if checkpoint_path else None)\n\n# Save final model\nfinal_model_path = os.path.join(input_dir, 'final_model')\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\n# Evaluate\nmetrics = trainer.evaluate()\npd.DataFrame([metrics]).to_csv(os.path.join(input_dir, \"final_eval_metrics.csv\"), index=False)\nlogger.info(f\"Training completed. Model saved to {final_model_path}. Metrics saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:44:46.797151Z","iopub.execute_input":"2025-05-09T09:44:46.797719Z"}},"outputs":[{"name":"stderr","text":"2025-05-09 09:44:53.730936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746783893.916051      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746783893.967810      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_31/507290954.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mask |= (torch.tensor(seq) == token_id)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/115981 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f748208bc94453482d6d3ad15b43d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24853 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6691dbaf45645239a8039b4f64461cb"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f63e30df7f4e91b3accd60afcdc430"}},"metadata":{}},{"name":"stderr","text":"Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fbf8c7d5eba4acb93d49b56fbf3e87f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='145' max='43491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  145/43491 05:53 < 29:43:33, 0.41 it/s, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}